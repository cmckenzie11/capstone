{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Capstone_files/train/NORMAL/IM-0115-0001.jpeg'\n",
    "image1 = io.imread(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to grayscale, resize and normalize all images from a folder\n",
    "\n",
    "def img_process(folder, size):\n",
    "    \"\"\"Accepts an folder path as input and returns a list of all images in that folder \n",
    "    in grayscale, resized to square x by x dimensions with normalized pixel values\"\"\"\n",
    "    imageset = io.imread_collection(folder)\n",
    "    normed_images = []\n",
    "    for i in range(0,len(imageset)):\n",
    "        try:\n",
    "            rgb = gray2rgb(imageset[i])\n",
    "        except:\n",
    "            rgb = imageset[i]\n",
    "        resized = resize(rgb, (size,size))\n",
    "        normed_images.append(resized)\n",
    "    return normed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate category labels for each image set\n",
    "\n",
    "def generate_labels(set1, set2):\n",
    "    \"\"\"Takes image sets in different categories and returns a list of binary labels\"\"\"\n",
    "    labels = []\n",
    "    for i in range(len(set1)):\n",
    "        labels.append(0)\n",
    "    for j in range(len(set2)):\n",
    "        labels.append(1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for decoding class labels from model predictions\n",
    "\n",
    "def label_decoder(x):\n",
    "    \"\"\"Accepts list of model-predicted probabilities and returns binary class label\"\"\"\n",
    "    if x[0] > x[1]:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for keras progress logging\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and processing image sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ssu = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "#Process the normal training set\n",
    "folder = '../Capstone_files/train/NORMAL/*.jpeg'\n",
    "training_n = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "#Process the pneumonia training set\n",
    "folder = '../Capstone_files/train/PNEUMONIA/*.jpeg'\n",
    "training_p = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341\n",
      "3875\n",
      "1341\n",
      "5216\n",
      "training baseline: 0.7429064417177914\n"
     ]
    }
   ],
   "source": [
    "#Concatenate the images into one set, confirm size of subsets and calculate baseline accuracy for this set\n",
    "train_images = training_n + training_p\n",
    "#Standardise images\n",
    "train_images_s = [[ss.fit_transform(i) for i in j] for j in train_images]\n",
    "train_labels = generate_labels(training_n, training_p)\n",
    "#Also create sets with undersampling on the pneumonia set to address imbalance\n",
    "train_images_usample = training_n + training_p[:1341]\n",
    "train_images_usample_s = [[ssu.fit_transform(i) for i in j] for j in train_images_usample]\n",
    "train_labels_usample = generate_labels(training_n, training_p[:1341])\n",
    "print(len(training_n))\n",
    "print(len(training_p))\n",
    "print(len(training_p[:1341]))\n",
    "print(len(train_images))\n",
    "baseline_train = (len(training_p)/len(train_images))\n",
    "print('training baseline: ' + str(baseline_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "#Repeat the process for the test images\n",
    "folder = '../Capstone_files/test/NORMAL/*.jpeg'\n",
    "test_n = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "folder = '../Capstone_files/test/PNEUMONIA/*.jpeg'\n",
    "test_p = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "390\n",
      "624\n",
      "test baseline: 0.625\n"
     ]
    }
   ],
   "source": [
    "test_images = test_n + test_p\n",
    "#standardise\n",
    "test_images_s = [[ssu.transform(i) for i in j] for j in test_images]\n",
    "test_labels = generate_labels(test_n, test_p)\n",
    "print(len(test_n))\n",
    "print(len(test_p))\n",
    "print(len(test_images))\n",
    "baseline_test = (len(test_p)/len(test_images))\n",
    "print('test baseline: ' + str(baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "#And finally the validation set\n",
    "folder = '../Capstone_files/val/NORMAL/*.jpeg'\n",
    "val_n = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "folder = '../Capstone_files/val/PNEUMONIA/*.jpeg'\n",
    "val_p = img_process(folder, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "16\n",
      "validation baseline: 0.5\n"
     ]
    }
   ],
   "source": [
    "val_images = val_n + val_p\n",
    "val_images_s = [[ssu.transform(i) for i in j] for j in val_images]\n",
    "val_labels = generate_labels(val_n, val_p)\n",
    "print(len(val_n))\n",
    "print(len(val_p))\n",
    "print(len(val_images))\n",
    "baseline_val = (len(val_p)/len(val_images))\n",
    "print('validation baseline: ' + str(baseline_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate undersampled arrays, standardised and non-standardised\n",
    "X_train_array = np.array(train_images_usample).reshape(-1, 224, 224, 3)\n",
    "X_train_array_s = np.array(train_images_usample_s).reshape(-1, 224, 224, 3)\n",
    "y_train_array = np.array(train_labels_usample).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate hot-encoded labels\n",
    "y_train_array_k = keras.utils.to_categorical(y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for test images\n",
    "X_test = np.array(test_images).reshape(-1, 224, 224, 3)\n",
    "X_test_s = np.array(test_images_s).reshape(-1, 224, 224, 3)\n",
    "y_test = np.array(test_labels).reshape(-1,)\n",
    "y_test_k = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And the validation set\n",
    "X_val = np.array(val_images).reshape(-1, 224, 224, 3)\n",
    "X_val_s = np.array(val_images_s).reshape(-1, 224, 224, 3)\n",
    "y_val = np.array(val_labels).reshape(-1)\n",
    "y_val_k = keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a simple CNN using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 2\n",
    "batch_size = 16\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape, data_format='channels_last'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/100\n",
      "2682/2682 [==============================] - 33s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 2/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 3/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 4/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 5/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 6/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 7/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 8/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 9/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 10/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 11/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 12/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 13/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 14/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 15/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 16/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 17/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 18/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 19/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 20/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 21/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 22/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 23/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 24/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 25/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 26/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 27/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 28/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 29/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 30/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 31/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 32/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 33/100\n",
      "2682/2682 [==============================] - 33s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 34/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 35/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 36/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 37/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 38/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 39/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 40/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 41/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 42/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 43/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 44/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 45/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 46/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 47/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 48/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 49/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 50/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 51/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 52/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 53/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 54/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 55/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 56/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 57/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 58/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 59/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 61/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 62/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 63/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 64/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 65/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 66/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 67/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 68/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 69/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 70/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 71/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 72/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 73/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 74/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 75/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 76/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 77/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 78/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 79/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 80/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 81/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 82/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 83/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 84/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 85/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 86/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 87/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 88/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 89/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 90/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 91/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 92/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 93/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 94/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 95/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 96/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 97/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 98/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 99/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n",
      "Epoch 100/100\n",
      "2682/2682 [==============================] - 32s 12ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 6.0443 - val_acc: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1ecd38a42b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "model.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ResNet50 architecture\n",
    "resmodel = keras.applications.ResNet50(include_top = True, weights=None, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 55, 55, 64)   4160        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 55, 55, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 55, 55, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 55, 55, 256)  16640       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 55, 55, 256)  16640       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 55, 55, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 55, 55, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 55, 55, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 55, 55, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 55, 55, 64)   16448       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 55, 55, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 55, 55, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 55, 55, 256)  16640       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 55, 55, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 55, 55, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 55, 55, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 55, 55, 64)   16448       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 55, 55, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 55, 55, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 55, 55, 256)  16640       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 55, 55, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 55, 55, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 55, 55, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 28, 28, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 28, 28, 512)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 28, 28, 512)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 28, 28, 512)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 1024) 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 1024) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 1024) 0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 1024) 0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 1024) 0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 1024) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 7, 7, 2048)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 7, 7, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 7, 7, 2048)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 2)            4098        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/100\n",
      "2682/2682 [==============================] - 63s 23ms/step - loss: 0.4943 - acc: 0.8382 - val_loss: 3.8362 - val_acc: 0.6250\n",
      "Epoch 2/100\n",
      "2682/2682 [==============================] - 47s 18ms/step - loss: 0.2237 - acc: 0.9150 - val_loss: 6.0014 - val_acc: 0.6250\n",
      "Epoch 3/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1774 - acc: 0.9381 - val_loss: 1.9630 - val_acc: 0.6218\n",
      "Epoch 4/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1159 - acc: 0.9564 - val_loss: 0.7959 - val_acc: 0.7083\n",
      "Epoch 5/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1308 - acc: 0.9530 - val_loss: 0.3840 - val_acc: 0.8429\n",
      "Epoch 6/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1062 - acc: 0.9616 - val_loss: 9.9480 - val_acc: 0.3766\n",
      "Epoch 7/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0921 - acc: 0.9605 - val_loss: 9.9180 - val_acc: 0.3766\n",
      "Epoch 8/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1328 - acc: 0.9523 - val_loss: 0.5708 - val_acc: 0.7917\n",
      "Epoch 9/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0822 - acc: 0.9713 - val_loss: 3.9056 - val_acc: 0.4167\n",
      "Epoch 10/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0812 - acc: 0.9702 - val_loss: 1.4694 - val_acc: 0.6971\n",
      "Epoch 11/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0804 - acc: 0.9698 - val_loss: 10.0480 - val_acc: 0.3766\n",
      "Epoch 12/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0799 - acc: 0.9728 - val_loss: 0.6958 - val_acc: 0.7500\n",
      "Epoch 13/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0766 - acc: 0.9728 - val_loss: 8.6346 - val_acc: 0.3990\n",
      "Epoch 14/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0498 - acc: 0.9832 - val_loss: 0.5264 - val_acc: 0.8013\n",
      "Epoch 15/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0560 - acc: 0.9784 - val_loss: 1.0349 - val_acc: 0.7596\n",
      "Epoch 16/100\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0483 - acc: 0.9806 - val_loss: 0.6151 - val_acc: 0.8173\n",
      "Epoch 17/100\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0630 - acc: 0.9769 - val_loss: 1.8067 - val_acc: 0.6987\n",
      "Epoch 18/100\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0472 - acc: 0.9825 - val_loss: 1.0560 - val_acc: 0.7788\n",
      "Epoch 19/100\n",
      "2682/2682 [==============================] - 47s 18ms/step - loss: 0.0235 - acc: 0.9914 - val_loss: 0.6291 - val_acc: 0.8141\n",
      "Epoch 20/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0352 - acc: 0.9881 - val_loss: 7.8035 - val_acc: 0.3910\n",
      "Epoch 21/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0492 - acc: 0.9802 - val_loss: 0.5033 - val_acc: 0.8702\n",
      "Epoch 22/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0299 - acc: 0.9892 - val_loss: 1.8723 - val_acc: 0.6811\n",
      "Epoch 23/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0286 - acc: 0.9888 - val_loss: 0.7515 - val_acc: 0.7885\n",
      "Epoch 24/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0539 - acc: 0.9814 - val_loss: 2.0507 - val_acc: 0.6955\n",
      "Epoch 25/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0485 - acc: 0.9843 - val_loss: 3.8009 - val_acc: 0.6426\n",
      "Epoch 26/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0397 - acc: 0.9888 - val_loss: 1.0569 - val_acc: 0.7724\n",
      "Epoch 27/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0464 - acc: 0.9836 - val_loss: 2.1659 - val_acc: 0.6362\n",
      "Epoch 28/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0162 - acc: 0.9933 - val_loss: 1.0593 - val_acc: 0.8109\n",
      "Epoch 29/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0622 - acc: 0.9787 - val_loss: 1.0310 - val_acc: 0.7500\n",
      "Epoch 30/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0285 - acc: 0.9888 - val_loss: 1.5080 - val_acc: 0.7676\n",
      "Epoch 31/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0143 - acc: 0.9944 - val_loss: 1.2460 - val_acc: 0.7388\n",
      "Epoch 32/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0303 - acc: 0.9866 - val_loss: 2.0930 - val_acc: 0.7099\n",
      "Epoch 33/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0175 - acc: 0.9937 - val_loss: 0.7579 - val_acc: 0.7612\n",
      "Epoch 34/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0121 - acc: 0.9948 - val_loss: 1.2431 - val_acc: 0.7933\n",
      "Epoch 35/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 1.1141 - val_acc: 0.8157\n",
      "Epoch 36/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0086 - acc: 0.9970 - val_loss: 1.2893 - val_acc: 0.8173\n",
      "Epoch 37/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0219 - acc: 0.9925 - val_loss: 4.9893 - val_acc: 0.4487\n",
      "Epoch 38/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0837 - acc: 0.9743 - val_loss: 5.5597 - val_acc: 0.6346\n",
      "Epoch 39/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0575 - acc: 0.9814 - val_loss: 2.4493 - val_acc: 0.7131\n",
      "Epoch 40/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0146 - acc: 0.9937 - val_loss: 1.8741 - val_acc: 0.7452\n",
      "Epoch 41/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 1.2474 - val_acc: 0.8109\n",
      "Epoch 42/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0034 - acc: 0.9993 - val_loss: 2.3937 - val_acc: 0.7196\n",
      "Epoch 43/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0045 - acc: 0.9978 - val_loss: 1.4167 - val_acc: 0.8077\n",
      "Epoch 44/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 6.4947e-04 - acc: 1.0000 - val_loss: 1.3460 - val_acc: 0.8221\n",
      "Epoch 45/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 6.8869e-04 - acc: 1.0000 - val_loss: 1.5143 - val_acc: 0.8173\n",
      "Epoch 46/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 7.9870e-04 - acc: 1.0000 - val_loss: 1.5431 - val_acc: 0.8013\n",
      "Epoch 47/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1219 - acc: 0.9646 - val_loss: 4.7998 - val_acc: 0.6202\n",
      "Epoch 48/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0314 - acc: 0.9862 - val_loss: 6.2309 - val_acc: 0.4471\n",
      "Epoch 49/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0304 - acc: 0.9914 - val_loss: 5.7541 - val_acc: 0.5705\n",
      "Epoch 50/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0343 - acc: 0.9881 - val_loss: 1.3164 - val_acc: 0.7869\n",
      "Epoch 51/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.9148 - val_acc: 0.8413\n",
      "Epoch 52/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0028 - acc: 0.9996 - val_loss: 1.6939 - val_acc: 0.7708\n",
      "Epoch 53/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.8771 - val_acc: 0.8365\n",
      "Epoch 54/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 6.9041e-04 - acc: 1.0000 - val_loss: 1.2128 - val_acc: 0.8189\n",
      "Epoch 55/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 2.7821 - val_acc: 0.7179\n",
      "Epoch 56/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0276 - acc: 0.9914 - val_loss: 9.9261 - val_acc: 0.3830\n",
      "Epoch 57/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0335 - acc: 0.9884 - val_loss: 3.0470 - val_acc: 0.6619\n",
      "Epoch 58/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0184 - acc: 0.9929 - val_loss: 1.9413 - val_acc: 0.6891\n",
      "Epoch 59/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0103 - acc: 0.9963 - val_loss: 1.7916 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0056 - acc: 0.9989 - val_loss: 1.3776 - val_acc: 0.7965\n",
      "Epoch 61/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 1.3367 - val_acc: 0.8029\n",
      "Epoch 62/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0080 - acc: 0.9952 - val_loss: 1.1601 - val_acc: 0.8269\n",
      "Epoch 63/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 1.5969 - val_acc: 0.8301\n",
      "Epoch 64/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0271 - acc: 0.9899 - val_loss: 0.8003 - val_acc: 0.8349\n",
      "Epoch 65/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0127 - acc: 0.9944 - val_loss: 5.4957 - val_acc: 0.5016\n",
      "Epoch 66/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 6.0810 - val_acc: 0.5064\n",
      "Epoch 67/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.9516 - val_acc: 0.8253\n",
      "Epoch 68/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 1.1662 - val_acc: 0.8365\n",
      "Epoch 69/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 2.5743 - val_acc: 0.7372\n",
      "Epoch 70/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.6378 - val_acc: 0.8077\n",
      "Epoch 71/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 1.6610 - val_acc: 0.7885\n",
      "Epoch 72/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0813 - acc: 0.9784 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 73/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0540 - acc: 0.9814 - val_loss: 1.3892 - val_acc: 0.7756\n",
      "Epoch 74/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0109 - acc: 0.9963 - val_loss: 1.4785 - val_acc: 0.7708\n",
      "Epoch 75/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 1.6789 - val_acc: 0.7708\n",
      "Epoch 76/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.3615 - val_acc: 0.7981\n",
      "Epoch 77/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 5.9832e-04 - acc: 1.0000 - val_loss: 1.2382 - val_acc: 0.8093\n",
      "Epoch 78/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 9.0789e-04 - acc: 0.9996 - val_loss: 1.3244 - val_acc: 0.8109\n",
      "Epoch 79/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 5.2965e-04 - acc: 1.0000 - val_loss: 1.5757 - val_acc: 0.7949\n",
      "Epoch 80/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 1.9028e-04 - acc: 1.0000 - val_loss: 1.4299 - val_acc: 0.8077\n",
      "Epoch 81/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 2.8298e-04 - acc: 1.0000 - val_loss: 1.3884 - val_acc: 0.8157\n",
      "Epoch 82/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 2.5959e-04 - acc: 1.0000 - val_loss: 1.6206 - val_acc: 0.8013\n",
      "Epoch 83/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 8.0834e-05 - acc: 1.0000 - val_loss: 1.6018 - val_acc: 0.8045\n",
      "Epoch 84/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 8.1964e-05 - acc: 1.0000 - val_loss: 1.5228 - val_acc: 0.8189\n",
      "Epoch 85/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 5.5183e-05 - acc: 1.0000 - val_loss: 1.4953 - val_acc: 0.8221\n",
      "Epoch 86/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 1.3606e-04 - acc: 1.0000 - val_loss: 1.9078 - val_acc: 0.7804\n",
      "Epoch 87/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 7.6555e-05 - acc: 1.0000 - val_loss: 1.5512 - val_acc: 0.8173\n",
      "Epoch 88/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 1.3034e-04 - acc: 1.0000 - val_loss: 1.6816 - val_acc: 0.8061\n",
      "Epoch 89/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 5.8528e-05 - acc: 1.0000 - val_loss: 1.5055 - val_acc: 0.8189\n",
      "Epoch 90/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0181 - acc: 0.9937 - val_loss: 2.8889 - val_acc: 0.6234\n",
      "Epoch 91/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.1154 - acc: 0.9650 - val_loss: 10.0547 - val_acc: 0.3750\n",
      "Epoch 92/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0513 - acc: 0.9851 - val_loss: 2.3459 - val_acc: 0.7003\n",
      "Epoch 93/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0085 - acc: 0.9966 - val_loss: 0.9038 - val_acc: 0.8301\n",
      "Epoch 94/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 1.1376 - val_acc: 0.8109\n",
      "Epoch 95/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0079 - acc: 0.9981 - val_loss: 1.4579 - val_acc: 0.7917\n",
      "Epoch 96/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0138 - acc: 0.9944 - val_loss: 2.0662 - val_acc: 0.7708\n",
      "Epoch 97/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 1.3594 - val_acc: 0.8157\n",
      "Epoch 98/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 0.9124 - val_acc: 0.8205\n",
      "Epoch 99/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0075 - acc: 0.9970 - val_loss: 3.2188 - val_acc: 0.5641\n",
      "Epoch 100/100\n",
      "2682/2682 [==============================] - 48s 18ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 1.3613 - val_acc: 0.8029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x294670b7e10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model over 100 epochs\n",
    "resmodel.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model in case I need to use it later\n",
    "filename = 'resnet100.hdf5'\n",
    "keras.models.save_model(resmodel, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.49      0.65       234\n",
      "          1       0.76      0.99      0.86       390\n",
      "\n",
      "avg / total       0.84      0.80      0.78       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294f6e9b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate model\n",
    "res_pred_base = resmodel.predict(X_test)\n",
    "res_pred = [label_decoder(i) for i in res_pred_base]\n",
    "res_cm = confusion_matrix(y_test, res_pred)\n",
    "sns.heatmap(res_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, res_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77         8\n",
      "          1       0.73      1.00      0.84         8\n",
      "\n",
      "avg / total       0.86      0.81      0.81        16\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294f6eba518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_pred_base = resmodel.predict(X_val)\n",
    "val_pred = [label_decoder(i) for i in val_pred_base]\n",
    "val_cm = confusion_matrix(y_val, val_pred)\n",
    "sns.heatmap(val_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/1000\n",
      "2682/2682 [==============================] - 51s 19ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 1.8148 - val_acc: 0.7628\n",
      "Epoch 2/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 1.5300 - val_acc: 0.7837\n",
      "Epoch 3/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9974e-04 - acc: 1.0000 - val_loss: 1.5261 - val_acc: 0.7949\n",
      "Epoch 4/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.9534e-04 - acc: 1.0000 - val_loss: 1.9020 - val_acc: 0.7644\n",
      "Epoch 5/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6350e-04 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 0.7756\n",
      "Epoch 6/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2805e-04 - acc: 1.0000 - val_loss: 1.7311 - val_acc: 0.7821\n",
      "Epoch 7/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4333e-04 - acc: 1.0000 - val_loss: 1.5850 - val_acc: 0.8077\n",
      "Epoch 8/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9184e-05 - acc: 1.0000 - val_loss: 1.6857 - val_acc: 0.7965\n",
      "Epoch 9/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.6512e-04 - acc: 1.0000 - val_loss: 1.9311 - val_acc: 0.7740\n",
      "Epoch 10/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 5.2224e-05 - acc: 1.0000 - val_loss: 1.7321 - val_acc: 0.7949\n",
      "Epoch 11/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 3.1210e-05 - acc: 1.0000 - val_loss: 1.6932 - val_acc: 0.8029\n",
      "Epoch 12/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 4.3415e-05 - acc: 1.0000 - val_loss: 1.7981 - val_acc: 0.7885\n",
      "Epoch 13/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 8.5951e-05 - acc: 1.0000 - val_loss: 1.8324 - val_acc: 0.7933\n",
      "Epoch 14/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7546e-05 - acc: 1.0000 - val_loss: 1.6905 - val_acc: 0.8013\n",
      "Epoch 15/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 5.3054e-05 - acc: 1.0000 - val_loss: 1.6860 - val_acc: 0.8013\n",
      "Epoch 16/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.2961e-05 - acc: 1.0000 - val_loss: 1.6650 - val_acc: 0.8029\n",
      "Epoch 17/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.2955e-05 - acc: 1.0000 - val_loss: 1.6048 - val_acc: 0.8061\n",
      "Epoch 18/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2696e-05 - acc: 1.0000 - val_loss: 1.6564 - val_acc: 0.8045\n",
      "Epoch 19/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 4.1056e-05 - acc: 1.0000 - val_loss: 1.8763 - val_acc: 0.7885\n",
      "Epoch 20/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.7884e-05 - acc: 1.0000 - val_loss: 1.7176 - val_acc: 0.7997\n",
      "Epoch 21/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.1760e-05 - acc: 1.0000 - val_loss: 1.7535 - val_acc: 0.7981\n",
      "Epoch 22/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.0864e-05 - acc: 1.0000 - val_loss: 1.7723 - val_acc: 0.7965\n",
      "Epoch 23/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 7.8623e-06 - acc: 1.0000 - val_loss: 1.7309 - val_acc: 0.7981\n",
      "Epoch 24/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 8.9396e-06 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 0.7997\n",
      "Epoch 25/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8712e-06 - acc: 1.0000 - val_loss: 1.7660 - val_acc: 0.7997\n",
      "Epoch 26/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6607e-06 - acc: 1.0000 - val_loss: 1.7870 - val_acc: 0.7981\n",
      "Epoch 27/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0701e-06 - acc: 1.0000 - val_loss: 1.7412 - val_acc: 0.8045\n",
      "Epoch 28/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9429e-06 - acc: 1.0000 - val_loss: 1.8172 - val_acc: 0.7965\n",
      "Epoch 29/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6873e-05 - acc: 1.0000 - val_loss: 1.8385 - val_acc: 0.8061\n",
      "Epoch 30/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2126e-05 - acc: 1.0000 - val_loss: 1.7945 - val_acc: 0.8061\n",
      "Epoch 31/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6527e-06 - acc: 1.0000 - val_loss: 1.8795 - val_acc: 0.8029\n",
      "Epoch 32/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0567e-05 - acc: 1.0000 - val_loss: 2.1071 - val_acc: 0.7853\n",
      "Epoch 33/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5944e-06 - acc: 1.0000 - val_loss: 1.9746 - val_acc: 0.7997\n",
      "Epoch 34/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3569e-06 - acc: 1.0000 - val_loss: 1.9041 - val_acc: 0.8077\n",
      "Epoch 35/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8568e-06 - acc: 1.0000 - val_loss: 1.8837 - val_acc: 0.8093\n",
      "Epoch 36/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3658e-06 - acc: 1.0000 - val_loss: 1.9147 - val_acc: 0.8061\n",
      "Epoch 37/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0993e-06 - acc: 1.0000 - val_loss: 2.0260 - val_acc: 0.7997\n",
      "Epoch 38/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2403e-06 - acc: 1.0000 - val_loss: 1.9721 - val_acc: 0.8029\n",
      "Epoch 39/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2447e-06 - acc: 1.0000 - val_loss: 2.0934 - val_acc: 0.7949\n",
      "Epoch 40/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7082e-06 - acc: 1.0000 - val_loss: 2.1407 - val_acc: 0.7965\n",
      "Epoch 41/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9994e-06 - acc: 1.0000 - val_loss: 1.8700 - val_acc: 0.8029\n",
      "Epoch 42/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1821e-06 - acc: 1.0000 - val_loss: 1.9387 - val_acc: 0.8013\n",
      "Epoch 43/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6601e-06 - acc: 1.0000 - val_loss: 1.9369 - val_acc: 0.8029\n",
      "Epoch 44/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9899e-06 - acc: 1.0000 - val_loss: 1.9743 - val_acc: 0.8013\n",
      "Epoch 45/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5712e-06 - acc: 1.0000 - val_loss: 2.0277 - val_acc: 0.8029\n",
      "Epoch 46/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2853e-06 - acc: 1.0000 - val_loss: 2.1900 - val_acc: 0.7917\n",
      "Epoch 47/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8835e-06 - acc: 1.0000 - val_loss: 1.9921 - val_acc: 0.8029\n",
      "Epoch 48/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1035e-06 - acc: 1.0000 - val_loss: 2.0713 - val_acc: 0.7965\n",
      "Epoch 49/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4613e-06 - acc: 1.0000 - val_loss: 1.9983 - val_acc: 0.8029\n",
      "Epoch 50/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1832e-06 - acc: 1.0000 - val_loss: 2.0279 - val_acc: 0.7981\n",
      "Epoch 51/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9554e-07 - acc: 1.0000 - val_loss: 2.0784 - val_acc: 0.7965\n",
      "Epoch 52/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3762e-06 - acc: 1.0000 - val_loss: 1.7599 - val_acc: 0.8189\n",
      "Epoch 53/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6312e-06 - acc: 1.0000 - val_loss: 2.0065 - val_acc: 0.7997\n",
      "Epoch 54/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6474e-07 - acc: 1.0000 - val_loss: 1.9923 - val_acc: 0.8077\n",
      "Epoch 55/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2602e-07 - acc: 1.0000 - val_loss: 1.9967 - val_acc: 0.8077\n",
      "Epoch 56/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8365e-07 - acc: 1.0000 - val_loss: 2.0579 - val_acc: 0.8013\n",
      "Epoch 57/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7969e-06 - acc: 1.0000 - val_loss: 1.9898 - val_acc: 0.8061\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4086e-07 - acc: 1.0000 - val_loss: 1.9742 - val_acc: 0.8077\n",
      "Epoch 59/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2677e-07 - acc: 1.0000 - val_loss: 2.0429 - val_acc: 0.8029\n",
      "Epoch 60/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0630e-07 - acc: 1.0000 - val_loss: 2.0127 - val_acc: 0.8029\n",
      "Epoch 61/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3378e-07 - acc: 1.0000 - val_loss: 1.9933 - val_acc: 0.8045\n",
      "Epoch 62/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5373e-07 - acc: 1.0000 - val_loss: 1.9876 - val_acc: 0.8045\n",
      "Epoch 63/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1996e-07 - acc: 1.0000 - val_loss: 2.0090 - val_acc: 0.8045\n",
      "Epoch 64/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3534e-07 - acc: 1.0000 - val_loss: 2.0677 - val_acc: 0.8013\n",
      "Epoch 65/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3112e-07 - acc: 1.0000 - val_loss: 2.0887 - val_acc: 0.8029\n",
      "Epoch 66/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3189e-07 - acc: 1.0000 - val_loss: 2.0642 - val_acc: 0.8029\n",
      "Epoch 67/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8892e-07 - acc: 1.0000 - val_loss: 2.0557 - val_acc: 0.8045\n",
      "Epoch 68/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8955e-07 - acc: 1.0000 - val_loss: 1.9866 - val_acc: 0.8077\n",
      "Epoch 69/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2206e-07 - acc: 1.0000 - val_loss: 2.0145 - val_acc: 0.8045\n",
      "Epoch 70/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1633e-07 - acc: 1.0000 - val_loss: 2.0774 - val_acc: 0.8013\n",
      "Epoch 71/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7673e-07 - acc: 1.0000 - val_loss: 2.0554 - val_acc: 0.8061\n",
      "Epoch 72/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6774e-07 - acc: 1.0000 - val_loss: 1.9872 - val_acc: 0.8077\n",
      "Epoch 73/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1537e-06 - acc: 1.0000 - val_loss: 1.7587 - val_acc: 0.8317\n",
      "Epoch 74/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6757e-07 - acc: 1.0000 - val_loss: 1.9345 - val_acc: 0.8253\n",
      "Epoch 75/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2112e-07 - acc: 1.0000 - val_loss: 2.0709 - val_acc: 0.8109\n",
      "Epoch 76/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6905e-07 - acc: 1.0000 - val_loss: 2.0054 - val_acc: 0.8157\n",
      "Epoch 77/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4549e-07 - acc: 1.0000 - val_loss: 2.1408 - val_acc: 0.8045\n",
      "Epoch 78/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4524e-07 - acc: 1.0000 - val_loss: 1.9868 - val_acc: 0.8141\n",
      "Epoch 79/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5788e-07 - acc: 1.0000 - val_loss: 2.0033 - val_acc: 0.8141\n",
      "Epoch 80/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8492e-07 - acc: 1.0000 - val_loss: 1.9969 - val_acc: 0.8125\n",
      "Epoch 81/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6470e-07 - acc: 1.0000 - val_loss: 2.0314 - val_acc: 0.8109\n",
      "Epoch 82/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4874e-07 - acc: 1.0000 - val_loss: 2.3238 - val_acc: 0.7917\n",
      "Epoch 83/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9864e-07 - acc: 1.0000 - val_loss: 2.1463 - val_acc: 0.8061\n",
      "Epoch 84/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0377e-07 - acc: 1.0000 - val_loss: 2.1942 - val_acc: 0.8013\n",
      "Epoch 85/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5755e-07 - acc: 1.0000 - val_loss: 2.1172 - val_acc: 0.8093\n",
      "Epoch 86/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5939e-07 - acc: 1.0000 - val_loss: 2.0725 - val_acc: 0.8093\n",
      "Epoch 87/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6068e-07 - acc: 1.0000 - val_loss: 2.1302 - val_acc: 0.8093\n",
      "Epoch 88/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4346e-07 - acc: 1.0000 - val_loss: 2.1367 - val_acc: 0.8109\n",
      "Epoch 89/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4159e-07 - acc: 1.0000 - val_loss: 2.1193 - val_acc: 0.8109\n",
      "Epoch 90/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7419e-07 - acc: 1.0000 - val_loss: 2.1485 - val_acc: 0.8093\n",
      "Epoch 91/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7150e-07 - acc: 1.0000 - val_loss: 2.2726 - val_acc: 0.7997\n",
      "Epoch 92/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4421e-07 - acc: 1.0000 - val_loss: 2.1693 - val_acc: 0.8093\n",
      "Epoch 93/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3517e-07 - acc: 1.0000 - val_loss: 2.1966 - val_acc: 0.8077\n",
      "Epoch 94/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2870e-07 - acc: 1.0000 - val_loss: 2.1677 - val_acc: 0.8093\n",
      "Epoch 95/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3050e-07 - acc: 1.0000 - val_loss: 2.1431 - val_acc: 0.8093\n",
      "Epoch 96/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4466e-07 - acc: 1.0000 - val_loss: 2.1955 - val_acc: 0.8093\n",
      "Epoch 97/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2677e-07 - acc: 1.0000 - val_loss: 2.1472 - val_acc: 0.8093\n",
      "Epoch 98/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0684e-07 - acc: 1.0000 - val_loss: 2.2395 - val_acc: 0.8077\n",
      "Epoch 99/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2843e-07 - acc: 1.0000 - val_loss: 2.1644 - val_acc: 0.8093\n",
      "Epoch 100/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2877e-07 - acc: 1.0000 - val_loss: 2.1338 - val_acc: 0.8109\n",
      "Epoch 101/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4281e-07 - acc: 1.0000 - val_loss: 2.1390 - val_acc: 0.8125\n",
      "Epoch 102/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1747 - acc: 0.9594 - val_loss: 1.0677 - val_acc: 0.7853\n",
      "Epoch 103/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0324 - acc: 0.9892 - val_loss: 1.2012 - val_acc: 0.8125\n",
      "Epoch 104/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0139 - acc: 0.9940 - val_loss: 1.5893 - val_acc: 0.7612\n",
      "Epoch 105/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.9436 - val_acc: 0.7452\n",
      "Epoch 106/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.5968 - val_acc: 0.7901\n",
      "Epoch 107/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8187e-04 - acc: 1.0000 - val_loss: 1.4797 - val_acc: 0.8013\n",
      "Epoch 108/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3146e-04 - acc: 1.0000 - val_loss: 1.7646 - val_acc: 0.7821\n",
      "Epoch 109/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 5.6161e-04 - acc: 1.0000 - val_loss: 1.5263 - val_acc: 0.7901\n",
      "Epoch 110/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 4.2137e-04 - acc: 1.0000 - val_loss: 1.7824 - val_acc: 0.7708\n",
      "Epoch 111/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0310e-04 - acc: 1.0000 - val_loss: 1.7088 - val_acc: 0.7869\n",
      "Epoch 112/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.4974e-04 - acc: 1.0000 - val_loss: 1.6534 - val_acc: 0.7917\n",
      "Epoch 113/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3121e-05 - acc: 1.0000 - val_loss: 1.5426 - val_acc: 0.8077\n",
      "Epoch 114/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 7.5450e-05 - acc: 1.0000 - val_loss: 1.5235 - val_acc: 0.8077\n",
      "Epoch 115/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.5055e-05 - acc: 1.0000 - val_loss: 1.5459 - val_acc: 0.8077\n",
      "Epoch 116/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 9.6206e-05 - acc: 1.0000 - val_loss: 1.7154 - val_acc: 0.7917\n",
      "Epoch 117/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7525e-05 - acc: 1.0000 - val_loss: 1.5999 - val_acc: 0.8013\n",
      "Epoch 118/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 5.9113e-05 - acc: 1.0000 - val_loss: 1.5797 - val_acc: 0.8077\n",
      "Epoch 119/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8753e-05 - acc: 1.0000 - val_loss: 1.5977 - val_acc: 0.8093\n",
      "Epoch 120/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 4.0554e-05 - acc: 1.0000 - val_loss: 1.6398 - val_acc: 0.7997\n",
      "Epoch 121/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3250e-05 - acc: 1.0000 - val_loss: 1.6412 - val_acc: 0.8029\n",
      "Epoch 122/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6832e-05 - acc: 1.0000 - val_loss: 1.5531 - val_acc: 0.8125\n",
      "Epoch 123/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8016e-05 - acc: 1.0000 - val_loss: 1.7093 - val_acc: 0.8013\n",
      "Epoch 124/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7543e-05 - acc: 1.0000 - val_loss: 1.7134 - val_acc: 0.8013\n",
      "Epoch 125/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7492e-05 - acc: 1.0000 - val_loss: 1.5295 - val_acc: 0.8189\n",
      "Epoch 126/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6084e-05 - acc: 1.0000 - val_loss: 1.7152 - val_acc: 0.8029\n",
      "Epoch 127/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9176e-05 - acc: 1.0000 - val_loss: 1.6122 - val_acc: 0.8173\n",
      "Epoch 128/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8780e-06 - acc: 1.0000 - val_loss: 1.6284 - val_acc: 0.8173\n",
      "Epoch 129/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1239e-05 - acc: 1.0000 - val_loss: 1.6455 - val_acc: 0.8125\n",
      "Epoch 130/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5851e-05 - acc: 1.0000 - val_loss: 1.6155 - val_acc: 0.8189\n",
      "Epoch 131/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0304e-05 - acc: 1.0000 - val_loss: 1.5854 - val_acc: 0.8205\n",
      "Epoch 132/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5389e-06 - acc: 1.0000 - val_loss: 1.6218 - val_acc: 0.8189\n",
      "Epoch 133/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6338e-05 - acc: 1.0000 - val_loss: 1.6356 - val_acc: 0.8205\n",
      "Epoch 134/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7098e-05 - acc: 1.0000 - val_loss: 1.8912 - val_acc: 0.7933\n",
      "Epoch 135/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4474e-06 - acc: 1.0000 - val_loss: 1.7331 - val_acc: 0.8077\n",
      "Epoch 136/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6667e-06 - acc: 1.0000 - val_loss: 1.7018 - val_acc: 0.8173\n",
      "Epoch 137/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6189e-06 - acc: 1.0000 - val_loss: 1.8273 - val_acc: 0.8013\n",
      "Epoch 138/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0434e-06 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 0.8125\n",
      "Epoch 139/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6417e-06 - acc: 1.0000 - val_loss: 1.6893 - val_acc: 0.8173\n",
      "Epoch 140/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5894e-06 - acc: 1.0000 - val_loss: 1.6650 - val_acc: 0.8189\n",
      "Epoch 141/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3275e-06 - acc: 1.0000 - val_loss: 1.7473 - val_acc: 0.8109\n",
      "Epoch 142/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0473e-06 - acc: 1.0000 - val_loss: 1.7355 - val_acc: 0.8093\n",
      "Epoch 143/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1248e-06 - acc: 1.0000 - val_loss: 1.7235 - val_acc: 0.8125\n",
      "Epoch 144/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8860e-06 - acc: 1.0000 - val_loss: 1.7166 - val_acc: 0.8189\n",
      "Epoch 145/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2404e-06 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 0.7981\n",
      "Epoch 146/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1171e-06 - acc: 1.0000 - val_loss: 1.8009 - val_acc: 0.8013\n",
      "Epoch 147/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5956e-06 - acc: 1.0000 - val_loss: 1.7062 - val_acc: 0.8221\n",
      "Epoch 148/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4711e-06 - acc: 1.0000 - val_loss: 1.8110 - val_acc: 0.8077\n",
      "Epoch 149/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4601e-06 - acc: 1.0000 - val_loss: 1.8439 - val_acc: 0.8013\n",
      "Epoch 150/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6644e-06 - acc: 1.0000 - val_loss: 1.7656 - val_acc: 0.8141\n",
      "Epoch 151/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1493e-06 - acc: 1.0000 - val_loss: 1.9287 - val_acc: 0.7949\n",
      "Epoch 152/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2850e-06 - acc: 1.0000 - val_loss: 1.8158 - val_acc: 0.8125\n",
      "Epoch 153/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3563e-06 - acc: 1.0000 - val_loss: 1.8284 - val_acc: 0.8125\n",
      "Epoch 154/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7603e-06 - acc: 1.0000 - val_loss: 2.0325 - val_acc: 0.7933\n",
      "Epoch 155/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0518 - acc: 0.9892 - val_loss: 4.9353 - val_acc: 0.6266\n",
      "Epoch 156/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1003 - acc: 0.9687 - val_loss: 1.3152 - val_acc: 0.7484\n",
      "Epoch 157/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0212 - acc: 0.9925 - val_loss: 0.5577 - val_acc: 0.8397\n",
      "Epoch 158/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 1.0425 - val_acc: 0.8045\n",
      "Epoch 159/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.3384 - val_acc: 0.7869\n",
      "Epoch 160/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8311e-04 - acc: 1.0000 - val_loss: 1.1170 - val_acc: 0.8093\n",
      "Epoch 161/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3537e-04 - acc: 1.0000 - val_loss: 1.2011 - val_acc: 0.8093\n",
      "Epoch 162/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8685e-04 - acc: 1.0000 - val_loss: 1.3760 - val_acc: 0.8029\n",
      "Epoch 163/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6593e-04 - acc: 1.0000 - val_loss: 1.2967 - val_acc: 0.8141\n",
      "Epoch 164/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3585e-04 - acc: 1.0000 - val_loss: 1.0493 - val_acc: 0.8237\n",
      "Epoch 165/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0278e-04 - acc: 1.0000 - val_loss: 1.4146 - val_acc: 0.8045\n",
      "Epoch 166/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3026e-04 - acc: 1.0000 - val_loss: 1.3821 - val_acc: 0.8109\n",
      "Epoch 167/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2680e-04 - acc: 1.0000 - val_loss: 1.4703 - val_acc: 0.8045\n",
      "Epoch 168/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5584e-04 - acc: 1.0000 - val_loss: 1.4201 - val_acc: 0.8125\n",
      "Epoch 169/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1792e-05 - acc: 1.0000 - val_loss: 1.4988 - val_acc: 0.8077\n",
      "Epoch 170/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.3519e-05 - acc: 1.0000 - val_loss: 1.4910 - val_acc: 0.8093\n",
      "Epoch 171/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7996e-05 - acc: 1.0000 - val_loss: 1.6262 - val_acc: 0.7997\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3273e-05 - acc: 1.0000 - val_loss: 1.4325 - val_acc: 0.8157\n",
      "Epoch 173/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8458e-05 - acc: 1.0000 - val_loss: 1.4695 - val_acc: 0.8125\n",
      "Epoch 174/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2179e-05 - acc: 1.0000 - val_loss: 1.5310 - val_acc: 0.8093\n",
      "Epoch 175/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9130e-05 - acc: 1.0000 - val_loss: 1.4903 - val_acc: 0.8109\n",
      "Epoch 176/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5654e-05 - acc: 1.0000 - val_loss: 1.5384 - val_acc: 0.8125\n",
      "Epoch 177/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8208e-05 - acc: 1.0000 - val_loss: 1.8324 - val_acc: 0.7901\n",
      "Epoch 178/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6722e-05 - acc: 1.0000 - val_loss: 1.7142 - val_acc: 0.8045\n",
      "Epoch 179/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4019e-05 - acc: 1.0000 - val_loss: 1.7784 - val_acc: 0.7981\n",
      "Epoch 180/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2543e-05 - acc: 1.0000 - val_loss: 1.6278 - val_acc: 0.8109\n",
      "Epoch 181/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6730e-05 - acc: 1.0000 - val_loss: 2.3588 - val_acc: 0.7628\n",
      "Epoch 182/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8155e-05 - acc: 1.0000 - val_loss: 1.6341 - val_acc: 0.8093\n",
      "Epoch 183/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5032e-05 - acc: 1.0000 - val_loss: 1.5923 - val_acc: 0.8141\n",
      "Epoch 184/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2281e-06 - acc: 1.0000 - val_loss: 1.6074 - val_acc: 0.8109\n",
      "Epoch 185/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2454e-05 - acc: 1.0000 - val_loss: 1.6742 - val_acc: 0.8045\n",
      "Epoch 186/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3793e-05 - acc: 1.0000 - val_loss: 1.6940 - val_acc: 0.8045\n",
      "Epoch 187/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0147e-05 - acc: 1.0000 - val_loss: 1.6877 - val_acc: 0.8061\n",
      "Epoch 188/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3224e-05 - acc: 1.0000 - val_loss: 1.8296 - val_acc: 0.7981\n",
      "Epoch 189/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4058e-06 - acc: 1.0000 - val_loss: 1.7386 - val_acc: 0.8045\n",
      "Epoch 190/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3178e-06 - acc: 1.0000 - val_loss: 1.6790 - val_acc: 0.8125\n",
      "Epoch 191/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7735e-05 - acc: 1.0000 - val_loss: 1.6526 - val_acc: 0.8109\n",
      "Epoch 192/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8861e-06 - acc: 1.0000 - val_loss: 1.6844 - val_acc: 0.8093\n",
      "Epoch 193/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9242e-06 - acc: 1.0000 - val_loss: 1.7126 - val_acc: 0.8077\n",
      "Epoch 194/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3822e-06 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 0.8061\n",
      "Epoch 195/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8577e-06 - acc: 1.0000 - val_loss: 1.7696 - val_acc: 0.8061\n",
      "Epoch 196/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8458e-06 - acc: 1.0000 - val_loss: 1.9541 - val_acc: 0.7917\n",
      "Epoch 197/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5579e-06 - acc: 1.0000 - val_loss: 1.7813 - val_acc: 0.8093\n",
      "Epoch 198/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4767e-06 - acc: 1.0000 - val_loss: 1.7905 - val_acc: 0.8077\n",
      "Epoch 199/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8061e-06 - acc: 1.0000 - val_loss: 1.8506 - val_acc: 0.8029\n",
      "Epoch 200/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0203 - acc: 0.9940 - val_loss: 10.0495 - val_acc: 0.3750\n",
      "Epoch 201/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0936 - acc: 0.9728 - val_loss: 1.8741 - val_acc: 0.5737\n",
      "Epoch 202/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0131 - acc: 0.9948 - val_loss: 1.5972 - val_acc: 0.7660\n",
      "Epoch 203/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 1.7518 - val_acc: 0.7644\n",
      "Epoch 204/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 1.3118 - val_acc: 0.8061\n",
      "Epoch 205/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2517e-04 - acc: 1.0000 - val_loss: 1.5648 - val_acc: 0.7933\n",
      "Epoch 206/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4256e-04 - acc: 1.0000 - val_loss: 1.3820 - val_acc: 0.8125\n",
      "Epoch 207/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2089e-04 - acc: 1.0000 - val_loss: 1.6513 - val_acc: 0.7949\n",
      "Epoch 208/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8858e-04 - acc: 1.0000 - val_loss: 1.4866 - val_acc: 0.8077\n",
      "Epoch 209/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4462e-04 - acc: 1.0000 - val_loss: 1.6330 - val_acc: 0.7965\n",
      "Epoch 210/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6693e-04 - acc: 1.0000 - val_loss: 1.7451 - val_acc: 0.7885\n",
      "Epoch 211/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8334e-05 - acc: 1.0000 - val_loss: 1.8283 - val_acc: 0.7885\n",
      "Epoch 212/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0365e-04 - acc: 1.0000 - val_loss: 1.9042 - val_acc: 0.7788\n",
      "Epoch 213/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3729e-04 - acc: 1.0000 - val_loss: 1.8193 - val_acc: 0.7933\n",
      "Epoch 214/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7169e-04 - acc: 1.0000 - val_loss: 2.1816 - val_acc: 0.7676\n",
      "Epoch 215/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6524e-04 - acc: 1.0000 - val_loss: 2.2523 - val_acc: 0.7500\n",
      "Epoch 216/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0308 - acc: 0.9888 - val_loss: 2.2933 - val_acc: 0.7276\n",
      "Epoch 217/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0546 - acc: 0.9866 - val_loss: 2.0649 - val_acc: 0.7516\n",
      "Epoch 218/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0090 - acc: 0.9963 - val_loss: 0.9195 - val_acc: 0.8494\n",
      "Epoch 219/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0041 - acc: 0.9981 - val_loss: 0.7587 - val_acc: 0.8718\n",
      "Epoch 220/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 1.2092 - val_acc: 0.8333\n",
      "Epoch 221/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9016e-04 - acc: 0.9996 - val_loss: 2.0746 - val_acc: 0.7580\n",
      "Epoch 222/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0602e-04 - acc: 1.0000 - val_loss: 1.6815 - val_acc: 0.7949\n",
      "Epoch 223/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0042 - acc: 0.9985 - val_loss: 2.0283 - val_acc: 0.7692\n",
      "Epoch 224/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.1703 - val_acc: 0.8205\n",
      "Epoch 225/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 2.0421 - val_acc: 0.7756\n",
      "Epoch 226/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0455e-04 - acc: 1.0000 - val_loss: 2.0525 - val_acc: 0.7676\n",
      "Epoch 227/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3914e-04 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 0.7917\n",
      "Epoch 228/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1880e-05 - acc: 1.0000 - val_loss: 1.7439 - val_acc: 0.7997\n",
      "Epoch 229/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1351e-04 - acc: 1.0000 - val_loss: 1.9511 - val_acc: 0.7740\n",
      "Epoch 230/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0105e-04 - acc: 1.0000 - val_loss: 1.9527 - val_acc: 0.7885\n",
      "Epoch 231/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4371e-04 - acc: 1.0000 - val_loss: 1.7286 - val_acc: 0.8013\n",
      "Epoch 232/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7493e-05 - acc: 1.0000 - val_loss: 1.7831 - val_acc: 0.7981\n",
      "Epoch 233/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2177e-04 - acc: 1.0000 - val_loss: 1.6300 - val_acc: 0.8109\n",
      "Epoch 234/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7795e-05 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 0.8045\n",
      "Epoch 235/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2758e-05 - acc: 1.0000 - val_loss: 1.9114 - val_acc: 0.7981\n",
      "Epoch 236/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7214e-05 - acc: 1.0000 - val_loss: 1.8663 - val_acc: 0.7981\n",
      "Epoch 237/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4448e-05 - acc: 1.0000 - val_loss: 1.7471 - val_acc: 0.8093\n",
      "Epoch 238/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0521 - acc: 0.9855 - val_loss: 0.9543 - val_acc: 0.8189\n",
      "Epoch 239/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0320 - acc: 0.9899 - val_loss: 2.8410 - val_acc: 0.6971\n",
      "Epoch 240/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0068 - acc: 0.9989 - val_loss: 0.8751 - val_acc: 0.8429\n",
      "Epoch 241/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 2.3444 - val_acc: 0.7484\n",
      "Epoch 242/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0386e-04 - acc: 1.0000 - val_loss: 1.8809 - val_acc: 0.7949\n",
      "Epoch 243/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6804e-04 - acc: 1.0000 - val_loss: 2.9249 - val_acc: 0.7067\n",
      "Epoch 244/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1662e-04 - acc: 1.0000 - val_loss: 1.1612 - val_acc: 0.8269\n",
      "Epoch 245/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5888e-04 - acc: 1.0000 - val_loss: 1.9721 - val_acc: 0.7933\n",
      "Epoch 246/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8884e-04 - acc: 1.0000 - val_loss: 1.8113 - val_acc: 0.8061\n",
      "Epoch 247/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7826e-05 - acc: 1.0000 - val_loss: 1.7922 - val_acc: 0.8077\n",
      "Epoch 248/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5194e-05 - acc: 1.0000 - val_loss: 1.7618 - val_acc: 0.8093\n",
      "Epoch 249/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 2.6484 - val_acc: 0.6971\n",
      "Epoch 250/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.9849 - val_acc: 0.8349\n",
      "Epoch 251/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 1.4523 - val_acc: 0.8285\n",
      "Epoch 252/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7086e-04 - acc: 1.0000 - val_loss: 1.8879 - val_acc: 0.8029\n",
      "Epoch 253/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 3.6849 - val_acc: 0.7131\n",
      "Epoch 254/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 2.5224 - val_acc: 0.7468\n",
      "Epoch 255/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0359 - acc: 0.9873 - val_loss: 1.1107 - val_acc: 0.8109\n",
      "Epoch 256/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0034 - acc: 0.9993 - val_loss: 1.3297 - val_acc: 0.8269\n",
      "Epoch 257/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 0.9431 - val_acc: 0.8365\n",
      "Epoch 258/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0162 - acc: 0.9944 - val_loss: 1.0029 - val_acc: 0.8429\n",
      "Epoch 259/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 1.4992 - val_acc: 0.8285\n",
      "Epoch 260/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3534e-04 - acc: 1.0000 - val_loss: 1.6076 - val_acc: 0.8205\n",
      "Epoch 261/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0916e-04 - acc: 1.0000 - val_loss: 1.5814 - val_acc: 0.8237\n",
      "Epoch 262/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0441e-04 - acc: 1.0000 - val_loss: 2.1314 - val_acc: 0.7788\n",
      "Epoch 263/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1346e-04 - acc: 1.0000 - val_loss: 1.7658 - val_acc: 0.8077\n",
      "Epoch 264/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0302e-04 - acc: 1.0000 - val_loss: 1.7011 - val_acc: 0.8109\n",
      "Epoch 265/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2948e-05 - acc: 1.0000 - val_loss: 1.6598 - val_acc: 0.8157\n",
      "Epoch 266/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9678e-05 - acc: 1.0000 - val_loss: 1.7831 - val_acc: 0.8045\n",
      "Epoch 267/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1636e-05 - acc: 1.0000 - val_loss: 1.7079 - val_acc: 0.8109\n",
      "Epoch 268/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8929e-05 - acc: 1.0000 - val_loss: 1.6833 - val_acc: 0.8141\n",
      "Epoch 269/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8596e-05 - acc: 1.0000 - val_loss: 1.6888 - val_acc: 0.8141\n",
      "Epoch 270/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4320e-05 - acc: 1.0000 - val_loss: 1.6934 - val_acc: 0.8125\n",
      "Epoch 271/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6154e-05 - acc: 1.0000 - val_loss: 1.7170 - val_acc: 0.8141\n",
      "Epoch 272/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1195e-05 - acc: 1.0000 - val_loss: 1.8252 - val_acc: 0.8045\n",
      "Epoch 273/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9022e-05 - acc: 1.0000 - val_loss: 1.8022 - val_acc: 0.8061\n",
      "Epoch 274/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7283e-05 - acc: 1.0000 - val_loss: 1.7140 - val_acc: 0.8157\n",
      "Epoch 275/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0283e-05 - acc: 1.0000 - val_loss: 1.6866 - val_acc: 0.8205\n",
      "Epoch 276/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8343e-05 - acc: 1.0000 - val_loss: 1.6895 - val_acc: 0.8221\n",
      "Epoch 277/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2003e-05 - acc: 1.0000 - val_loss: 1.6851 - val_acc: 0.8221\n",
      "Epoch 278/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5039e-05 - acc: 1.0000 - val_loss: 1.7286 - val_acc: 0.8173\n",
      "Epoch 279/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2990e-05 - acc: 1.0000 - val_loss: 1.7987 - val_acc: 0.8125\n",
      "Epoch 280/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2703e-05 - acc: 1.0000 - val_loss: 1.7664 - val_acc: 0.8157\n",
      "Epoch 281/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0644e-05 - acc: 1.0000 - val_loss: 1.7455 - val_acc: 0.8141\n",
      "Epoch 282/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2328e-05 - acc: 1.0000 - val_loss: 1.7029 - val_acc: 0.8205\n",
      "Epoch 283/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6996e-06 - acc: 1.0000 - val_loss: 1.7519 - val_acc: 0.8173\n",
      "Epoch 284/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1059e-05 - acc: 1.0000 - val_loss: 1.7647 - val_acc: 0.8189\n",
      "Epoch 285/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9846e-06 - acc: 1.0000 - val_loss: 1.7655 - val_acc: 0.8189\n",
      "Epoch 286/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9675e-06 - acc: 1.0000 - val_loss: 1.7298 - val_acc: 0.8237\n",
      "Epoch 287/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6876e-06 - acc: 1.0000 - val_loss: 1.7704 - val_acc: 0.8189\n",
      "Epoch 288/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4528e-05 - acc: 1.0000 - val_loss: 1.8724 - val_acc: 0.8109\n",
      "Epoch 289/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7703e-06 - acc: 1.0000 - val_loss: 1.8146 - val_acc: 0.8141\n",
      "Epoch 290/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.3936e-06 - acc: 1.0000 - val_loss: 1.7387 - val_acc: 0.8221\n",
      "Epoch 291/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9903e-06 - acc: 1.0000 - val_loss: 1.8289 - val_acc: 0.8173\n",
      "Epoch 292/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3256e-06 - acc: 1.0000 - val_loss: 1.7834 - val_acc: 0.8173\n",
      "Epoch 293/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0241 - acc: 0.9937 - val_loss: 1.9102 - val_acc: 0.6971\n",
      "Epoch 294/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0280 - acc: 0.9903 - val_loss: 2.0393 - val_acc: 0.7821\n",
      "Epoch 295/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0038 - acc: 0.9978 - val_loss: 0.8918 - val_acc: 0.8349\n",
      "Epoch 296/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9989 - val_loss: 1.7695 - val_acc: 0.8013\n",
      "Epoch 297/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2896e-04 - acc: 1.0000 - val_loss: 1.4951 - val_acc: 0.8205\n",
      "Epoch 298/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2239e-05 - acc: 1.0000 - val_loss: 1.7815 - val_acc: 0.8029\n",
      "Epoch 299/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5187e-04 - acc: 1.0000 - val_loss: 1.9773 - val_acc: 0.7885\n",
      "Epoch 300/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5127e-04 - acc: 1.0000 - val_loss: 1.8059 - val_acc: 0.8077\n",
      "Epoch 301/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.7071e-05 - acc: 1.0000 - val_loss: 1.9201 - val_acc: 0.7997\n",
      "Epoch 302/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9438e-05 - acc: 1.0000 - val_loss: 1.7458 - val_acc: 0.8061\n",
      "Epoch 303/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7892e-05 - acc: 1.0000 - val_loss: 1.7993 - val_acc: 0.8061\n",
      "Epoch 304/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6396e-05 - acc: 1.0000 - val_loss: 1.8013 - val_acc: 0.8077\n",
      "Epoch 305/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7365e-05 - acc: 1.0000 - val_loss: 1.9143 - val_acc: 0.7997\n",
      "Epoch 306/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2993e-04 - acc: 1.0000 - val_loss: 1.9163 - val_acc: 0.7981\n",
      "Epoch 307/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6221e-05 - acc: 1.0000 - val_loss: 1.8492 - val_acc: 0.8013\n",
      "Epoch 308/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2222e-05 - acc: 1.0000 - val_loss: 1.7967 - val_acc: 0.8061\n",
      "Epoch 309/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0743e-04 - acc: 1.0000 - val_loss: 1.9264 - val_acc: 0.8013\n",
      "Epoch 310/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3355e-05 - acc: 1.0000 - val_loss: 1.8428 - val_acc: 0.8061\n",
      "Epoch 311/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5499e-05 - acc: 1.0000 - val_loss: 1.7224 - val_acc: 0.8061\n",
      "Epoch 312/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8957e-05 - acc: 1.0000 - val_loss: 1.8266 - val_acc: 0.8045\n",
      "Epoch 313/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8177e-05 - acc: 1.0000 - val_loss: 1.8912 - val_acc: 0.8029\n",
      "Epoch 314/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4369e-05 - acc: 1.0000 - val_loss: 2.0689 - val_acc: 0.7949\n",
      "Epoch 315/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8252e-05 - acc: 1.0000 - val_loss: 1.9626 - val_acc: 0.7997\n",
      "Epoch 316/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8922e-04 - acc: 1.0000 - val_loss: 1.8007 - val_acc: 0.8013\n",
      "Epoch 317/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7814e-05 - acc: 1.0000 - val_loss: 1.9737 - val_acc: 0.8045\n",
      "Epoch 318/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3041e-05 - acc: 1.0000 - val_loss: 1.4102 - val_acc: 0.8333\n",
      "Epoch 319/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4293e-05 - acc: 1.0000 - val_loss: 1.7772 - val_acc: 0.8125\n",
      "Epoch 320/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1379e-05 - acc: 1.0000 - val_loss: 1.8301 - val_acc: 0.8109\n",
      "Epoch 321/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0342e-05 - acc: 1.0000 - val_loss: 1.8519 - val_acc: 0.8109\n",
      "Epoch 322/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0263e-06 - acc: 1.0000 - val_loss: 1.9301 - val_acc: 0.8061\n",
      "Epoch 323/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1989e-06 - acc: 1.0000 - val_loss: 1.9078 - val_acc: 0.8061\n",
      "Epoch 324/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4536e-06 - acc: 1.0000 - val_loss: 1.8890 - val_acc: 0.8077\n",
      "Epoch 325/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5768e-06 - acc: 1.0000 - val_loss: 1.8193 - val_acc: 0.8157\n",
      "Epoch 326/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8164e-06 - acc: 1.0000 - val_loss: 1.8793 - val_acc: 0.8093\n",
      "Epoch 327/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8152e-06 - acc: 1.0000 - val_loss: 1.8541 - val_acc: 0.8141\n",
      "Epoch 328/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8906e-05 - acc: 1.0000 - val_loss: 1.8663 - val_acc: 0.8077\n",
      "Epoch 329/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0949e-06 - acc: 1.0000 - val_loss: 1.8715 - val_acc: 0.8061\n",
      "Epoch 330/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3056e-06 - acc: 1.0000 - val_loss: 1.9172 - val_acc: 0.8077\n",
      "Epoch 331/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4108e-06 - acc: 1.0000 - val_loss: 1.9357 - val_acc: 0.8045\n",
      "Epoch 332/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7211e-06 - acc: 1.0000 - val_loss: 1.9103 - val_acc: 0.8061\n",
      "Epoch 333/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8776e-05 - acc: 1.0000 - val_loss: 1.8557 - val_acc: 0.8109\n",
      "Epoch 334/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9013e-06 - acc: 1.0000 - val_loss: 1.9231 - val_acc: 0.8029\n",
      "Epoch 335/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0273 - acc: 0.9925 - val_loss: 1.4874 - val_acc: 0.7660\n",
      "Epoch 336/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0312 - acc: 0.9903 - val_loss: 2.0210 - val_acc: 0.7917\n",
      "Epoch 337/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 1.7885 - val_acc: 0.8045\n",
      "Epoch 338/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0014 - acc: 0.9989 - val_loss: 1.6035 - val_acc: 0.8157\n",
      "Epoch 339/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0159e-04 - acc: 1.0000 - val_loss: 1.4232 - val_acc: 0.8157\n",
      "Epoch 340/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1039e-04 - acc: 1.0000 - val_loss: 1.5869 - val_acc: 0.8141\n",
      "Epoch 341/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7833e-04 - acc: 1.0000 - val_loss: 1.6858 - val_acc: 0.8013\n",
      "Epoch 342/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3447e-04 - acc: 1.0000 - val_loss: 1.7457 - val_acc: 0.7981\n",
      "Epoch 343/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2850e-04 - acc: 1.0000 - val_loss: 1.7087 - val_acc: 0.7997\n",
      "Epoch 344/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4369e-04 - acc: 1.0000 - val_loss: 1.4326 - val_acc: 0.8269\n",
      "Epoch 345/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5870e-05 - acc: 1.0000 - val_loss: 1.6277 - val_acc: 0.8109\n",
      "Epoch 346/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.2791e-05 - acc: 1.0000 - val_loss: 1.6227 - val_acc: 0.8125\n",
      "Epoch 347/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3229e-05 - acc: 1.0000 - val_loss: 1.6524 - val_acc: 0.8109\n",
      "Epoch 348/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0122e-05 - acc: 1.0000 - val_loss: 1.6502 - val_acc: 0.8093\n",
      "Epoch 349/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3170e-05 - acc: 1.0000 - val_loss: 1.7932 - val_acc: 0.7949\n",
      "Epoch 350/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0304e-05 - acc: 1.0000 - val_loss: 1.7271 - val_acc: 0.8013\n",
      "Epoch 351/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0718e-05 - acc: 1.0000 - val_loss: 1.6064 - val_acc: 0.8157\n",
      "Epoch 352/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4178e-05 - acc: 1.0000 - val_loss: 1.7002 - val_acc: 0.8029\n",
      "Epoch 353/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6435e-05 - acc: 1.0000 - val_loss: 1.6855 - val_acc: 0.8077\n",
      "Epoch 354/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6490e-05 - acc: 1.0000 - val_loss: 1.6875 - val_acc: 0.8093\n",
      "Epoch 355/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4798e-05 - acc: 1.0000 - val_loss: 1.6808 - val_acc: 0.8109\n",
      "Epoch 356/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6200e-05 - acc: 1.0000 - val_loss: 1.7280 - val_acc: 0.8061\n",
      "Epoch 357/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0432e-05 - acc: 1.0000 - val_loss: 1.7231 - val_acc: 0.8109\n",
      "Epoch 358/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0369e-05 - acc: 1.0000 - val_loss: 1.7025 - val_acc: 0.8157\n",
      "Epoch 359/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3262e-05 - acc: 1.0000 - val_loss: 1.7636 - val_acc: 0.8061\n",
      "Epoch 360/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0143e-05 - acc: 1.0000 - val_loss: 1.7510 - val_acc: 0.8109\n",
      "Epoch 361/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6805e-06 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 0.8093\n",
      "Epoch 362/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2507e-06 - acc: 1.0000 - val_loss: 1.8053 - val_acc: 0.8029\n",
      "Epoch 363/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0589e-05 - acc: 1.0000 - val_loss: 1.7281 - val_acc: 0.8157\n",
      "Epoch 364/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0583 - acc: 0.9873 - val_loss: 1.1100 - val_acc: 0.8269\n",
      "Epoch 365/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0075 - acc: 0.9974 - val_loss: 1.8792 - val_acc: 0.8029\n",
      "Epoch 366/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 1.7790 - val_acc: 0.7965\n",
      "Epoch 367/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 1.5089 - val_acc: 0.8061\n",
      "Epoch 368/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8344e-04 - acc: 1.0000 - val_loss: 1.4759 - val_acc: 0.8109\n",
      "Epoch 369/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3842e-04 - acc: 1.0000 - val_loss: 1.6222 - val_acc: 0.8077\n",
      "Epoch 370/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1297e-04 - acc: 1.0000 - val_loss: 1.7161 - val_acc: 0.7965\n",
      "Epoch 371/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7308e-05 - acc: 1.0000 - val_loss: 1.6682 - val_acc: 0.8013\n",
      "Epoch 372/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4745e-04 - acc: 1.0000 - val_loss: 1.6941 - val_acc: 0.8013\n",
      "Epoch 373/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0679e-04 - acc: 1.0000 - val_loss: 1.6381 - val_acc: 0.8093\n",
      "Epoch 374/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5064e-05 - acc: 1.0000 - val_loss: 1.6645 - val_acc: 0.8109\n",
      "Epoch 375/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8284e-05 - acc: 1.0000 - val_loss: 1.7199 - val_acc: 0.7981\n",
      "Epoch 376/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2895e-05 - acc: 1.0000 - val_loss: 1.7228 - val_acc: 0.8013\n",
      "Epoch 377/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7428e-05 - acc: 1.0000 - val_loss: 1.8414 - val_acc: 0.7949\n",
      "Epoch 378/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1384e-05 - acc: 1.0000 - val_loss: 1.7410 - val_acc: 0.8013\n",
      "Epoch 379/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5398e-05 - acc: 1.0000 - val_loss: 1.7629 - val_acc: 0.7981\n",
      "Epoch 380/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6315e-05 - acc: 1.0000 - val_loss: 1.8381 - val_acc: 0.7965\n",
      "Epoch 381/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5024e-05 - acc: 1.0000 - val_loss: 1.8096 - val_acc: 0.7997\n",
      "Epoch 382/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2330e-05 - acc: 1.0000 - val_loss: 1.7577 - val_acc: 0.8045\n",
      "Epoch 383/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3755e-05 - acc: 1.0000 - val_loss: 1.7615 - val_acc: 0.8045\n",
      "Epoch 384/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3919e-05 - acc: 1.0000 - val_loss: 1.9076 - val_acc: 0.7965\n",
      "Epoch 385/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6530e-05 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 0.8029\n",
      "Epoch 386/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.0401e-05 - acc: 1.0000 - val_loss: 1.9205 - val_acc: 0.7965\n",
      "Epoch 387/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0304e-05 - acc: 1.0000 - val_loss: 1.9114 - val_acc: 0.7933\n",
      "Epoch 388/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6451e-06 - acc: 1.0000 - val_loss: 1.8956 - val_acc: 0.7949\n",
      "Epoch 389/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4727e-05 - acc: 1.0000 - val_loss: 2.2429 - val_acc: 0.7756\n",
      "Epoch 390/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0506 - acc: 0.9888 - val_loss: 0.8079 - val_acc: 0.8221\n",
      "Epoch 391/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0039 - acc: 0.9978 - val_loss: 1.5136 - val_acc: 0.7917\n",
      "Epoch 392/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 1.4786 - val_acc: 0.8157\n",
      "Epoch 393/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8157e-04 - acc: 1.0000 - val_loss: 1.5066 - val_acc: 0.8109\n",
      "Epoch 394/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4258e-04 - acc: 1.0000 - val_loss: 1.7192 - val_acc: 0.7965\n",
      "Epoch 395/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6387e-04 - acc: 1.0000 - val_loss: 1.5996 - val_acc: 0.8141\n",
      "Epoch 396/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3106e-04 - acc: 1.0000 - val_loss: 1.2791 - val_acc: 0.8317\n",
      "Epoch 397/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3462e-04 - acc: 1.0000 - val_loss: 1.7509 - val_acc: 0.8061\n",
      "Epoch 398/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6156e-05 - acc: 1.0000 - val_loss: 1.6651 - val_acc: 0.8093\n",
      "Epoch 399/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5863e-05 - acc: 1.0000 - val_loss: 1.7360 - val_acc: 0.8077\n",
      "Epoch 400/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6289e-05 - acc: 1.0000 - val_loss: 1.7244 - val_acc: 0.8093\n",
      "Epoch 401/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0092e-05 - acc: 1.0000 - val_loss: 1.7157 - val_acc: 0.8109\n",
      "Epoch 402/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9260e-05 - acc: 1.0000 - val_loss: 1.7286 - val_acc: 0.8093\n",
      "Epoch 403/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9189e-05 - acc: 1.0000 - val_loss: 1.7303 - val_acc: 0.8109\n",
      "Epoch 404/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0107 - acc: 0.9966 - val_loss: 2.6595 - val_acc: 0.7131\n",
      "Epoch 405/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0117 - acc: 0.9966 - val_loss: 3.0173 - val_acc: 0.6138\n",
      "Epoch 406/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 1.6814 - val_acc: 0.8285\n",
      "Epoch 407/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6685e-04 - acc: 0.9996 - val_loss: 2.3647 - val_acc: 0.7724\n",
      "Epoch 408/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7288e-04 - acc: 1.0000 - val_loss: 2.6687 - val_acc: 0.7532\n",
      "Epoch 409/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2796e-04 - acc: 1.0000 - val_loss: 2.0742 - val_acc: 0.7901\n",
      "Epoch 410/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4375e-04 - acc: 1.0000 - val_loss: 1.7708 - val_acc: 0.8157\n",
      "Epoch 411/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4993e-05 - acc: 1.0000 - val_loss: 1.9003 - val_acc: 0.8125\n",
      "Epoch 412/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0306e-04 - acc: 1.0000 - val_loss: 1.9841 - val_acc: 0.7997\n",
      "Epoch 413/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8363e-05 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 0.8013\n",
      "Epoch 414/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4866e-05 - acc: 1.0000 - val_loss: 1.8860 - val_acc: 0.8125\n",
      "Epoch 415/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1274e-05 - acc: 1.0000 - val_loss: 1.8845 - val_acc: 0.8125\n",
      "Epoch 416/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8398e-05 - acc: 1.0000 - val_loss: 1.7693 - val_acc: 0.8189\n",
      "Epoch 417/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7611e-05 - acc: 1.0000 - val_loss: 1.8355 - val_acc: 0.8157\n",
      "Epoch 418/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3583e-06 - acc: 1.0000 - val_loss: 1.8422 - val_acc: 0.8173\n",
      "Epoch 419/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8588e-04 - acc: 1.0000 - val_loss: 2.2480 - val_acc: 0.7853\n",
      "Epoch 420/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6129e-04 - acc: 1.0000 - val_loss: 1.9573 - val_acc: 0.8077\n",
      "Epoch 421/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0147 - acc: 0.9963 - val_loss: 2.2273 - val_acc: 0.7612\n",
      "Epoch 422/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0127 - acc: 0.9955 - val_loss: 1.3389 - val_acc: 0.8285\n",
      "Epoch 423/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0086 - acc: 0.9981 - val_loss: 1.3666 - val_acc: 0.8301\n",
      "Epoch 424/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4277e-04 - acc: 1.0000 - val_loss: 1.8779 - val_acc: 0.8061\n",
      "Epoch 425/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0195e-04 - acc: 1.0000 - val_loss: 1.9787 - val_acc: 0.7949\n",
      "Epoch 426/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7382e-04 - acc: 1.0000 - val_loss: 1.9634 - val_acc: 0.7901\n",
      "Epoch 427/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2622e-05 - acc: 1.0000 - val_loss: 1.9227 - val_acc: 0.7933\n",
      "Epoch 428/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7183e-05 - acc: 1.0000 - val_loss: 1.8913 - val_acc: 0.7965\n",
      "Epoch 429/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0671e-05 - acc: 1.0000 - val_loss: 1.9706 - val_acc: 0.7917\n",
      "Epoch 430/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8911e-05 - acc: 1.0000 - val_loss: 1.8624 - val_acc: 0.7997\n",
      "Epoch 431/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5148e-05 - acc: 1.0000 - val_loss: 1.8213 - val_acc: 0.8029\n",
      "Epoch 432/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0715e-05 - acc: 1.0000 - val_loss: 1.9226 - val_acc: 0.7997\n",
      "Epoch 433/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7793e-05 - acc: 1.0000 - val_loss: 1.9090 - val_acc: 0.7981\n",
      "Epoch 434/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7348e-05 - acc: 1.0000 - val_loss: 1.9070 - val_acc: 0.8013\n",
      "Epoch 435/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2274e-05 - acc: 1.0000 - val_loss: 1.9119 - val_acc: 0.8013\n",
      "Epoch 436/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3152e-05 - acc: 1.0000 - val_loss: 1.9074 - val_acc: 0.8013\n",
      "Epoch 437/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7058e-05 - acc: 1.0000 - val_loss: 1.9629 - val_acc: 0.7997\n",
      "Epoch 438/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9886e-05 - acc: 1.0000 - val_loss: 1.9432 - val_acc: 0.7997\n",
      "Epoch 439/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8127e-05 - acc: 1.0000 - val_loss: 2.0229 - val_acc: 0.7933\n",
      "Epoch 440/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1377e-05 - acc: 1.0000 - val_loss: 1.9374 - val_acc: 0.7997\n",
      "Epoch 441/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1773e-05 - acc: 1.0000 - val_loss: 1.9179 - val_acc: 0.8029\n",
      "Epoch 442/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0617e-05 - acc: 1.0000 - val_loss: 1.9310 - val_acc: 0.8013\n",
      "Epoch 443/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3582e-05 - acc: 1.0000 - val_loss: 1.9261 - val_acc: 0.8013\n",
      "Epoch 444/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1312e-05 - acc: 1.0000 - val_loss: 2.0041 - val_acc: 0.7981\n",
      "Epoch 445/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1732e-06 - acc: 1.0000 - val_loss: 1.9635 - val_acc: 0.7997\n",
      "Epoch 446/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6951e-05 - acc: 1.0000 - val_loss: 1.8105 - val_acc: 0.8141\n",
      "Epoch 447/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8766e-06 - acc: 1.0000 - val_loss: 1.8968 - val_acc: 0.8093\n",
      "Epoch 448/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1084e-06 - acc: 1.0000 - val_loss: 1.9027 - val_acc: 0.8093\n",
      "Epoch 449/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6279e-06 - acc: 1.0000 - val_loss: 1.9146 - val_acc: 0.8093\n",
      "Epoch 450/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0413e-06 - acc: 1.0000 - val_loss: 1.9584 - val_acc: 0.8029\n",
      "Epoch 451/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2971e-06 - acc: 1.0000 - val_loss: 1.9946 - val_acc: 0.8013\n",
      "Epoch 452/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2112e-06 - acc: 1.0000 - val_loss: 2.0354 - val_acc: 0.8013\n",
      "Epoch 453/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0435e-06 - acc: 1.0000 - val_loss: 2.0348 - val_acc: 0.8029\n",
      "Epoch 454/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7812e-06 - acc: 1.0000 - val_loss: 2.0439 - val_acc: 0.7981\n",
      "Epoch 455/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0701e-06 - acc: 1.0000 - val_loss: 2.0254 - val_acc: 0.8013\n",
      "Epoch 456/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7224e-06 - acc: 1.0000 - val_loss: 2.0023 - val_acc: 0.8045\n",
      "Epoch 457/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3887e-06 - acc: 1.0000 - val_loss: 1.9849 - val_acc: 0.8061\n",
      "Epoch 458/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5181e-06 - acc: 1.0000 - val_loss: 1.9972 - val_acc: 0.8061\n",
      "Epoch 459/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3383e-06 - acc: 1.0000 - val_loss: 2.0309 - val_acc: 0.8029\n",
      "Epoch 460/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3604e-06 - acc: 1.0000 - val_loss: 2.1366 - val_acc: 0.7933\n",
      "Epoch 461/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2480e-06 - acc: 1.0000 - val_loss: 2.0705 - val_acc: 0.7981\n",
      "Epoch 462/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8137e-06 - acc: 1.0000 - val_loss: 2.0771 - val_acc: 0.7965\n",
      "Epoch 463/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8587e-06 - acc: 1.0000 - val_loss: 2.0626 - val_acc: 0.7997\n",
      "Epoch 464/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8205e-06 - acc: 1.0000 - val_loss: 2.0683 - val_acc: 0.7981\n",
      "Epoch 465/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9717e-06 - acc: 1.0000 - val_loss: 2.0306 - val_acc: 0.8013\n",
      "Epoch 466/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2160e-06 - acc: 1.0000 - val_loss: 2.0246 - val_acc: 0.8013\n",
      "Epoch 467/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6242e-06 - acc: 1.0000 - val_loss: 2.0710 - val_acc: 0.7997\n",
      "Epoch 468/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4366e-06 - acc: 1.0000 - val_loss: 2.0547 - val_acc: 0.7997\n",
      "Epoch 469/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5301e-06 - acc: 1.0000 - val_loss: 2.0831 - val_acc: 0.7965\n",
      "Epoch 470/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2050e-06 - acc: 1.0000 - val_loss: 2.0690 - val_acc: 0.7981\n",
      "Epoch 471/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8761e-06 - acc: 1.0000 - val_loss: 2.1589 - val_acc: 0.7949\n",
      "Epoch 472/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8647e-06 - acc: 1.0000 - val_loss: 2.1066 - val_acc: 0.8029\n",
      "Epoch 473/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2824e-06 - acc: 1.0000 - val_loss: 2.0307 - val_acc: 0.8045\n",
      "Epoch 474/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5249e-06 - acc: 1.0000 - val_loss: 2.1758 - val_acc: 0.7965\n",
      "Epoch 475/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0765e-07 - acc: 1.0000 - val_loss: 2.0721 - val_acc: 0.8029\n",
      "Epoch 476/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2996e-06 - acc: 1.0000 - val_loss: 2.1275 - val_acc: 0.7997\n",
      "Epoch 477/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1792e-07 - acc: 1.0000 - val_loss: 2.1167 - val_acc: 0.7997\n",
      "Epoch 478/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9756e-07 - acc: 1.0000 - val_loss: 2.1274 - val_acc: 0.7997\n",
      "Epoch 479/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3139e-07 - acc: 1.0000 - val_loss: 2.1160 - val_acc: 0.7997\n",
      "Epoch 480/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 6.8422 - val_acc: 0.5112\n",
      "Epoch 481/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0351 - acc: 0.9907 - val_loss: 1.0533 - val_acc: 0.8558\n",
      "Epoch 482/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0020 - acc: 0.9985 - val_loss: 1.5788 - val_acc: 0.8077\n",
      "Epoch 483/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0042 - acc: 0.9996 - val_loss: 2.4569 - val_acc: 0.7532\n",
      "Epoch 484/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5018e-04 - acc: 1.0000 - val_loss: 2.1031 - val_acc: 0.7917\n",
      "Epoch 485/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4947e-04 - acc: 1.0000 - val_loss: 2.2497 - val_acc: 0.7724\n",
      "Epoch 486/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4289e-05 - acc: 1.0000 - val_loss: 1.9858 - val_acc: 0.8045\n",
      "Epoch 487/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3221e-04 - acc: 1.0000 - val_loss: 2.1431 - val_acc: 0.7933\n",
      "Epoch 488/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1649e-05 - acc: 1.0000 - val_loss: 2.0121 - val_acc: 0.8045\n",
      "Epoch 489/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9691e-05 - acc: 1.0000 - val_loss: 1.9229 - val_acc: 0.8045\n",
      "Epoch 490/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7665e-05 - acc: 1.0000 - val_loss: 1.9196 - val_acc: 0.8045\n",
      "Epoch 491/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5267e-05 - acc: 1.0000 - val_loss: 1.9099 - val_acc: 0.8061\n",
      "Epoch 492/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5456e-05 - acc: 1.0000 - val_loss: 1.9311 - val_acc: 0.8061\n",
      "Epoch 493/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0104 - acc: 0.9970 - val_loss: 5.6841 - val_acc: 0.6266\n",
      "Epoch 494/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0081 - acc: 0.9963 - val_loss: 1.6696 - val_acc: 0.8093\n",
      "Epoch 495/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0028 - acc: 0.9993 - val_loss: 1.8151 - val_acc: 0.8237\n",
      "Epoch 496/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.3712e-05 - acc: 1.0000 - val_loss: 2.0795 - val_acc: 0.8061\n",
      "Epoch 497/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6949e-05 - acc: 1.0000 - val_loss: 2.1859 - val_acc: 0.7981\n",
      "Epoch 498/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6574e-05 - acc: 1.0000 - val_loss: 2.1356 - val_acc: 0.8029\n",
      "Epoch 499/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1933e-05 - acc: 1.0000 - val_loss: 2.2081 - val_acc: 0.7981\n",
      "Epoch 500/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3475e-04 - acc: 1.0000 - val_loss: 2.1824 - val_acc: 0.8013\n",
      "Epoch 501/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8184e-05 - acc: 1.0000 - val_loss: 2.2487 - val_acc: 0.7997\n",
      "Epoch 502/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4195e-05 - acc: 1.0000 - val_loss: 2.2589 - val_acc: 0.7997\n",
      "Epoch 503/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5621e-05 - acc: 1.0000 - val_loss: 2.2682 - val_acc: 0.7997\n",
      "Epoch 504/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8339e-05 - acc: 1.0000 - val_loss: 2.2390 - val_acc: 0.8013\n",
      "Epoch 505/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2315e-05 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.8061\n",
      "Epoch 506/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2260e-05 - acc: 1.0000 - val_loss: 2.2105 - val_acc: 0.8013\n",
      "Epoch 507/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8029e-05 - acc: 1.0000 - val_loss: 2.2217 - val_acc: 0.8029\n",
      "Epoch 508/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5647e-06 - acc: 1.0000 - val_loss: 2.2400 - val_acc: 0.8029\n",
      "Epoch 509/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 4.1328 - val_acc: 0.6731\n",
      "Epoch 510/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0142 - acc: 0.9955 - val_loss: 1.2338 - val_acc: 0.7885\n",
      "Epoch 511/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0082 - acc: 0.9974 - val_loss: 2.8400 - val_acc: 0.7468\n",
      "Epoch 512/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0029 - acc: 0.9989 - val_loss: 2.0918 - val_acc: 0.7965\n",
      "Epoch 513/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8138e-04 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.7885\n",
      "Epoch 514/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9093e-05 - acc: 1.0000 - val_loss: 2.0665 - val_acc: 0.7949\n",
      "Epoch 515/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6557e-05 - acc: 1.0000 - val_loss: 2.0245 - val_acc: 0.7965\n",
      "Epoch 516/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1078e-05 - acc: 1.0000 - val_loss: 2.1643 - val_acc: 0.7933\n",
      "Epoch 517/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3601e-05 - acc: 1.0000 - val_loss: 2.0701 - val_acc: 0.7965\n",
      "Epoch 518/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0713e-05 - acc: 1.0000 - val_loss: 2.0502 - val_acc: 0.7965\n",
      "Epoch 519/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2257e-04 - acc: 1.0000 - val_loss: 1.9327 - val_acc: 0.8029\n",
      "Epoch 520/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7265e-05 - acc: 1.0000 - val_loss: 1.9841 - val_acc: 0.8029\n",
      "Epoch 521/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.7875e-06 - acc: 1.0000 - val_loss: 2.0501 - val_acc: 0.8029\n",
      "Epoch 522/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5302e-04 - acc: 1.0000 - val_loss: 1.9542 - val_acc: 0.7965\n",
      "Epoch 523/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7950e-05 - acc: 1.0000 - val_loss: 1.9087 - val_acc: 0.8013\n",
      "Epoch 524/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3455e-05 - acc: 1.0000 - val_loss: 1.9278 - val_acc: 0.8013\n",
      "Epoch 525/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 2.4247 - val_acc: 0.7644\n",
      "Epoch 526/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0092 - acc: 0.9970 - val_loss: 1.3539 - val_acc: 0.8189\n",
      "Epoch 527/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 2.2176 - val_acc: 0.7804\n",
      "Epoch 528/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1023e-04 - acc: 1.0000 - val_loss: 1.9924 - val_acc: 0.7949\n",
      "Epoch 529/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5134e-04 - acc: 1.0000 - val_loss: 2.2452 - val_acc: 0.7837\n",
      "Epoch 530/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1529e-05 - acc: 1.0000 - val_loss: 1.9026 - val_acc: 0.7981\n",
      "Epoch 531/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6900e-05 - acc: 1.0000 - val_loss: 1.8903 - val_acc: 0.8029\n",
      "Epoch 532/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9708e-05 - acc: 1.0000 - val_loss: 1.8767 - val_acc: 0.8013\n",
      "Epoch 533/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3659e-05 - acc: 1.0000 - val_loss: 1.9067 - val_acc: 0.7981\n",
      "Epoch 534/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1791e-05 - acc: 1.0000 - val_loss: 1.8358 - val_acc: 0.8013\n",
      "Epoch 535/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2832e-04 - acc: 1.0000 - val_loss: 3.1255 - val_acc: 0.7356\n",
      "Epoch 536/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4926e-04 - acc: 1.0000 - val_loss: 2.0798 - val_acc: 0.7949\n",
      "Epoch 537/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9754e-05 - acc: 1.0000 - val_loss: 1.9283 - val_acc: 0.8093\n",
      "Epoch 538/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0447e-05 - acc: 1.0000 - val_loss: 2.0399 - val_acc: 0.7997\n",
      "Epoch 539/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0267e-06 - acc: 1.0000 - val_loss: 2.0124 - val_acc: 0.8013\n",
      "Epoch 540/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5761e-06 - acc: 1.0000 - val_loss: 2.0347 - val_acc: 0.8013\n",
      "Epoch 541/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3146e-05 - acc: 1.0000 - val_loss: 2.1256 - val_acc: 0.7965\n",
      "Epoch 542/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3147e-06 - acc: 1.0000 - val_loss: 2.0387 - val_acc: 0.8029\n",
      "Epoch 543/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8618e-06 - acc: 1.0000 - val_loss: 2.0215 - val_acc: 0.8045\n",
      "Epoch 544/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5983e-05 - acc: 1.0000 - val_loss: 2.0893 - val_acc: 0.7965\n",
      "Epoch 545/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5133e-06 - acc: 1.0000 - val_loss: 2.0838 - val_acc: 0.7981\n",
      "Epoch 546/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.2795e-06 - acc: 1.0000 - val_loss: 2.0864 - val_acc: 0.7981\n",
      "Epoch 547/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8840e-06 - acc: 1.0000 - val_loss: 1.9385 - val_acc: 0.8077\n",
      "Epoch 548/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3631e-06 - acc: 1.0000 - val_loss: 2.0227 - val_acc: 0.8013\n",
      "Epoch 549/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0488e-06 - acc: 1.0000 - val_loss: 1.9912 - val_acc: 0.8077\n",
      "Epoch 550/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0189e-06 - acc: 1.0000 - val_loss: 2.0424 - val_acc: 0.8013\n",
      "Epoch 551/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5895e-06 - acc: 1.0000 - val_loss: 2.0213 - val_acc: 0.8013\n",
      "Epoch 552/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4323e-06 - acc: 1.0000 - val_loss: 2.0189 - val_acc: 0.8029\n",
      "Epoch 553/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1354e-06 - acc: 1.0000 - val_loss: 2.0318 - val_acc: 0.7997\n",
      "Epoch 554/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8123e-06 - acc: 1.0000 - val_loss: 2.0372 - val_acc: 0.7997\n",
      "Epoch 555/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4337e-06 - acc: 1.0000 - val_loss: 2.0474 - val_acc: 0.7997\n",
      "Epoch 556/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0302e-06 - acc: 1.0000 - val_loss: 2.0645 - val_acc: 0.7981\n",
      "Epoch 557/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5565e-06 - acc: 1.0000 - val_loss: 2.0153 - val_acc: 0.8045\n",
      "Epoch 558/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4499e-06 - acc: 1.0000 - val_loss: 2.1329 - val_acc: 0.7949\n",
      "Epoch 559/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5405e-06 - acc: 1.0000 - val_loss: 2.1088 - val_acc: 0.7949\n",
      "Epoch 560/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1218e-06 - acc: 1.0000 - val_loss: 2.0657 - val_acc: 0.7981\n",
      "Epoch 561/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3407e-06 - acc: 1.0000 - val_loss: 2.0593 - val_acc: 0.7981\n",
      "Epoch 562/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5031e-06 - acc: 1.0000 - val_loss: 2.1587 - val_acc: 0.7933\n",
      "Epoch 563/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2660e-06 - acc: 1.0000 - val_loss: 2.1217 - val_acc: 0.7965\n",
      "Epoch 564/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0733e-06 - acc: 1.0000 - val_loss: 2.0752 - val_acc: 0.8013\n",
      "Epoch 565/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5189e-06 - acc: 1.0000 - val_loss: 2.0204 - val_acc: 0.8045\n",
      "Epoch 566/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0111e-06 - acc: 1.0000 - val_loss: 2.0356 - val_acc: 0.8045\n",
      "Epoch 567/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5712e-05 - acc: 1.0000 - val_loss: 2.6095 - val_acc: 0.7644\n",
      "Epoch 568/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0302 - acc: 0.9925 - val_loss: 1.9921 - val_acc: 0.8045\n",
      "Epoch 569/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 1.6919 - val_acc: 0.8125\n",
      "Epoch 570/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 2.0969 - val_acc: 0.7821\n",
      "Epoch 571/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2027e-04 - acc: 1.0000 - val_loss: 2.1531 - val_acc: 0.7821\n",
      "Epoch 572/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6980e-05 - acc: 1.0000 - val_loss: 1.6151 - val_acc: 0.8157\n",
      "Epoch 573/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7049e-05 - acc: 1.0000 - val_loss: 2.0276 - val_acc: 0.7869\n",
      "Epoch 574/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9532e-05 - acc: 1.0000 - val_loss: 1.8146 - val_acc: 0.8029\n",
      "Epoch 575/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3853e-05 - acc: 1.0000 - val_loss: 1.8429 - val_acc: 0.7997\n",
      "Epoch 576/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9711e-06 - acc: 1.0000 - val_loss: 1.8390 - val_acc: 0.8013\n",
      "Epoch 577/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7601e-05 - acc: 1.0000 - val_loss: 1.8935 - val_acc: 0.7949\n",
      "Epoch 578/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6252e-05 - acc: 1.0000 - val_loss: 1.8232 - val_acc: 0.8013\n",
      "Epoch 579/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3969e-05 - acc: 1.0000 - val_loss: 1.8389 - val_acc: 0.8013\n",
      "Epoch 580/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5799e-05 - acc: 1.0000 - val_loss: 1.8755 - val_acc: 0.7949\n",
      "Epoch 581/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1785e-06 - acc: 1.0000 - val_loss: 1.8864 - val_acc: 0.7949\n",
      "Epoch 582/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7462e-05 - acc: 1.0000 - val_loss: 1.9507 - val_acc: 0.7933\n",
      "Epoch 583/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4924e-05 - acc: 1.0000 - val_loss: 1.9342 - val_acc: 0.7933\n",
      "Epoch 584/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1916e-05 - acc: 1.0000 - val_loss: 1.8280 - val_acc: 0.8013\n",
      "Epoch 585/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3791e-05 - acc: 1.0000 - val_loss: 1.9059 - val_acc: 0.7949\n",
      "Epoch 586/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8324e-06 - acc: 1.0000 - val_loss: 1.8850 - val_acc: 0.7949\n",
      "Epoch 587/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3582e-06 - acc: 1.0000 - val_loss: 1.9045 - val_acc: 0.7949\n",
      "Epoch 588/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0183e-06 - acc: 1.0000 - val_loss: 1.9228 - val_acc: 0.7933\n",
      "Epoch 589/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0983e-06 - acc: 1.0000 - val_loss: 1.9384 - val_acc: 0.7933\n",
      "Epoch 590/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4791e-05 - acc: 1.0000 - val_loss: 1.9330 - val_acc: 0.7965\n",
      "Epoch 591/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3344e-06 - acc: 1.0000 - val_loss: 1.8715 - val_acc: 0.7981\n",
      "Epoch 592/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1245e-06 - acc: 1.0000 - val_loss: 2.0362 - val_acc: 0.7901\n",
      "Epoch 593/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7244e-06 - acc: 1.0000 - val_loss: 1.9562 - val_acc: 0.7949\n",
      "Epoch 594/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8537e-05 - acc: 1.0000 - val_loss: 2.1725 - val_acc: 0.7869\n",
      "Epoch 595/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0641e-06 - acc: 1.0000 - val_loss: 2.0338 - val_acc: 0.7949\n",
      "Epoch 596/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9280e-06 - acc: 1.0000 - val_loss: 1.9845 - val_acc: 0.7981\n",
      "Epoch 597/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8438e-06 - acc: 1.0000 - val_loss: 1.9881 - val_acc: 0.7981\n",
      "Epoch 598/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0018 - acc: 0.9989 - val_loss: 5.0238 - val_acc: 0.5385\n",
      "Epoch 599/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0383 - acc: 0.9911 - val_loss: 1.5255 - val_acc: 0.8157\n",
      "Epoch 600/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5941e-04 - acc: 1.0000 - val_loss: 2.0556 - val_acc: 0.7901\n",
      "Epoch 601/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6544e-04 - acc: 1.0000 - val_loss: 2.0926 - val_acc: 0.7949\n",
      "Epoch 602/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6423e-04 - acc: 0.9996 - val_loss: 1.4435 - val_acc: 0.8429\n",
      "Epoch 603/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4779e-04 - acc: 1.0000 - val_loss: 2.0618 - val_acc: 0.7901\n",
      "Epoch 604/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3912e-04 - acc: 1.0000 - val_loss: 2.4370 - val_acc: 0.7708\n",
      "Epoch 605/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3437e-05 - acc: 1.0000 - val_loss: 2.1659 - val_acc: 0.7837\n",
      "Epoch 606/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0271e-05 - acc: 1.0000 - val_loss: 2.0997 - val_acc: 0.7885\n",
      "Epoch 607/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6852e-05 - acc: 1.0000 - val_loss: 2.0748 - val_acc: 0.7885\n",
      "Epoch 608/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6149e-05 - acc: 1.0000 - val_loss: 2.0264 - val_acc: 0.7901\n",
      "Epoch 609/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3493e-05 - acc: 1.0000 - val_loss: 2.1013 - val_acc: 0.7869\n",
      "Epoch 610/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9688e-05 - acc: 1.0000 - val_loss: 2.1780 - val_acc: 0.7853\n",
      "Epoch 611/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0857e-05 - acc: 1.0000 - val_loss: 2.0988 - val_acc: 0.7869\n",
      "Epoch 612/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1329e-05 - acc: 1.0000 - val_loss: 2.2318 - val_acc: 0.7804\n",
      "Epoch 613/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8482e-05 - acc: 1.0000 - val_loss: 2.1149 - val_acc: 0.7885\n",
      "Epoch 614/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1307e-06 - acc: 1.0000 - val_loss: 2.1162 - val_acc: 0.7853\n",
      "Epoch 615/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1157e-05 - acc: 1.0000 - val_loss: 2.1802 - val_acc: 0.7821\n",
      "Epoch 616/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2488e-06 - acc: 1.0000 - val_loss: 2.1281 - val_acc: 0.7837\n",
      "Epoch 617/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1668e-05 - acc: 1.0000 - val_loss: 2.0692 - val_acc: 0.7917\n",
      "Epoch 618/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8808e-06 - acc: 1.0000 - val_loss: 2.0547 - val_acc: 0.7917\n",
      "Epoch 619/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5044e-06 - acc: 1.0000 - val_loss: 2.0968 - val_acc: 0.7901\n",
      "Epoch 620/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1959e-05 - acc: 1.0000 - val_loss: 2.0706 - val_acc: 0.7917\n",
      "Epoch 621/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4168e-06 - acc: 1.0000 - val_loss: 2.1407 - val_acc: 0.7901\n",
      "Epoch 622/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2891e-06 - acc: 1.0000 - val_loss: 2.1550 - val_acc: 0.7869\n",
      "Epoch 623/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1729e-06 - acc: 1.0000 - val_loss: 2.2007 - val_acc: 0.7837\n",
      "Epoch 624/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4768e-06 - acc: 1.0000 - val_loss: 2.1958 - val_acc: 0.7853\n",
      "Epoch 625/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8578e-06 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.7853\n",
      "Epoch 626/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6528e-06 - acc: 1.0000 - val_loss: 2.1382 - val_acc: 0.7869\n",
      "Epoch 627/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7686e-06 - acc: 1.0000 - val_loss: 2.1468 - val_acc: 0.7869\n",
      "Epoch 628/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0549e-06 - acc: 1.0000 - val_loss: 2.1654 - val_acc: 0.7869\n",
      "Epoch 629/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3732e-06 - acc: 1.0000 - val_loss: 2.1485 - val_acc: 0.7869\n",
      "Epoch 630/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8015e-06 - acc: 1.0000 - val_loss: 2.1278 - val_acc: 0.7901\n",
      "Epoch 631/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0068e-06 - acc: 1.0000 - val_loss: 2.2308 - val_acc: 0.7821\n",
      "Epoch 632/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2457e-06 - acc: 1.0000 - val_loss: 2.1840 - val_acc: 0.7821\n",
      "Epoch 633/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5600e-06 - acc: 1.0000 - val_loss: 2.1257 - val_acc: 0.7917\n",
      "Epoch 634/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0686e-06 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.7853\n",
      "Epoch 635/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3292e-06 - acc: 1.0000 - val_loss: 2.1355 - val_acc: 0.7917\n",
      "Epoch 636/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1372e-06 - acc: 1.0000 - val_loss: 2.3604 - val_acc: 0.7756\n",
      "Epoch 637/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5270e-06 - acc: 1.0000 - val_loss: 2.2885 - val_acc: 0.7772\n",
      "Epoch 638/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5365e-06 - acc: 1.0000 - val_loss: 2.2220 - val_acc: 0.7837\n",
      "Epoch 639/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7686e-06 - acc: 1.0000 - val_loss: 2.1867 - val_acc: 0.7853\n",
      "Epoch 640/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0080e-06 - acc: 1.0000 - val_loss: 2.2526 - val_acc: 0.7821\n",
      "Epoch 641/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8640e-06 - acc: 1.0000 - val_loss: 2.2768 - val_acc: 0.7821\n",
      "Epoch 642/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0300 - acc: 0.9929 - val_loss: 2.1628 - val_acc: 0.6987\n",
      "Epoch 643/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0197 - acc: 0.9940 - val_loss: 1.3076 - val_acc: 0.7837\n",
      "Epoch 644/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0061 - acc: 0.9985 - val_loss: 2.1738 - val_acc: 0.8077\n",
      "Epoch 645/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3602e-05 - acc: 1.0000 - val_loss: 1.9700 - val_acc: 0.8221\n",
      "Epoch 646/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2840e-05 - acc: 1.0000 - val_loss: 1.9083 - val_acc: 0.8237\n",
      "Epoch 647/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7495e-05 - acc: 1.0000 - val_loss: 1.9497 - val_acc: 0.8253\n",
      "Epoch 648/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7795e-04 - acc: 0.9996 - val_loss: 2.2071 - val_acc: 0.7965\n",
      "Epoch 649/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4777e-04 - acc: 1.0000 - val_loss: 1.8348 - val_acc: 0.8285\n",
      "Epoch 650/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6139e-05 - acc: 1.0000 - val_loss: 1.9268 - val_acc: 0.8157\n",
      "Epoch 651/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1221e-05 - acc: 1.0000 - val_loss: 1.9049 - val_acc: 0.8173\n",
      "Epoch 652/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0615e-05 - acc: 1.0000 - val_loss: 1.9239 - val_acc: 0.8173\n",
      "Epoch 653/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0419e-05 - acc: 1.0000 - val_loss: 1.9107 - val_acc: 0.8205\n",
      "Epoch 654/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1058e-05 - acc: 1.0000 - val_loss: 1.9241 - val_acc: 0.8173\n",
      "Epoch 655/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6344e-04 - acc: 1.0000 - val_loss: 1.8476 - val_acc: 0.8237\n",
      "Epoch 656/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2863e-05 - acc: 1.0000 - val_loss: 1.8493 - val_acc: 0.8253\n",
      "Epoch 657/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2799e-05 - acc: 1.0000 - val_loss: 1.8729 - val_acc: 0.8269\n",
      "Epoch 658/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3204e-05 - acc: 1.0000 - val_loss: 1.9108 - val_acc: 0.8269\n",
      "Epoch 659/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1382e-06 - acc: 1.0000 - val_loss: 1.9065 - val_acc: 0.8237\n",
      "Epoch 660/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0099e-05 - acc: 1.0000 - val_loss: 1.9807 - val_acc: 0.8189\n",
      "Epoch 661/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0346e-06 - acc: 1.0000 - val_loss: 1.9381 - val_acc: 0.8205\n",
      "Epoch 662/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6525e-06 - acc: 1.0000 - val_loss: 1.9264 - val_acc: 0.8237\n",
      "Epoch 663/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1275e-05 - acc: 1.0000 - val_loss: 1.9459 - val_acc: 0.8189\n",
      "Epoch 664/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9697e-06 - acc: 1.0000 - val_loss: 1.9497 - val_acc: 0.8173\n",
      "Epoch 665/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9261e-06 - acc: 1.0000 - val_loss: 1.9486 - val_acc: 0.8173\n",
      "Epoch 666/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6428e-06 - acc: 1.0000 - val_loss: 1.9387 - val_acc: 0.8205\n",
      "Epoch 667/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1403e-06 - acc: 1.0000 - val_loss: 1.9257 - val_acc: 0.8221\n",
      "Epoch 668/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3482e-06 - acc: 1.0000 - val_loss: 1.9218 - val_acc: 0.8221\n",
      "Epoch 669/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4990e-06 - acc: 1.0000 - val_loss: 1.9267 - val_acc: 0.8221\n",
      "Epoch 670/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5293e-06 - acc: 1.0000 - val_loss: 1.9627 - val_acc: 0.8205\n",
      "Epoch 671/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5974e-06 - acc: 1.0000 - val_loss: 1.9438 - val_acc: 0.8221\n",
      "Epoch 672/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1354e-06 - acc: 1.0000 - val_loss: 1.9261 - val_acc: 0.8221\n",
      "Epoch 673/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7686e-06 - acc: 1.0000 - val_loss: 1.9624 - val_acc: 0.8205\n",
      "Epoch 674/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0575e-06 - acc: 1.0000 - val_loss: 1.9291 - val_acc: 0.8221\n",
      "Epoch 675/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2420e-06 - acc: 1.0000 - val_loss: 1.9090 - val_acc: 0.8221\n",
      "Epoch 676/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2498e-06 - acc: 1.0000 - val_loss: 1.9216 - val_acc: 0.8221\n",
      "Epoch 677/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7015e-06 - acc: 1.0000 - val_loss: 1.9341 - val_acc: 0.8205\n",
      "Epoch 678/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2865e-06 - acc: 1.0000 - val_loss: 1.9540 - val_acc: 0.8221\n",
      "Epoch 679/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8650e-06 - acc: 1.0000 - val_loss: 1.9814 - val_acc: 0.8189\n",
      "Epoch 680/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7824e-06 - acc: 1.0000 - val_loss: 1.9728 - val_acc: 0.8189\n",
      "Epoch 681/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1026e-06 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 0.8205\n",
      "Epoch 682/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2589e-06 - acc: 1.0000 - val_loss: 1.9475 - val_acc: 0.8205\n",
      "Epoch 683/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3653e-06 - acc: 1.0000 - val_loss: 1.9904 - val_acc: 0.8189\n",
      "Epoch 684/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6663e-06 - acc: 1.0000 - val_loss: 1.9689 - val_acc: 0.8189\n",
      "Epoch 685/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9804e-06 - acc: 1.0000 - val_loss: 1.9320 - val_acc: 0.8189\n",
      "Epoch 686/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0032e-06 - acc: 1.0000 - val_loss: 1.9216 - val_acc: 0.8205\n",
      "Epoch 687/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1666e-06 - acc: 1.0000 - val_loss: 1.9208 - val_acc: 0.8221\n",
      "Epoch 688/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8328e-06 - acc: 1.0000 - val_loss: 1.9534 - val_acc: 0.8189\n",
      "Epoch 689/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3682e-06 - acc: 1.0000 - val_loss: 1.9336 - val_acc: 0.8205\n",
      "Epoch 690/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2624e-06 - acc: 1.0000 - val_loss: 1.9040 - val_acc: 0.8237\n",
      "Epoch 691/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1646e-06 - acc: 1.0000 - val_loss: 1.9508 - val_acc: 0.8205\n",
      "Epoch 692/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7292e-06 - acc: 1.0000 - val_loss: 1.9185 - val_acc: 0.8237\n",
      "Epoch 693/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1147e-06 - acc: 1.0000 - val_loss: 1.9373 - val_acc: 0.8205\n",
      "Epoch 694/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1072e-07 - acc: 1.0000 - val_loss: 1.9640 - val_acc: 0.8205\n",
      "Epoch 695/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8983e-07 - acc: 1.0000 - val_loss: 1.9719 - val_acc: 0.8205\n",
      "Epoch 696/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6159e-07 - acc: 1.0000 - val_loss: 1.9625 - val_acc: 0.8205\n",
      "Epoch 697/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8365e-07 - acc: 1.0000 - val_loss: 1.9457 - val_acc: 0.8221\n",
      "Epoch 698/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1277e-06 - acc: 1.0000 - val_loss: 1.9687 - val_acc: 0.8189\n",
      "Epoch 699/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4216e-07 - acc: 1.0000 - val_loss: 1.9768 - val_acc: 0.8221\n",
      "Epoch 700/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9799e-07 - acc: 1.0000 - val_loss: 1.9904 - val_acc: 0.8189\n",
      "Epoch 701/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9715e-07 - acc: 1.0000 - val_loss: 1.9692 - val_acc: 0.8221\n",
      "Epoch 702/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0923e-06 - acc: 1.0000 - val_loss: 1.9292 - val_acc: 0.8269\n",
      "Epoch 703/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4176e-07 - acc: 1.0000 - val_loss: 1.9920 - val_acc: 0.8157\n",
      "Epoch 704/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8973e-07 - acc: 1.0000 - val_loss: 1.9705 - val_acc: 0.8189\n",
      "Epoch 705/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2697e-07 - acc: 1.0000 - val_loss: 1.9694 - val_acc: 0.8221\n",
      "Epoch 706/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4731e-07 - acc: 1.0000 - val_loss: 1.9257 - val_acc: 0.8253\n",
      "Epoch 707/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2517e-07 - acc: 1.0000 - val_loss: 1.9572 - val_acc: 0.8237\n",
      "Epoch 708/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8738e-07 - acc: 1.0000 - val_loss: 1.9594 - val_acc: 0.8221\n",
      "Epoch 709/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9463e-07 - acc: 1.0000 - val_loss: 1.9926 - val_acc: 0.8221\n",
      "Epoch 710/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9068e-07 - acc: 1.0000 - val_loss: 2.0366 - val_acc: 0.8189\n",
      "Epoch 711/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5671e-07 - acc: 1.0000 - val_loss: 1.9990 - val_acc: 0.8221\n",
      "Epoch 712/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 3.6283e-07 - acc: 1.0000 - val_loss: 1.9987 - val_acc: 0.8237\n",
      "Epoch 713/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 4.4525e-07 - acc: 1.0000 - val_loss: 2.0189 - val_acc: 0.8237\n",
      "Epoch 714/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0745e-07 - acc: 1.0000 - val_loss: 2.0487 - val_acc: 0.8237\n",
      "Epoch 715/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8700e-07 - acc: 1.0000 - val_loss: 2.0192 - val_acc: 0.8237\n",
      "Epoch 716/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7411e-07 - acc: 1.0000 - val_loss: 2.0062 - val_acc: 0.8253\n",
      "Epoch 717/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3736e-07 - acc: 1.0000 - val_loss: 1.9989 - val_acc: 0.8253\n",
      "Epoch 718/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.9214e-07 - acc: 1.0000 - val_loss: 1.9947 - val_acc: 0.8269\n",
      "Epoch 719/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8308e-07 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 0.8269\n",
      "Epoch 720/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6540e-07 - acc: 1.0000 - val_loss: 1.9663 - val_acc: 0.8285\n",
      "Epoch 721/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4602e-07 - acc: 1.0000 - val_loss: 2.0157 - val_acc: 0.8237\n",
      "Epoch 722/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6253e-07 - acc: 1.0000 - val_loss: 1.9820 - val_acc: 0.8301\n",
      "Epoch 723/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1377e-07 - acc: 1.0000 - val_loss: 2.0206 - val_acc: 0.8221\n",
      "Epoch 724/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8559e-07 - acc: 1.0000 - val_loss: 2.0184 - val_acc: 0.8269\n",
      "Epoch 725/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0475e-07 - acc: 1.0000 - val_loss: 1.9992 - val_acc: 0.8285\n",
      "Epoch 726/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9819e-07 - acc: 1.0000 - val_loss: 2.0175 - val_acc: 0.8269\n",
      "Epoch 727/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0066e-07 - acc: 1.0000 - val_loss: 2.0302 - val_acc: 0.8253\n",
      "Epoch 728/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 5.2318 - val_acc: 0.6458\n",
      "Epoch 729/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0374 - acc: 0.9922 - val_loss: 1.5253 - val_acc: 0.8269\n",
      "Epoch 730/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7053e-04 - acc: 0.9996 - val_loss: 2.5930 - val_acc: 0.7676\n",
      "Epoch 731/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4395e-04 - acc: 1.0000 - val_loss: 2.2235 - val_acc: 0.7869\n",
      "Epoch 732/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3356e-04 - acc: 1.0000 - val_loss: 2.1023 - val_acc: 0.7997\n",
      "Epoch 733/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3790e-05 - acc: 1.0000 - val_loss: 2.2617 - val_acc: 0.7869\n",
      "Epoch 734/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3047e-05 - acc: 1.0000 - val_loss: 2.3071 - val_acc: 0.7869\n",
      "Epoch 735/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5836e-05 - acc: 1.0000 - val_loss: 2.1659 - val_acc: 0.7965\n",
      "Epoch 736/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8344e-05 - acc: 1.0000 - val_loss: 2.2154 - val_acc: 0.7933\n",
      "Epoch 737/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5485e-05 - acc: 1.0000 - val_loss: 2.3272 - val_acc: 0.7885\n",
      "Epoch 738/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6598e-05 - acc: 1.0000 - val_loss: 2.2544 - val_acc: 0.7901\n",
      "Epoch 739/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7928e-06 - acc: 1.0000 - val_loss: 2.3043 - val_acc: 0.7885\n",
      "Epoch 740/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4391e-04 - acc: 0.9996 - val_loss: 2.1813 - val_acc: 0.7965\n",
      "Epoch 741/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9075e-05 - acc: 1.0000 - val_loss: 2.0693 - val_acc: 0.7997\n",
      "Epoch 742/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0327e-06 - acc: 1.0000 - val_loss: 2.0970 - val_acc: 0.7981\n",
      "Epoch 743/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7058e-04 - acc: 0.9993 - val_loss: 3.1218 - val_acc: 0.7308\n",
      "Epoch 744/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9352e-04 - acc: 0.9996 - val_loss: 1.7505 - val_acc: 0.8221\n",
      "Epoch 745/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4432e-04 - acc: 1.0000 - val_loss: 2.1634 - val_acc: 0.8061\n",
      "Epoch 746/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3822e-05 - acc: 1.0000 - val_loss: 2.2656 - val_acc: 0.8029\n",
      "Epoch 747/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9169e-05 - acc: 1.0000 - val_loss: 2.2249 - val_acc: 0.8061\n",
      "Epoch 748/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2532e-05 - acc: 1.0000 - val_loss: 2.4117 - val_acc: 0.7981\n",
      "Epoch 749/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1102e-05 - acc: 1.0000 - val_loss: 2.2515 - val_acc: 0.8029\n",
      "Epoch 750/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3601e-06 - acc: 1.0000 - val_loss: 2.2565 - val_acc: 0.8029\n",
      "Epoch 751/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7454e-06 - acc: 1.0000 - val_loss: 2.2511 - val_acc: 0.8029\n",
      "Epoch 752/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5770e-06 - acc: 1.0000 - val_loss: 2.2708 - val_acc: 0.7997\n",
      "Epoch 753/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2437e-06 - acc: 1.0000 - val_loss: 2.2842 - val_acc: 0.7997\n",
      "Epoch 754/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1015e-06 - acc: 1.0000 - val_loss: 2.2852 - val_acc: 0.7997\n",
      "Epoch 755/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3192e-06 - acc: 1.0000 - val_loss: 2.2793 - val_acc: 0.7997\n",
      "Epoch 756/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6112e-06 - acc: 1.0000 - val_loss: 2.2880 - val_acc: 0.7997\n",
      "Epoch 757/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1081e-06 - acc: 1.0000 - val_loss: 2.2980 - val_acc: 0.7997\n",
      "Epoch 758/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9336e-06 - acc: 1.0000 - val_loss: 2.2867 - val_acc: 0.7997\n",
      "Epoch 759/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3134e-06 - acc: 1.0000 - val_loss: 2.3273 - val_acc: 0.8013\n",
      "Epoch 760/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.0221e-07 - acc: 1.0000 - val_loss: 2.3227 - val_acc: 0.8013\n",
      "Epoch 761/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2967e-06 - acc: 1.0000 - val_loss: 2.2870 - val_acc: 0.7997\n",
      "Epoch 762/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1018e-06 - acc: 1.0000 - val_loss: 2.2938 - val_acc: 0.7997\n",
      "Epoch 763/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9666e-06 - acc: 1.0000 - val_loss: 2.2768 - val_acc: 0.7997\n",
      "Epoch 764/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0369e-06 - acc: 1.0000 - val_loss: 2.3124 - val_acc: 0.7997\n",
      "Epoch 765/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9525e-06 - acc: 1.0000 - val_loss: 2.3178 - val_acc: 0.7997\n",
      "Epoch 766/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0896e-06 - acc: 1.0000 - val_loss: 2.3472 - val_acc: 0.8013\n",
      "Epoch 767/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1488e-06 - acc: 1.0000 - val_loss: 2.3253 - val_acc: 0.8013\n",
      "Epoch 768/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0129e-07 - acc: 1.0000 - val_loss: 2.3296 - val_acc: 0.8013\n",
      "Epoch 769/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8436e-07 - acc: 1.0000 - val_loss: 2.3162 - val_acc: 0.8013\n",
      "Epoch 770/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9855e-07 - acc: 1.0000 - val_loss: 2.3356 - val_acc: 0.8029\n",
      "Epoch 771/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1894e-07 - acc: 1.0000 - val_loss: 2.3364 - val_acc: 0.8029\n",
      "Epoch 772/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8090e-06 - acc: 1.0000 - val_loss: 2.3625 - val_acc: 0.8013\n",
      "Epoch 773/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6526e-07 - acc: 1.0000 - val_loss: 2.3340 - val_acc: 0.8029\n",
      "Epoch 774/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0402e-06 - acc: 1.0000 - val_loss: 2.3477 - val_acc: 0.8013\n",
      "Epoch 775/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0324e-07 - acc: 1.0000 - val_loss: 2.2961 - val_acc: 0.8013\n",
      "Epoch 776/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4088e-07 - acc: 1.0000 - val_loss: 2.3028 - val_acc: 0.8013\n",
      "Epoch 777/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3089e-07 - acc: 1.0000 - val_loss: 2.2992 - val_acc: 0.8013\n",
      "Epoch 778/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7788e-06 - acc: 1.0000 - val_loss: 2.3991 - val_acc: 0.7965\n",
      "Epoch 779/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4898e-06 - acc: 1.0000 - val_loss: 2.3021 - val_acc: 0.7997\n",
      "Epoch 780/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1056e-07 - acc: 1.0000 - val_loss: 2.3120 - val_acc: 0.7997\n",
      "Epoch 781/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1377e-07 - acc: 1.0000 - val_loss: 2.3072 - val_acc: 0.7997\n",
      "Epoch 782/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8004e-07 - acc: 1.0000 - val_loss: 2.3112 - val_acc: 0.8013\n",
      "Epoch 783/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2480e-07 - acc: 1.0000 - val_loss: 2.3344 - val_acc: 0.7997\n",
      "Epoch 784/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8480e-07 - acc: 1.0000 - val_loss: 2.3306 - val_acc: 0.7997\n",
      "Epoch 785/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2194e-07 - acc: 1.0000 - val_loss: 2.3236 - val_acc: 0.8013\n",
      "Epoch 786/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2437e-06 - acc: 1.0000 - val_loss: 2.4156 - val_acc: 0.7997\n",
      "Epoch 787/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3681e-07 - acc: 1.0000 - val_loss: 2.3158 - val_acc: 0.8029\n",
      "Epoch 788/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2973e-07 - acc: 1.0000 - val_loss: 2.3023 - val_acc: 0.8029\n",
      "Epoch 789/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2773e-07 - acc: 1.0000 - val_loss: 2.2984 - val_acc: 0.8029\n",
      "Epoch 790/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9800e-07 - acc: 1.0000 - val_loss: 2.3077 - val_acc: 0.8029\n",
      "Epoch 791/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6370e-07 - acc: 1.0000 - val_loss: 2.3224 - val_acc: 0.8029\n",
      "Epoch 792/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4798e-07 - acc: 1.0000 - val_loss: 2.3573 - val_acc: 0.8013\n",
      "Epoch 793/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0136e-07 - acc: 1.0000 - val_loss: 2.3543 - val_acc: 0.8013\n",
      "Epoch 794/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9413e-07 - acc: 1.0000 - val_loss: 2.3159 - val_acc: 0.8029\n",
      "Epoch 795/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1991e-07 - acc: 1.0000 - val_loss: 2.3107 - val_acc: 0.8029\n",
      "Epoch 796/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8949e-07 - acc: 1.0000 - val_loss: 2.1533 - val_acc: 0.8125\n",
      "Epoch 797/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 6.7482e-07 - acc: 1.0000 - val_loss: 2.3145 - val_acc: 0.8013\n",
      "Epoch 798/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2001e-07 - acc: 1.0000 - val_loss: 2.2850 - val_acc: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1868e-07 - acc: 1.0000 - val_loss: 2.2995 - val_acc: 0.7997\n",
      "Epoch 800/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0348e-07 - acc: 1.0000 - val_loss: 2.3139 - val_acc: 0.8013\n",
      "Epoch 801/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9444e-07 - acc: 1.0000 - val_loss: 2.3129 - val_acc: 0.7997\n",
      "Epoch 802/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.0344e-07 - acc: 1.0000 - val_loss: 2.2966 - val_acc: 0.7997\n",
      "Epoch 803/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2158e-07 - acc: 1.0000 - val_loss: 2.3894 - val_acc: 0.8013\n",
      "Epoch 804/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8436e-07 - acc: 1.0000 - val_loss: 2.3568 - val_acc: 0.8029\n",
      "Epoch 805/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8168e-07 - acc: 1.0000 - val_loss: 2.3342 - val_acc: 0.8029\n",
      "Epoch 806/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9708e-07 - acc: 1.0000 - val_loss: 2.3319 - val_acc: 0.8029\n",
      "Epoch 807/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2204e-07 - acc: 1.0000 - val_loss: 2.3205 - val_acc: 0.8013\n",
      "Epoch 808/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.7526e-07 - acc: 1.0000 - val_loss: 2.3153 - val_acc: 0.7997\n",
      "Epoch 809/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6521e-07 - acc: 1.0000 - val_loss: 2.3590 - val_acc: 0.8029\n",
      "Epoch 810/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7813e-07 - acc: 1.0000 - val_loss: 2.3458 - val_acc: 0.8029\n",
      "Epoch 811/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8313e-07 - acc: 1.0000 - val_loss: 2.3128 - val_acc: 0.7997\n",
      "Epoch 812/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 1.6426e-07 - acc: 1.0000 - val_loss: 2.3042 - val_acc: 0.8013\n",
      "Epoch 813/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6557e-07 - acc: 1.0000 - val_loss: 2.3218 - val_acc: 0.7997\n",
      "Epoch 814/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8406e-07 - acc: 1.0000 - val_loss: 2.6862 - val_acc: 0.7821\n",
      "Epoch 815/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8337e-06 - acc: 1.0000 - val_loss: 2.5406 - val_acc: 0.7885\n",
      "Epoch 816/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0146 - acc: 0.9952 - val_loss: 2.3137 - val_acc: 0.7965\n",
      "Epoch 817/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0088 - acc: 0.9955 - val_loss: 1.8090 - val_acc: 0.8205\n",
      "Epoch 818/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3727e-04 - acc: 1.0000 - val_loss: 2.4244 - val_acc: 0.8045\n",
      "Epoch 819/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7287e-05 - acc: 1.0000 - val_loss: 2.5317 - val_acc: 0.7981\n",
      "Epoch 820/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3386e-04 - acc: 0.9993 - val_loss: 2.4342 - val_acc: 0.7981\n",
      "Epoch 821/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.2153 - val_acc: 0.8141\n",
      "Epoch 822/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5172e-05 - acc: 1.0000 - val_loss: 2.5010 - val_acc: 0.7997\n",
      "Epoch 823/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4804e-04 - acc: 1.0000 - val_loss: 2.7098 - val_acc: 0.7837\n",
      "Epoch 824/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2216e-04 - acc: 1.0000 - val_loss: 2.3422 - val_acc: 0.8045\n",
      "Epoch 825/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3237e-06 - acc: 1.0000 - val_loss: 2.3407 - val_acc: 0.8061\n",
      "Epoch 826/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2481e-06 - acc: 1.0000 - val_loss: 2.3516 - val_acc: 0.8061\n",
      "Epoch 827/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 2.1399 - val_acc: 0.8141\n",
      "Epoch 828/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2531e-04 - acc: 1.0000 - val_loss: 2.5747 - val_acc: 0.7869\n",
      "Epoch 829/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5898e-05 - acc: 1.0000 - val_loss: 2.4715 - val_acc: 0.7965\n",
      "Epoch 830/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2716e-05 - acc: 1.0000 - val_loss: 2.3888 - val_acc: 0.8029\n",
      "Epoch 831/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.0823e-06 - acc: 1.0000 - val_loss: 2.3700 - val_acc: 0.8061\n",
      "Epoch 832/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4951e-05 - acc: 1.0000 - val_loss: 2.2774 - val_acc: 0.8125\n",
      "Epoch 833/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8534e-05 - acc: 1.0000 - val_loss: 2.2648 - val_acc: 0.8141\n",
      "Epoch 834/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4757e-06 - acc: 1.0000 - val_loss: 2.2803 - val_acc: 0.8109\n",
      "Epoch 835/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.4138e-05 - acc: 1.0000 - val_loss: 2.5203 - val_acc: 0.7933\n",
      "Epoch 836/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 1.7971 - val_acc: 0.8381\n",
      "Epoch 837/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4755e-04 - acc: 1.0000 - val_loss: 2.8366 - val_acc: 0.7708\n",
      "Epoch 838/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.3404e-06 - acc: 1.0000 - val_loss: 2.5777 - val_acc: 0.7901\n",
      "Epoch 839/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4973e-06 - acc: 1.0000 - val_loss: 2.4748 - val_acc: 0.7949\n",
      "Epoch 840/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5450e-05 - acc: 1.0000 - val_loss: 2.5245 - val_acc: 0.7933\n",
      "Epoch 841/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4896e-05 - acc: 1.0000 - val_loss: 2.1405 - val_acc: 0.8093\n",
      "Epoch 842/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2453e-06 - acc: 1.0000 - val_loss: 2.2511 - val_acc: 0.8077\n",
      "Epoch 843/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9076e-05 - acc: 1.0000 - val_loss: 2.6073 - val_acc: 0.7869\n",
      "Epoch 844/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9677e-06 - acc: 1.0000 - val_loss: 2.5858 - val_acc: 0.7869\n",
      "Epoch 845/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3755e-06 - acc: 1.0000 - val_loss: 2.5013 - val_acc: 0.7965\n",
      "Epoch 846/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8890e-06 - acc: 1.0000 - val_loss: 2.4943 - val_acc: 0.7965\n",
      "Epoch 847/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6360e-06 - acc: 1.0000 - val_loss: 2.3927 - val_acc: 0.7997\n",
      "Epoch 848/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1152e-06 - acc: 1.0000 - val_loss: 2.3839 - val_acc: 0.8029\n",
      "Epoch 849/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5466e-06 - acc: 1.0000 - val_loss: 2.4259 - val_acc: 0.8013\n",
      "Epoch 850/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5334e-07 - acc: 1.0000 - val_loss: 2.4484 - val_acc: 0.7997\n",
      "Epoch 851/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1377e-07 - acc: 1.0000 - val_loss: 2.4414 - val_acc: 0.7997\n",
      "Epoch 852/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3674e-06 - acc: 1.0000 - val_loss: 2.4132 - val_acc: 0.7997\n",
      "Epoch 853/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8809e-07 - acc: 1.0000 - val_loss: 2.3933 - val_acc: 0.8013\n",
      "Epoch 854/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4358e-07 - acc: 1.0000 - val_loss: 2.3949 - val_acc: 0.8013\n",
      "Epoch 855/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6024e-07 - acc: 1.0000 - val_loss: 2.4059 - val_acc: 0.7997\n",
      "Epoch 856/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9280e-07 - acc: 1.0000 - val_loss: 2.4053 - val_acc: 0.7997\n",
      "Epoch 857/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5601e-07 - acc: 1.0000 - val_loss: 2.4090 - val_acc: 0.7997\n",
      "Epoch 858/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9824e-06 - acc: 1.0000 - val_loss: 2.4873 - val_acc: 0.7965\n",
      "Epoch 859/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1754e-06 - acc: 1.0000 - val_loss: 2.7290 - val_acc: 0.7804\n",
      "Epoch 860/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4265e-06 - acc: 1.0000 - val_loss: 2.5696 - val_acc: 0.7821\n",
      "Epoch 861/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9344e-06 - acc: 1.0000 - val_loss: 2.5764 - val_acc: 0.7821\n",
      "Epoch 862/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5209e-07 - acc: 1.0000 - val_loss: 2.5209 - val_acc: 0.7869\n",
      "Epoch 863/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0637e-07 - acc: 1.0000 - val_loss: 2.5160 - val_acc: 0.7885\n",
      "Epoch 864/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2159e-06 - acc: 1.0000 - val_loss: 2.4606 - val_acc: 0.7965\n",
      "Epoch 865/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8606e-07 - acc: 1.0000 - val_loss: 2.4648 - val_acc: 0.7949\n",
      "Epoch 866/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9755e-06 - acc: 1.0000 - val_loss: 2.2932 - val_acc: 0.8045\n",
      "Epoch 867/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4715e-07 - acc: 1.0000 - val_loss: 2.2826 - val_acc: 0.8045\n",
      "Epoch 868/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6395e-07 - acc: 1.0000 - val_loss: 2.2889 - val_acc: 0.8045\n",
      "Epoch 869/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2367e-07 - acc: 1.0000 - val_loss: 2.3412 - val_acc: 0.8045\n",
      "Epoch 870/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6949e-06 - acc: 1.0000 - val_loss: 2.1353 - val_acc: 0.8189\n",
      "Epoch 871/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7767e-07 - acc: 1.0000 - val_loss: 2.2405 - val_acc: 0.8125\n",
      "Epoch 872/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5173e-07 - acc: 1.0000 - val_loss: 2.2648 - val_acc: 0.8045\n",
      "Epoch 873/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1617e-07 - acc: 1.0000 - val_loss: 2.2822 - val_acc: 0.8045\n",
      "Epoch 874/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7621e-07 - acc: 1.0000 - val_loss: 2.3078 - val_acc: 0.8029\n",
      "Epoch 875/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7855e-07 - acc: 1.0000 - val_loss: 2.4382 - val_acc: 0.7981\n",
      "Epoch 876/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2579e-07 - acc: 1.0000 - val_loss: 2.4410 - val_acc: 0.7981\n",
      "Epoch 877/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7964e-07 - acc: 1.0000 - val_loss: 2.4375 - val_acc: 0.7997\n",
      "Epoch 878/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3563e-07 - acc: 1.0000 - val_loss: 2.4402 - val_acc: 0.7997\n",
      "Epoch 879/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4628e-07 - acc: 1.0000 - val_loss: 2.4222 - val_acc: 0.7997\n",
      "Epoch 880/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8843e-06 - acc: 1.0000 - val_loss: 2.7317 - val_acc: 0.7772\n",
      "Epoch 881/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6521e-07 - acc: 1.0000 - val_loss: 2.6731 - val_acc: 0.7853\n",
      "Epoch 882/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5141e-07 - acc: 1.0000 - val_loss: 2.6185 - val_acc: 0.7901\n",
      "Epoch 883/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8177e-07 - acc: 1.0000 - val_loss: 2.5569 - val_acc: 0.7933\n",
      "Epoch 884/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2953e-07 - acc: 1.0000 - val_loss: 2.5511 - val_acc: 0.7933\n",
      "Epoch 885/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6128e-07 - acc: 1.0000 - val_loss: 2.5389 - val_acc: 0.7933\n",
      "Epoch 886/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8086e-07 - acc: 1.0000 - val_loss: 2.5260 - val_acc: 0.7965\n",
      "Epoch 887/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5535e-07 - acc: 1.0000 - val_loss: 2.5371 - val_acc: 0.7933\n",
      "Epoch 888/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6317e-07 - acc: 1.0000 - val_loss: 2.5273 - val_acc: 0.7965\n",
      "Epoch 889/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3832e-07 - acc: 1.0000 - val_loss: 2.4966 - val_acc: 0.7949\n",
      "Epoch 890/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3340e-07 - acc: 1.0000 - val_loss: 2.5000 - val_acc: 0.7965\n",
      "Epoch 891/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6041e-07 - acc: 1.0000 - val_loss: 2.4875 - val_acc: 0.7965\n",
      "Epoch 892/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7815e-07 - acc: 1.0000 - val_loss: 2.4934 - val_acc: 0.7965\n",
      "Epoch 893/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0046e-07 - acc: 1.0000 - val_loss: 2.5012 - val_acc: 0.7965\n",
      "Epoch 894/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4690e-07 - acc: 1.0000 - val_loss: 2.5068 - val_acc: 0.7965\n",
      "Epoch 895/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4692e-07 - acc: 1.0000 - val_loss: 2.5034 - val_acc: 0.7965\n",
      "Epoch 896/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3912e-07 - acc: 1.0000 - val_loss: 2.4996 - val_acc: 0.7965\n",
      "Epoch 897/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6381e-07 - acc: 1.0000 - val_loss: 2.4667 - val_acc: 0.7981\n",
      "Epoch 898/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0195e-07 - acc: 1.0000 - val_loss: 2.4392 - val_acc: 0.7981\n",
      "Epoch 899/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7119e-07 - acc: 1.0000 - val_loss: 2.4259 - val_acc: 0.8013\n",
      "Epoch 900/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6392e-07 - acc: 1.0000 - val_loss: 2.4569 - val_acc: 0.7997\n",
      "Epoch 901/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4674e-07 - acc: 1.0000 - val_loss: 2.4291 - val_acc: 0.7997\n",
      "Epoch 902/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3992e-07 - acc: 1.0000 - val_loss: 2.4389 - val_acc: 0.7997\n",
      "Epoch 903/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4157e-07 - acc: 1.0000 - val_loss: 2.4607 - val_acc: 0.7997\n",
      "Epoch 904/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3723e-07 - acc: 1.0000 - val_loss: 2.4601 - val_acc: 0.8013\n",
      "Epoch 905/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1735e-06 - acc: 1.0000 - val_loss: 2.5964 - val_acc: 0.7804\n",
      "Epoch 906/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0316 - acc: 0.9952 - val_loss: 9.6694 - val_acc: 0.3926\n",
      "Epoch 907/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 1.6332 - val_acc: 0.8157\n",
      "Epoch 908/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0055 - acc: 0.9989 - val_loss: 1.9752 - val_acc: 0.8205\n",
      "Epoch 909/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 2.2077 - val_acc: 0.8157\n",
      "Epoch 910/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5796e-05 - acc: 1.0000 - val_loss: 2.1643 - val_acc: 0.8157\n",
      "Epoch 911/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1456e-05 - acc: 1.0000 - val_loss: 1.9086 - val_acc: 0.8365\n",
      "Epoch 912/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9989 - val_loss: 2.2263 - val_acc: 0.8221\n",
      "Epoch 913/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9748e-05 - acc: 1.0000 - val_loss: 2.1856 - val_acc: 0.8189\n",
      "Epoch 914/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1099e-05 - acc: 1.0000 - val_loss: 2.2115 - val_acc: 0.8173\n",
      "Epoch 915/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3301e-04 - acc: 1.0000 - val_loss: 2.0709 - val_acc: 0.8301\n",
      "Epoch 916/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1685e-05 - acc: 1.0000 - val_loss: 2.2554 - val_acc: 0.8157\n",
      "Epoch 917/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7031e-05 - acc: 1.0000 - val_loss: 2.2184 - val_acc: 0.8221\n",
      "Epoch 918/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5809e-06 - acc: 1.0000 - val_loss: 2.2177 - val_acc: 0.8189\n",
      "Epoch 919/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5378e-06 - acc: 1.0000 - val_loss: 2.2007 - val_acc: 0.8205\n",
      "Epoch 920/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2492e-06 - acc: 1.0000 - val_loss: 2.1976 - val_acc: 0.8205\n",
      "Epoch 921/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1199e-06 - acc: 1.0000 - val_loss: 2.1860 - val_acc: 0.8205\n",
      "Epoch 922/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6641e-05 - acc: 1.0000 - val_loss: 2.1882 - val_acc: 0.8173\n",
      "Epoch 923/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6658e-06 - acc: 1.0000 - val_loss: 2.1707 - val_acc: 0.8221\n",
      "Epoch 924/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0382e-06 - acc: 1.0000 - val_loss: 2.1628 - val_acc: 0.8221\n",
      "Epoch 925/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5532e-05 - acc: 1.0000 - val_loss: 2.0340 - val_acc: 0.8317\n",
      "Epoch 926/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2152e-07 - acc: 1.0000 - val_loss: 2.0501 - val_acc: 0.8317\n",
      "Epoch 927/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4119e-06 - acc: 1.0000 - val_loss: 2.0821 - val_acc: 0.8301\n",
      "Epoch 928/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4180e-06 - acc: 1.0000 - val_loss: 2.1256 - val_acc: 0.8237\n",
      "Epoch 929/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9993 - val_loss: 2.3097 - val_acc: 0.7965\n",
      "Epoch 930/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 1.8014 - val_acc: 0.8077\n",
      "Epoch 931/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0168 - acc: 0.9966 - val_loss: 1.7395 - val_acc: 0.8429\n",
      "Epoch 932/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0141 - acc: 0.9978 - val_loss: 2.5411 - val_acc: 0.7740\n",
      "Epoch 933/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6580e-04 - acc: 1.0000 - val_loss: 2.4537 - val_acc: 0.7708\n",
      "Epoch 934/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0044 - acc: 0.9993 - val_loss: 2.3412 - val_acc: 0.7869\n",
      "Epoch 935/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9989 - val_loss: 2.1816 - val_acc: 0.7821\n",
      "Epoch 936/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9730e-04 - acc: 1.0000 - val_loss: 2.0648 - val_acc: 0.8141\n",
      "Epoch 937/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5534e-06 - acc: 1.0000 - val_loss: 2.1658 - val_acc: 0.8029\n",
      "Epoch 938/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8382e-06 - acc: 1.0000 - val_loss: 2.1890 - val_acc: 0.8029\n",
      "Epoch 939/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9805e-06 - acc: 1.0000 - val_loss: 2.2151 - val_acc: 0.7997\n",
      "Epoch 940/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7201e-05 - acc: 1.0000 - val_loss: 2.1154 - val_acc: 0.8109\n",
      "Epoch 941/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3836e-06 - acc: 1.0000 - val_loss: 2.0605 - val_acc: 0.8173\n",
      "Epoch 942/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8895e-06 - acc: 1.0000 - val_loss: 2.0766 - val_acc: 0.8157\n",
      "Epoch 943/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5665e-06 - acc: 1.0000 - val_loss: 2.0992 - val_acc: 0.8141\n",
      "Epoch 944/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.7734e-06 - acc: 1.0000 - val_loss: 2.1405 - val_acc: 0.8125\n",
      "Epoch 945/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8493e-06 - acc: 1.0000 - val_loss: 2.1665 - val_acc: 0.8093\n",
      "Epoch 946/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9902e-06 - acc: 1.0000 - val_loss: 2.1672 - val_acc: 0.8093\n",
      "Epoch 947/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9710e-05 - acc: 1.0000 - val_loss: 2.2197 - val_acc: 0.8013\n",
      "Epoch 948/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6723e-04 - acc: 1.0000 - val_loss: 2.1203 - val_acc: 0.8125\n",
      "Epoch 949/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0378e-05 - acc: 1.0000 - val_loss: 2.3479 - val_acc: 0.8029\n",
      "Epoch 950/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0090 - acc: 0.9985 - val_loss: 2.4102 - val_acc: 0.7853\n",
      "Epoch 951/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7230e-04 - acc: 0.9996 - val_loss: 3.3497 - val_acc: 0.7292\n",
      "Epoch 952/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 2.3240 - val_acc: 0.7933\n",
      "Epoch 953/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 1.7794 - val_acc: 0.8189\n",
      "Epoch 954/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7053e-05 - acc: 1.0000 - val_loss: 2.0358 - val_acc: 0.8317\n",
      "Epoch 955/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7054e-05 - acc: 1.0000 - val_loss: 2.1017 - val_acc: 0.8285\n",
      "Epoch 956/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6277e-05 - acc: 1.0000 - val_loss: 2.0359 - val_acc: 0.8285\n",
      "Epoch 957/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4212e-06 - acc: 1.0000 - val_loss: 2.0579 - val_acc: 0.8285\n",
      "Epoch 958/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3290e-06 - acc: 1.0000 - val_loss: 2.0576 - val_acc: 0.8285\n",
      "Epoch 959/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0197e-06 - acc: 1.0000 - val_loss: 2.0523 - val_acc: 0.8285\n",
      "Epoch 960/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5816e-05 - acc: 1.0000 - val_loss: 2.1515 - val_acc: 0.8221\n",
      "Epoch 961/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4572e-07 - acc: 1.0000 - val_loss: 2.1394 - val_acc: 0.8237\n",
      "Epoch 962/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4666e-06 - acc: 1.0000 - val_loss: 2.1219 - val_acc: 0.8253\n",
      "Epoch 963/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3250e-06 - acc: 1.0000 - val_loss: 2.1326 - val_acc: 0.8237\n",
      "Epoch 964/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2410e-06 - acc: 1.0000 - val_loss: 2.1506 - val_acc: 0.8237\n",
      "Epoch 965/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0730e-07 - acc: 1.0000 - val_loss: 2.1567 - val_acc: 0.8237\n",
      "Epoch 966/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0478e-06 - acc: 1.0000 - val_loss: 2.1497 - val_acc: 0.8237\n",
      "Epoch 967/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1418e-06 - acc: 1.0000 - val_loss: 2.1324 - val_acc: 0.8253\n",
      "Epoch 968/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8243e-06 - acc: 1.0000 - val_loss: 2.1497 - val_acc: 0.8237\n",
      "Epoch 969/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6424e-06 - acc: 1.0000 - val_loss: 2.1467 - val_acc: 0.8237\n",
      "Epoch 970/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0410e-06 - acc: 1.0000 - val_loss: 2.1378 - val_acc: 0.8237\n",
      "Epoch 971/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3823e-07 - acc: 1.0000 - val_loss: 2.1347 - val_acc: 0.8237\n",
      "Epoch 972/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1820e-06 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.8189\n",
      "Epoch 973/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0025e-06 - acc: 1.0000 - val_loss: 2.1656 - val_acc: 0.8205\n",
      "Epoch 974/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9651e-07 - acc: 1.0000 - val_loss: 2.1588 - val_acc: 0.8205\n",
      "Epoch 975/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6771e-06 - acc: 1.0000 - val_loss: 2.1787 - val_acc: 0.8189\n",
      "Epoch 976/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8673e-07 - acc: 1.0000 - val_loss: 2.1702 - val_acc: 0.8237\n",
      "Epoch 977/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6641e-07 - acc: 1.0000 - val_loss: 2.1640 - val_acc: 0.8237\n",
      "Epoch 978/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0137 - acc: 0.9955 - val_loss: 4.9516 - val_acc: 0.6603\n",
      "Epoch 979/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0023 - acc: 0.9989 - val_loss: 3.0763 - val_acc: 0.7580\n",
      "Epoch 980/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0032 - acc: 0.9993 - val_loss: 2.4314 - val_acc: 0.7981\n",
      "Epoch 981/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2828e-05 - acc: 1.0000 - val_loss: 2.4283 - val_acc: 0.7949\n",
      "Epoch 982/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 2.4306 - val_acc: 0.7981\n",
      "Epoch 983/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 2.5724 - val_acc: 0.7869\n",
      "Epoch 984/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8208e-05 - acc: 1.0000 - val_loss: 2.6151 - val_acc: 0.7837\n",
      "Epoch 985/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2299e-04 - acc: 1.0000 - val_loss: 1.8605 - val_acc: 0.8349\n",
      "Epoch 986/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2604e-05 - acc: 1.0000 - val_loss: 2.2433 - val_acc: 0.8093\n",
      "Epoch 987/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1757e-05 - acc: 1.0000 - val_loss: 2.2920 - val_acc: 0.8029\n",
      "Epoch 988/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7531e-06 - acc: 1.0000 - val_loss: 2.3280 - val_acc: 0.7981\n",
      "Epoch 989/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0992e-05 - acc: 1.0000 - val_loss: 2.4219 - val_acc: 0.7917\n",
      "Epoch 990/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4651e-06 - acc: 1.0000 - val_loss: 2.4132 - val_acc: 0.7901\n",
      "Epoch 991/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9248e-06 - acc: 1.0000 - val_loss: 2.3969 - val_acc: 0.7933\n",
      "Epoch 992/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9449e-06 - acc: 1.0000 - val_loss: 2.4068 - val_acc: 0.7949\n",
      "Epoch 993/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2309e-06 - acc: 1.0000 - val_loss: 2.4313 - val_acc: 0.7917\n",
      "Epoch 994/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3805e-06 - acc: 1.0000 - val_loss: 2.4360 - val_acc: 0.7901\n",
      "Epoch 995/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7901e-07 - acc: 1.0000 - val_loss: 2.4264 - val_acc: 0.7901\n",
      "Epoch 996/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7733e-06 - acc: 1.0000 - val_loss: 2.4488 - val_acc: 0.7901\n",
      "Epoch 997/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4721e-06 - acc: 1.0000 - val_loss: 2.4294 - val_acc: 0.7901\n",
      "Epoch 998/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3860e-06 - acc: 1.0000 - val_loss: 2.4528 - val_acc: 0.7917\n",
      "Epoch 999/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5425e-06 - acc: 1.0000 - val_loss: 2.4475 - val_acc: 0.7917\n",
      "Epoch 1000/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6806e-06 - acc: 1.0000 - val_loss: 2.4592 - val_acc: 0.7917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x294fa5e8550>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run again with 1000 epochs\n",
    "resmodel.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.47      0.63       234\n",
      "          1       0.75      0.99      0.86       390\n",
      "\n",
      "avg / total       0.83      0.79      0.77       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAELCAYAAADnUlzVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHttJREFUeJzt3Xm8lHXd//HX+xxAkV1QNKDcINLc0ei2RdHcSs200PunqanUnbtZtliaVppa3pmVYi64hguY+jN3XHIXN3BLEncUAdkRBT73H9d1ZKTDOdc5Z2auuYb3k8f1OHNdcy2fYeZ8zne+13dRRGBmZtXXkHcAZmarKidgM7OcOAGbmeXECdjMLCdOwGZmOXECNjPLiROwmVlOnIDNzHLiBGxmlpNOlb7AhBdnuqud/Yf5HyzJOwSrQXts2l8dPUfXLY/MnHMWPXleh6/XERVPwGZmVaXifLF3Ajaz+qJcC7Vt4gRsZvXFJWAzs5y4BGxmlpOGxrwjyMwJ2Mzqi6sgzMxy4ioIM7OcuARsZpYTl4DNzHLiErCZWU7cCsLMLCcuAZuZ5aTBdcBmZvlwCdjMLCduBWFmlhPfhDMzy4mrIMzMcuIqCDOznLgEbGaWE5eAzcxy4hKwmVlO3ArCzCwnLgGbmeXEdcBmZjlxCdjMLCcuAZuZ5cQlYDOzfKjBCdjMLBdyFYSZWU6Kk3+dgM2svrgEbGaWEydgM7OcNBToJlxxIjUzy0JtWFo6jbS6pEclPS3pWUm/TLdfKelFSZMlXSypc7pdks6VNEXSM5K2ai1UJ2AzqyuSMi+tWAyMiIjNgS2AXSUNB64EhgKbAl2Bw9L9dwMGp8so4C+tXcBVEGZWV8pVBxwRAcxPVzunS0TELSXXehQYmK7uBVyWHvewpN6S1o2IaSu7hkvAZlZX2lICljRK0uMly6gVztUo6SlgOnBHRDxS8lxn4EDg1nTTAOD1ksPfSLetlEvAZlZX2lICjojRwOgWnl8KbCGpNzBe0mcjYnL69J+B+yLi/qZLN3eKlq7vBGxmdUUN5W+GFhGzJd0D7ApMlnQysBbw3ZLd3gAGlawPBN5q6byugjCzulKum3CS1kpLvkjqCuwEvCDpMGAXYP+IWFZyyI3At9PWEMOBOS3V/4JLwGZWZ8rYEWNdYIykRpLC6jURcbOkJcCrwEPptcZFxKnALcDuwBRgIXBIaxdwAjaz+lKm/BsRzwBbNrO92byZtn44oi3XcAI2s7rirshmZjlxAjYzy0mRxoJwAjaz+lKcArATsJnVF1dBmJnlxAnYzCwnTsCrqMv+8GsmPf4APXr14RfnXQnAgnlzufDMnzNz+jT6rr0uh594Gt2692TB/Llcdu5vmDHtTTp16cK3j/4pAz61Yc6vwCph7J/O4LmJD9K9Vx9+eM4YAG667M889/iDdOrUib7rDGDkET+ma7cezJo+jTOPPZC1P/FJAD45eGP2/e4JeYZfOJXoilwpxbldWACf33F3jjrlnI9tu/W6yxm6+dacdsE1DN18a2677vJk+7WXMWj9wfz8j5dzyHE/55oL/zePkK0Khu2wK4efdNbHtg3ZbBgnnHMpP/j9pfRbdyB3jbvio+f69h/A8WdfzPFnX+zk2w5lHA+44pyAy2jwZ7dkje49P7btmUfv5/Mjdgfg8yN25+lHkoGTpr0+laGbDwNgnYHrMXP6NOa+N6u6AVtVbLjxFv/xufj0FtvS2Jh8Af3UkE2YM/PdPEKrS3WRgCXNkzS3mWWepLnVDLLI5s6eRa81+wHQa81+zJv9HgAD1xvMkw/dA8DUfz3HrOnv8N7M6XmFaTl69O5bGLrV8I/WZ02fxu9POJQ//+IoXn7u6RwjK6a6SMAR0SMiejaz9IiInis7DvjYIMc3jx1T/qjrwC77HsjC+fP41TEHcc/N1zJog8E0NjbmHZZV2Z3XX0ZjYyNbffErAPTs05eTzr+W48++iD0POpIr/3Aq7y9ckHOUBVOmOeGqIfNNOElrA6s3rUfEayvbt3SQ4wkvzmxxQOJ617P3msyZNYNea/ZjzqwZ9OjdB4Cua3TjoGNOAiAi+Nnh+9C3/yfyDNWq7LF7/sHzEx/iuyef81FprFPnLnTq3AWAgRt+mr79B/DuW68zaKOheYZaKLVQss2q1TpgSXtKegmYCtwLvAL8o8Jx1Y3Ntv0CD92dTCH10N23sNm2XwRg4fx5LPnwQwD+efuNDN5kC7qu0S23OK26XnjyESbccBWHnHg6XVb7qFzD/DmzWbZ0KQAz33mLGW+/4T/MbdTQoMxL3pSMoNbCDtLTwAjgzojYUtIOJAMRj2rxwNSqVAL+61m/4F+Tn2T+3Nn07L0me+x/GJsP/xIXnnkSs959hzXX6s+oE39Ntx49efmFSVxyzmk0NDSw7qD1OfDon9Cte4s1O3Vl/gdL8g6haq4455f8+9knWTBvDj16rcnOIw/h7vFXsuTDD+jWoxewvLnZMw/fw21/u5iGxkYaGhrYeeR32GTYdjm/gurZY9P+Hc6Kg394a+ac89JZu+aahbMk4McjYliaiLeMiGWSHo2IbbNcYFVKwJbdqpSALbtyJOAhP8qegP91Zr4JOEsd8GxJ3YH7gCslTQf822NmNamu6oBJ5rpfBBxHMv3yv4E9KhmUmVl7SdmXvLVaAo6IBQCSegI3VTwiM7MOaGysgcyaUasJWNJ3gVNJSsHLSFrPBbBBZUMzM2u7IlVBZKkDPgHYJCJmVDoYM7OOKlD+zZSA/00yxbKZWc2rtxLwT4AHJT0CLG7aGBFHVywqM7N2qrcEfAFwNzCJpA7YzKxmFSj/ZkrASyLi+IpHYmZWBrXQxTirLAl4gqRRJE3QSqsgPHitmdWcequC+O/0509KtrkZmpnVpALl35YTsKQG4ICIeKBK8ZiZdUiRSsAtdkWOiGXA2VWKxcysw4rUFTnLWBC3S9pHRfqzYmarrCJNSZSlDvh4oBuwVNIi0q7IrU1LZGaWh7pqBRERPaoRiJlZOdRAwTazTHPCSdoT+FK6ek9E3Fy5kMzM2q8WqhayyjIa2hnANsCV6aZjJH0hIn5c0cjMzNqhQPk3Uwl4d2CLtEUEksYATwJOwGZWc+qqBJzqDTT1fOtVoVjMzDqsrm7CAacDT0qaQNIC4kt8vFecmVnNKFIJuNV2wBFxNTAcGJcun4+Iv1U6MDOz9ihXRwxJgyRNkPS8pGclHbPC8ydICkn90nVJOlfSFEnPSNqqtVizVkE0ADPS/YdIGhIR92U81sysaspYAl4C/CAinpDUA5go6Y6IeE7SIOArwGsl++8GDE6XzwF/SX+uVJZWEL8FRgLPsnw84CCZpt7MrKaUK/9GxDRgWvp4nqTngQHAc8A5wI+Av5ccshdwWUQE8LCk3pLWTc/TrCwl4K8Dn46Ixa3uaWaWs4Y2ZOB0qN1RJZtGR8ToZvZbD9gSeCTtF/FmRDy9Qml7APB6yfob6bYOJeCXgc6UjAVsZlar2tIKIk22/5FwS0nqDlwPHEtSLfEzYOfmdm3uEi2dO0sCXgg8JekuPCecmdW4crZCk9SZJPleGRHjJG0KrA80lX4HAk9I2pakxDuo5PCBwFstnT9LAr4xXczMal65bsKlI0BeBDwfEb8HiIhJwNol+7wCDIuIGZJuBI6U9DeSm29zWqr/hWyD8Yxp/0swM6uuMjYD3g44EJgk6al0208j4paV7H8LSc/hKSQ1B4e0doGszdDMzApBzVbFtl1E/JPm63VL91mv5HEAR7TlGk7AZlZXCtQT2QnYzOpLXYwFIekmWmhCERF7ViQiM7MOaEs74Ly1VAL2ZJxmVjgFyr8rT8ARcW81AzEzK4cijYaWZSyIwSRDUm4MrN60PSI2qGBcZmbtUqD8m+km3CXAySSDT+xA0ratQC/RzFYljQXKwK2OBwx0jYi7AEXEqxFxCjCismGZmbWPpMxL3rKUgN+X1AC8JOlI4E1KuuKZmdWSArVCy1QCPhZYAzga2Jqka95BlQzKzKy96qoEHBGPpQ/nk6Fvs5lZnmogr2aWpRXEBJrpkBERrgc2s5pTCyXbrLLUAZ9Q8nh1YB+SQYnNzGpOY4EqgbNUQUxcYdMDktxJw8xqUnHSb7YqiDVLVhtIbsStU7GIzMw6oF7GgmgykaQOWCRVD1OBQysZlJlZexUo/2ZKwJ+JiPdLN0harULxmJl1SJFuwmVpB/xgM9seKncgZmblIGVf8tbSeMDrkMxp31XSliyv2+5J0jHDzKzm1EsriF2Ag0mmVv4dyxPwXOCnWS/w+Q37tjc2q2N9tjky7xCsBi168rwOn6NIVRAtjQc8BhgjaZ+IuL6KMZmZtVuWetVakSXWrSX1blqR1EfSryoYk5lZuxVpLIgsCXi3iJjdtBIR7wG7Vy4kM7P2a1D2JW9ZmqE1SlotIhYDSOoKuBmamdWkerkJ1+QK4C5Jl5B0yPgOcFlFozIza6cC5d9MY0GcKekZYCeSlhCnRcRtFY/MzKwdaqBqN7MsJWAi4lbgVgBJ20n6U0QcUdHIzMzaod7GgkDSFsD+wEiSsSDGVTIoM7P2KlIztJZ6wg0B9iNJvDOBsSQTc+5QpdjMzNqsQAXgFkvALwD3A3tExBQAScdVJSozs3YqUiuIlkrr+wBvAxMkXShpR4o11rGZrYKK1A54pQk4IsZHxEhgKHAPcBzQX9JfJO1cpfjMzNqkQcq85K3V+uqIWBARV0bE10gG5nkK+HHFIzMza4ciDUfZphuGETErIi7wjMhmVquKVAWRqRmamVlRqEC3qpyAzayudCpQQ+AChWpm1rpyDkcp6WJJ0yVNXmH7UZJelPSspDNLtv9E0pT0uV1aO79LwGZWV8pct3spcB4lA5BJ2gHYC9gsIhZLWjvdvjFJ57VNgE8Ad0oaEhFLVxprWUM1M8tZOVtBRMR9wKwVNv8PcEbTEL0RMT3dvhfwt4hYHBFTgSnAti2d3wnYzOpKFdoBDwG+KOkRSfdK2ibdPgB4vWS/N9JtK+UqCDOrK41tKFZKGgWMKtk0OiJGt3JYJ6APMBzYBrhG0gY031M4WjuRmVndaGhDM7Q02baWcFf0BjAuIgJ4VNIyoF+6fVDJfgOBt1qO1cysjlShJ9wNwIjkWhoCdAFmADcC+0laTdL6wGDg0ZZO5BKwmdWVcraCkHQ1sD3QT9IbwMnAxcDFadO0D4CD0tLws5KuAZ4DlgBHtNQCApyAzazOlHOQnYjYfyVPHbCS/X8N/Drr+Z2Azayu1MIgO1k5AZtZXSnSgOxOwGZWV4rUssAJ2MzqSpYxHmqFE7CZ1ZXipF8nYDOrM7Uw1VBWTsBmVlcKdA/OCdjM6ovrgM3McuJWEGZmOXEJ2MwsJ8VJv07AZlZnXAI2M8tJoxOwmVk+ipN+nYDNrM4UqADsBGxm9aUtUxLlzQnYzOqKS8BmZjmRS8BmZvlwKwgzs5wUKP86AZtZfXECNjPLieuAzcxy4vGAzcxy4hkxzMxy4ioI+w+7fWUEa3TrRmNDA42dGrn6mnF5h2RVsFqXTtx50bF06dKJTo2NjL/zSX51/i1sv+0QfnPs3jQ0iAULF3P4yZfz8uszOGCPz/Gb477OW9PnAHD+2Hu5dPxDOb+KYnEVhDXrr5eMoU+fNfMOw6po8QdL2HXUuSxY9AGdOjVw98XHc/sDz3HuT/fjm8ddwItT32HUN7/Ijw/blVEnXwHA9bc9wXG/vTbnyIvLJWAz+8iCRR8A0LlTI506NRIRRAQ9u60OQM8eXZn27pw8Q6wrBaoCbj0BSxoO/BH4DNAFaAQWRETPCsdWXwTfO/xQJLHvN0ey77dG5h2RVUlDg3jwqhPZcNBaXDD2Ph6b/CrfP/Uqxv/x+7y/+APmLnifL3/7dx/tv9eOW7DdVhsx5bXp/Ojs63njndk5Rl88Bcq/meavOw/YH3gJ6AocRpKQV0rSKEmPS3r8ogtHdzzKOjDmiqsZe914/nT+hYy9+komPv5Y3iFZlSxbFgzf7ww22uUkhn32U2y84boc9f92YO+j/sxGu/6cy//+ML/9wTcAuOW+yQz96slsO/J07n7kRS489cCcoy+eRinzkrdME4hGxBSgMSKWRsQlwA6t7D86IoZFxLBDDx9VjjgLb+21+wPQt29fRuz0FSZPeibniKza5sxfxH2Pv8Qu223MpkMG8NjkVwG47vYnGL75+gDMmrOADz5cAsDF4x5gy898Mrd4C0ttWHKWJQEvlNQFeErSmZKOA7pVOK66snDhQhYsmP/R44cefICNNhqcc1RWDf36dKdX964ArL5aZ0Z87tO8MPUdenbvykafXBuAEcOH8uLUdwBYp9/ymr2vfXlTXpz6dvWDLji14V/estyEO5Ck3vdI4DhgELBPJYOqN7NmzuS4o48AYMnSpez+1a+x3Re/lHNUVg3r9OvJhaceSGNDAw0N4vo7nuAf90/miNOu4uqzD2NZLGP23EV895SkBcT399+er355U5YsXcp7cxZyeNoywrKrgZqFzBQRFb3A+0uo7AWskPpsc2TeIVgNWvTkeR1On4+9PCdzztlmg165puuVloAlXRMR35I0Cf4ziUbEZhWNzMysPQpUAm6pCuKY9OfXqhGImVk51MVYEBExLf35avXCMTPrmOKk3wytICR9Q9JLkuZImitpnqS51QjOzKzNytgMTdJxkp6VNFnS1ZJWl7S+pEfSvDg2bSXWLlmaoZ0J7BkRvSKiZ0T0cC84M6tV5WqGJmkAcDQwLCI+S9IabD/gt8A5ETEYeA84tL2xZknA70TE8+29gJlZNUnZlww6AV0ldQLWAKYBI4Dr0ufHAF9vb6xZ2gE/LmkscAOwuGljRHg8RTOrOW25BydpFFDaXXd0RIwGiIg3JZ0NvAYsAm4HJgKzI2JJuv8bwID2xpolAfcEFgI7l2wLwAnYzGpOW3q4pcm22QFrJPUB9gLWB2YD1wK7NXeatkeZaDUBR8Qh7T25mVm1lbEV2k7A1Ih4NzmvxgH/BfSW1CktBQ8E3mrvBbK0ghgoabyk6ZLekXS9pIHtvaCZWSWVsRHEa8BwSWtIErAj8BwwAdg33ecg4O/tjTXLTbhLgBuBT5DUddyUbjMzqz1lysAR8QjJzbYngEkk+XI0cCJwvKQpQF/govaGmqUOeK10CMoml0o6tr0XNDOrpHKOchYRJwMnr7D5ZWDbcpw/Swl4hqQDJDWmywHAzHJc3Mys3BqUfclblgT8HeBbwNskbeD2TbeZmdWeAg3InqUVxGvAnlWIxcysw2phoPWsskzKuT5wFLBe6f4R4aRsZjWnQIOhZboJdwPJXb6bgGWVDcfMrGMKlH8zJeD3I+LcikdiZlYOBcrAWRLwHySdTNIPunQsiCcqFpWZWTvVxYDsJTYlmZhzBMurICJdNzOrKcVJv9kS8N7ABhHxQaWDMTPrsAJl4CztgJ8Gelc6EDOzcijXgOzVkKUE3B94QdJjfLwO2M3QzKzmFKgKOFMCXrEftJlZzaqrBBwR91YjEDOzcqiFqoWssvSEm8fyEd+7AJ2BBZ6Y08xqUb2VgHuUrkv6OmUais3MrNwKlH8ztYL4mIi4AbcBNrMaVeZZkSsqSxXEN0pWG4BhdGASOjOzyqqBzJpRllYQe5Q8XgK8QjJTqJlZzamFgdaz8qzIZlZXaqFqIasssyIPkXSXpMnp+maSTqp8aGZmbVeknnBZbsJdCPwE+BAgIp4B9qtkUGZm7VZPUxIBa0TEo/p4uX5JheIxM+uQGsirmWVJwDMkbUja8kHSviSTc5qZ1Zx6Gw/4CGA0MFTSm8BU4ICKRmVm1l7Fyb+ZWkG8DOwkqRvQEBHzKh+WmVn7FCj/ZuqIsRqwD+msyE11wRFxakUjMzNrhwLVQGSqgvg7MAeYSMl4wGZmtagWmpdllSUBD4yIXSseiZlZGRSpBJylHfCDkjateCRmZmVQV4PxAF8ADpY0laQKQkBExGYVjczMrB3qrQpit4pHYWZWJrVQss0qSzO0VyVtRVISDuCBiHii4pGZmbVDgfJvpsF4fgGMAfoC/YBLPBiPmdWsOhsLYn9gy4h4H0DSGcATwK8qGZiZWXvUWx3wK8DqwPvp+mrAvysVkJlZR9TVgOwkLR+elXQHSR3wV4B/SjoXICKOrmB8ZmZtU2cJeHy6NLmnMqGYmXVckaogFOH5NatF0qiIGJ13HFZb/LlYdbV5WnrrkFF5B2A1yZ+LVZQTsJlZTpyAzcxystKbcJJuIp2GqDkRsWdFIqpvruez5vhzsYpa6U04SV9u6cCIuLciEZmZrSLcCsLMLCdZpiQaDJwObEzSIw6AiNiggnGZmdW9LDfhLgH+AiwBdgAuAy6vZFBtJWmppKckTZZ0raQ1OnCu7SXdnD7eU9KPW9i3t6Tvt+Map0g6ob0xVkrpa69lfr/z19r/lWWTJQF3jYi7SKorXo2IU4ARlQ2rzRZFxBYR8VngA+B7pU8q0eYWHxFxY0Sc0cIuvYE2/0JWgqTGvGOoolX+/c5bhv8ryyDLh/T99MP8kqQjJe0NrF3huDrifmAjSetJel7Sn0lGbxskaWdJD0l6Ii05dQeQtKukFyT9E/hG04kkHSzpvPRxf0njJT2dLv8FnAFsmJbGzkr3+6GkxyQ9I+mXJef6maQXJd0JfLq5wCVdKulcSQ9KelnSvul2STorLfFNkjQy3b69pAmSrgImpa/5BUl/Tfe9UtJOkh6Q9JKkbdPjtk2v8WT6s9l4CqLo7/f5ku6X9C9JXyuJY5ykW9P37cySY1b2ml6R1C99PEzSPenjUySNkXR7us83JJ2Zfo5uldQ53W/H9PMwSdLFSmZDbzrvL9PrTZI0tJn/qz0kPZIef6ek/h19U1cZEdHiAmwDdAcGklRHjAOGt3ZcNRdgfvqzE8kszv8DrAcsa4qVZCzj+4Bu6fqJwC9I6rVfBwaTDONxDXBzus/BwHnp47HAsenjRqBXeo3JJXHsTNKkSCR/3G4GvgRsDUwC1gB6AlOAE5p5HZcC16bHbgxMSbfvA9yRXrc/8BqwLrA9sABYP91vPZKqok3Tc0wELk7j2Qu4Id2vJ9ApfbwTcH36ePum117LS52937emxw4G3kjjOxh4Ob3m6sCrwKCVvab08StAv/TxMOCe9PEpwD+BzsDmwEJgt/S58cDXS/5PhqTbLyt57a8AR6WPvw/8tZn/qz4sv6F/GPC7vD8jRVmyzIjxWPpwPnBIa/vnpKukp9LH9wMXAZ8AXo2Ih9Ptw0mS2gNK5izpAjwEDAWmRsRLAJKuoPmuoSOAbwNExFJgjqQ+K+yzc7o8ma53J/nF6gGMj4iF6TVubOG13BARy4DnSkoSXwCuTq/7jqR7Sf4wzgUejYipJcdPjYhJ6XWeBe6KiJA0iSSBQPKLPUbJDdYg+eUsknp6v69J3++XJL2cxgfJ+zYnPf454FMkVSDNvabW/CMiPkw/A40kSR+SPxLrkZTQp0bEv9LtY4AjgP9N18elPydS8o2hxEBgrKR105imNrOPNSNLK4gJNNMhIyJqqR54UURsUboh/YAuKN0E3BER+6+w3xa00OGkjQScHhEXrHCNY9twjcUrnK/0Z3MWrLBeevyykvVlLH+/TwMmRMTektajeCPc1dP7veJ+Teul7+NSkveu2deUWsLyKsXVV3huMUBELJP0YaRFVZZ/JlobPqwplqY4VvRH4PcRcaOk7UlK3ZZBljrgE4AfpsvPgaeAxysZVIU8DGwnaSMASWtIGgK8AKwvacN0v+Y+3AB3kXzVRVKjpJ7APJLSTpPbgO+U1MsNkLQ2ydfGvSV1ldQD2KONsd8HjEyvuxbJ19xH23iOUr2AN9PHB3fgPLWsKO/3NyU1pPFsALzYjtcESVXB1unjfVo4R3NeANZrOi9wINCWjlaln6eD2njtVVqrCTgiJpYsD0TE8cDnqhBbWUXEuyTJ5mpJz5B8mIdGMtXSKOD/K7kp8+pKTnEMsEP6NW4isElEzCT5OjhZ0lkRcTtwFfBQut91QI9IJjEdS/LH63qSr81tMR54BngauBv4UUS83cZzlDoTOF3SAyRfSetOgd7vF0mS3T+A76Xxtek1pU//EviDpPtJSqqZpdc8BLg2fR3LgPPbcIpT0mPvB2a05dqrulZ7wklas2S1geSv7LkRUeQ752a5k3QpyQ3A6/KOxfKRZUaMiST1UiKpZ5oKHFrJoMzMVgVZSsCrr/i1SNJqEbF4ZceYmVnrstyEe7CZbVmavpiZWQtaGg94HWAASZvLLVneVKUnSQNzMzPrgJbqgHchueM6EPgdyxPwXOCnlQ3LzKz+ZakD3icirq9SPGZmq4wsdcBbS+rdtCKpj6RfVTAmM7NVQpYEvFtEzG5aiYj3gN0rF5KZ2aohSwJubBqaDkBSV2C1FvY3M7MMsnTEuAK4S9IlJB0yvkMyXJ2ZmXVApkk5Je1KMm6sgNsj4rZKB2ZmVu/aPCuypO2A/46IIyoTkpnZqiFLFUTTGKr7AyNJxoIY1/IRZmbWmpZ6wg0B9iNJvDNJhtdTROxQpdjMzOraSqsgJC0jGcf00IiYkm57OSI2qGJ8ZmZ1q6VmaPsAbwMTJF0oaUdan7rEzMwyytIVuRvJzKn7k0xUOIZkwsHbKx+emVn9alMriHR2jG8CI2tsUk4zs8JpczM0MzMrjyxdkc3MrAKcgM3McuIEbGaWEydgM7Oc/B983/73UKb7XgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294fa5e8e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_pred_base = resmodel.predict(X_test)\n",
    "res_pred = [label_decoder(i) for i in res_pred_base]\n",
    "res_cm = confusion_matrix(y_test, res_pred)\n",
    "sns.heatmap(res_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, res_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93         8\n",
      "          1       0.89      1.00      0.94         8\n",
      "\n",
      "avg / total       0.94      0.94      0.94        16\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294fc961898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_pred_base = resmodel.predict(X_val)\n",
    "val_pred = [label_decoder(i) for i in val_pred_base]\n",
    "val_cm = confusion_matrix(y_val, val_pred)\n",
    "sns.heatmap(val_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy:  0.7916666666666666\n",
      "validation set accuracy:  0.9375\n"
     ]
    }
   ],
   "source": [
    "print('test set accuracy: ', accuracy_score(y_test, res_pred))\n",
    "print('validation set accuracy: ', accuracy_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trained model in case I need to use it later\n",
    "filename = 'resnet1000.hdf5'\n",
    "keras.models.save_model(resmodel, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit with standardised images\n",
    "resmodel2 = keras.applications.ResNet50(include_top = True, weights=None, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resmodel2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/1000\n",
      "2682/2682 [==============================] - 79s 30ms/step - loss: 0.3613 - acc: 0.8740 - val_loss: 3.4064 - val_acc: 0.6250\n",
      "Epoch 2/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.2196 - acc: 0.9131 - val_loss: 2.3995 - val_acc: 0.6250\n",
      "Epoch 3/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1425 - acc: 0.9467 - val_loss: 1.5559 - val_acc: 0.6266\n",
      "Epoch 4/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1458 - acc: 0.9437 - val_loss: 5.1186 - val_acc: 0.6250\n",
      "Epoch 5/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1308 - acc: 0.9538 - val_loss: 2.6458 - val_acc: 0.6298\n",
      "Epoch 6/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1253 - acc: 0.9590 - val_loss: 1.0943 - val_acc: 0.6987\n",
      "Epoch 7/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1167 - acc: 0.9575 - val_loss: 0.9307 - val_acc: 0.6795\n",
      "Epoch 8/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0815 - acc: 0.9709 - val_loss: 1.0583 - val_acc: 0.7212\n",
      "Epoch 9/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0785 - acc: 0.9739 - val_loss: 0.6092 - val_acc: 0.7917\n",
      "Epoch 10/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1087 - acc: 0.9541 - val_loss: 2.6364 - val_acc: 0.6362\n",
      "Epoch 11/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0650 - acc: 0.9765 - val_loss: 1.5768 - val_acc: 0.6362\n",
      "Epoch 12/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0789 - acc: 0.9709 - val_loss: 1.5473 - val_acc: 0.6843\n",
      "Epoch 13/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0768 - acc: 0.9728 - val_loss: 0.5616 - val_acc: 0.7756\n",
      "Epoch 14/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0604 - acc: 0.9787 - val_loss: 1.6466 - val_acc: 0.6667\n",
      "Epoch 15/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0707 - acc: 0.9784 - val_loss: 0.7427 - val_acc: 0.7708\n",
      "Epoch 16/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0691 - acc: 0.9765 - val_loss: 1.8327 - val_acc: 0.6571\n",
      "Epoch 17/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0426 - acc: 0.9855 - val_loss: 1.1676 - val_acc: 0.7019\n",
      "Epoch 18/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0443 - acc: 0.9840 - val_loss: 3.9443 - val_acc: 0.6250\n",
      "Epoch 19/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0525 - acc: 0.9817 - val_loss: 3.3146 - val_acc: 0.6410\n",
      "Epoch 20/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0419 - acc: 0.9836 - val_loss: 1.2758 - val_acc: 0.7612\n",
      "Epoch 21/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0235 - acc: 0.9925 - val_loss: 2.7965 - val_acc: 0.6859\n",
      "Epoch 22/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0366 - acc: 0.9873 - val_loss: 2.0306 - val_acc: 0.6651\n",
      "Epoch 23/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1249 - acc: 0.9638 - val_loss: 1.4732 - val_acc: 0.7035\n",
      "Epoch 24/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0471 - acc: 0.9825 - val_loss: 2.2053 - val_acc: 0.6522\n",
      "Epoch 25/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0260 - acc: 0.9918 - val_loss: 1.8117 - val_acc: 0.6506\n",
      "Epoch 26/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0295 - acc: 0.9888 - val_loss: 2.0079 - val_acc: 0.6843\n",
      "Epoch 27/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0238 - acc: 0.9925 - val_loss: 4.2532 - val_acc: 0.6282\n",
      "Epoch 28/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0556 - acc: 0.9806 - val_loss: 2.0143 - val_acc: 0.6619\n",
      "Epoch 29/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0622 - acc: 0.9776 - val_loss: 2.1830 - val_acc: 0.6538\n",
      "Epoch 30/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0300 - acc: 0.9907 - val_loss: 2.0215 - val_acc: 0.6571\n",
      "Epoch 31/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0068 - acc: 0.9981 - val_loss: 2.2899 - val_acc: 0.6571\n",
      "Epoch 32/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0244 - acc: 0.9892 - val_loss: 3.6039 - val_acc: 0.6490\n",
      "Epoch 33/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0296 - acc: 0.9896 - val_loss: 2.1971 - val_acc: 0.6795\n",
      "Epoch 34/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0339 - acc: 0.9870 - val_loss: 2.6763 - val_acc: 0.6571\n",
      "Epoch 35/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0232 - acc: 0.9918 - val_loss: 2.3098 - val_acc: 0.6651\n",
      "Epoch 36/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 2.8156 - val_acc: 0.6554\n",
      "Epoch 37/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0303 - acc: 0.9911 - val_loss: 2.7270 - val_acc: 0.6587\n",
      "Epoch 38/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0256 - acc: 0.9914 - val_loss: 1.5301 - val_acc: 0.7468\n",
      "Epoch 39/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0397 - acc: 0.9847 - val_loss: 1.0577 - val_acc: 0.7356\n",
      "Epoch 40/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0171 - acc: 0.9937 - val_loss: 1.9572 - val_acc: 0.6987\n",
      "Epoch 41/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0155 - acc: 0.9952 - val_loss: 2.3717 - val_acc: 0.6795\n",
      "Epoch 42/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0032 - acc: 0.9996 - val_loss: 3.1750 - val_acc: 0.6715\n",
      "Epoch 43/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 2.1058 - val_acc: 0.6939\n",
      "Epoch 44/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0374 - acc: 0.9862 - val_loss: 1.2555 - val_acc: 0.7163\n",
      "Epoch 45/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0924 - acc: 0.9676 - val_loss: 1.5470 - val_acc: 0.7099\n",
      "Epoch 46/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0203 - acc: 0.9911 - val_loss: 1.7662 - val_acc: 0.6715\n",
      "Epoch 47/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0396 - acc: 0.9899 - val_loss: 1.3028 - val_acc: 0.7067\n",
      "Epoch 48/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0131 - acc: 0.9963 - val_loss: 1.8490 - val_acc: 0.6779\n",
      "Epoch 49/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0298 - acc: 0.9896 - val_loss: 1.1109 - val_acc: 0.7388\n",
      "Epoch 50/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0195 - acc: 0.9914 - val_loss: 1.9401 - val_acc: 0.6827\n",
      "Epoch 51/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 2.5783 - val_acc: 0.6667\n",
      "Epoch 52/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.5454e-04 - acc: 0.9996 - val_loss: 2.7276 - val_acc: 0.6667\n",
      "Epoch 53/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.6250 - val_acc: 0.6619\n",
      "Epoch 54/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.5245 - val_acc: 0.6811\n",
      "Epoch 55/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9455e-04 - acc: 1.0000 - val_loss: 2.6347 - val_acc: 0.6811\n",
      "Epoch 56/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3224e-04 - acc: 1.0000 - val_loss: 2.6609 - val_acc: 0.6907\n",
      "Epoch 57/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5060e-04 - acc: 1.0000 - val_loss: 2.7137 - val_acc: 0.6907\n",
      "Epoch 58/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0645e-04 - acc: 1.0000 - val_loss: 2.6497 - val_acc: 0.6907\n",
      "Epoch 59/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0676e-04 - acc: 1.0000 - val_loss: 2.6253 - val_acc: 0.6939\n",
      "Epoch 60/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5223e-05 - acc: 1.0000 - val_loss: 2.6132 - val_acc: 0.7003\n",
      "Epoch 61/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9042e-04 - acc: 1.0000 - val_loss: 2.3210 - val_acc: 0.7067\n",
      "Epoch 62/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 2.5988 - val_acc: 0.6971\n",
      "Epoch 63/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.1018 - acc: 0.9676 - val_loss: 1.6395 - val_acc: 0.7051\n",
      "Epoch 64/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0341 - acc: 0.9896 - val_loss: 3.4269 - val_acc: 0.6410\n",
      "Epoch 65/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0157 - acc: 0.9948 - val_loss: 2.7865 - val_acc: 0.6490\n",
      "Epoch 66/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0349 - acc: 0.9903 - val_loss: 2.0623 - val_acc: 0.6699\n",
      "Epoch 67/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0085 - acc: 0.9966 - val_loss: 1.2458 - val_acc: 0.7436\n",
      "Epoch 68/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 1.7255 - val_acc: 0.6955\n",
      "Epoch 69/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 3.2037 - val_acc: 0.6635\n",
      "Epoch 70/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0246 - acc: 0.9911 - val_loss: 1.3611 - val_acc: 0.7660\n",
      "Epoch 71/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0231 - acc: 0.9899 - val_loss: 1.9732 - val_acc: 0.6571\n",
      "Epoch 72/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 2.8819 - val_acc: 0.6522\n",
      "Epoch 73/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0172 - acc: 0.9940 - val_loss: 1.8718 - val_acc: 0.7388\n",
      "Epoch 74/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0324 - acc: 0.9892 - val_loss: 3.2025 - val_acc: 0.6442\n",
      "Epoch 75/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0080 - acc: 0.9978 - val_loss: 2.3777 - val_acc: 0.6635\n",
      "Epoch 76/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1847e-04 - acc: 1.0000 - val_loss: 2.4119 - val_acc: 0.6811\n",
      "Epoch 77/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5798e-04 - acc: 1.0000 - val_loss: 2.4022 - val_acc: 0.6891\n",
      "Epoch 78/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1590e-04 - acc: 1.0000 - val_loss: 2.5544 - val_acc: 0.6923\n",
      "Epoch 79/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4306e-04 - acc: 1.0000 - val_loss: 2.6112 - val_acc: 0.6907\n",
      "Epoch 80/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1212e-04 - acc: 1.0000 - val_loss: 2.4523 - val_acc: 0.6923\n",
      "Epoch 81/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7553e-04 - acc: 1.0000 - val_loss: 2.9788 - val_acc: 0.6715\n",
      "Epoch 82/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9053e-04 - acc: 0.9996 - val_loss: 2.7986 - val_acc: 0.6811\n",
      "Epoch 83/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6771e-04 - acc: 1.0000 - val_loss: 2.7606 - val_acc: 0.6907\n",
      "Epoch 84/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8556e-04 - acc: 1.0000 - val_loss: 2.4020 - val_acc: 0.7115\n",
      "Epoch 85/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0370 - acc: 0.9896 - val_loss: 1.4235 - val_acc: 0.7788\n",
      "Epoch 86/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0247 - acc: 0.9907 - val_loss: 2.9575 - val_acc: 0.6587\n",
      "Epoch 87/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0163 - acc: 0.9940 - val_loss: 3.3131 - val_acc: 0.6410\n",
      "Epoch 88/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0089 - acc: 0.9970 - val_loss: 2.1420 - val_acc: 0.7099\n",
      "Epoch 89/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0327 - acc: 0.9940 - val_loss: 1.2624 - val_acc: 0.7420\n",
      "Epoch 90/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 1.9898 - val_acc: 0.7212\n",
      "Epoch 91/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0140 - acc: 0.9948 - val_loss: 1.7648 - val_acc: 0.6987\n",
      "Epoch 92/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 2.3583 - val_acc: 0.6987\n",
      "Epoch 93/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1962e-04 - acc: 1.0000 - val_loss: 2.2848 - val_acc: 0.6971\n",
      "Epoch 94/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5648e-04 - acc: 1.0000 - val_loss: 2.3768 - val_acc: 0.7067\n",
      "Epoch 95/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8355e-05 - acc: 1.0000 - val_loss: 2.5014 - val_acc: 0.7051\n",
      "Epoch 96/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8914e-05 - acc: 1.0000 - val_loss: 2.4711 - val_acc: 0.7083\n",
      "Epoch 97/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 2.6722 - val_acc: 0.6859\n",
      "Epoch 98/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0125 - acc: 0.9948 - val_loss: 3.6387 - val_acc: 0.6635\n",
      "Epoch 99/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0264 - acc: 0.9896 - val_loss: 3.3752 - val_acc: 0.6683\n",
      "Epoch 100/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0122 - acc: 0.9963 - val_loss: 2.4194 - val_acc: 0.7003\n",
      "Epoch 101/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0069 - acc: 0.9978 - val_loss: 2.7197 - val_acc: 0.6747\n",
      "Epoch 102/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3771e-04 - acc: 1.0000 - val_loss: 2.6300 - val_acc: 0.6987\n",
      "Epoch 103/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7586e-04 - acc: 1.0000 - val_loss: 2.5622 - val_acc: 0.6987\n",
      "Epoch 104/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1087e-04 - acc: 1.0000 - val_loss: 2.7856 - val_acc: 0.6939\n",
      "Epoch 105/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2053e-04 - acc: 1.0000 - val_loss: 2.6853 - val_acc: 0.7051\n",
      "Epoch 106/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4186e-04 - acc: 1.0000 - val_loss: 2.5257 - val_acc: 0.7099\n",
      "Epoch 107/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1685e-04 - acc: 1.0000 - val_loss: 2.8306 - val_acc: 0.6971\n",
      "Epoch 108/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5621e-05 - acc: 1.0000 - val_loss: 2.7996 - val_acc: 0.7019\n",
      "Epoch 109/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7296e-04 - acc: 1.0000 - val_loss: 2.4621 - val_acc: 0.7196\n",
      "Epoch 110/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5810e-05 - acc: 1.0000 - val_loss: 2.6155 - val_acc: 0.7099\n",
      "Epoch 111/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0799e-05 - acc: 1.0000 - val_loss: 2.8056 - val_acc: 0.7083\n",
      "Epoch 112/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7463e-05 - acc: 1.0000 - val_loss: 2.8306 - val_acc: 0.7067\n",
      "Epoch 113/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6622e-05 - acc: 1.0000 - val_loss: 2.7974 - val_acc: 0.7083\n",
      "Epoch 114/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4978e-05 - acc: 1.0000 - val_loss: 2.8945 - val_acc: 0.7051\n",
      "Epoch 115/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0470 - acc: 0.9873 - val_loss: 4.0815 - val_acc: 0.6651\n",
      "Epoch 116/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0273 - acc: 0.9903 - val_loss: 1.6935 - val_acc: 0.6987\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0232 - acc: 0.9903 - val_loss: 3.0863 - val_acc: 0.6538\n",
      "Epoch 118/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0068 - acc: 0.9970 - val_loss: 1.1501 - val_acc: 0.7676\n",
      "Epoch 119/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.1017 - val_acc: 0.6875\n",
      "Epoch 120/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9103e-04 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.7051\n",
      "Epoch 121/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 2.2005 - val_acc: 0.6971\n",
      "Epoch 122/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8206e-04 - acc: 1.0000 - val_loss: 2.2996 - val_acc: 0.6875\n",
      "Epoch 123/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1836e-04 - acc: 1.0000 - val_loss: 2.8277 - val_acc: 0.6763\n",
      "Epoch 124/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9017e-04 - acc: 1.0000 - val_loss: 2.8583 - val_acc: 0.6779\n",
      "Epoch 125/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4713e-05 - acc: 1.0000 - val_loss: 2.9739 - val_acc: 0.6763\n",
      "Epoch 126/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0913e-04 - acc: 1.0000 - val_loss: 3.0081 - val_acc: 0.6795\n",
      "Epoch 127/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4540e-05 - acc: 1.0000 - val_loss: 2.9510 - val_acc: 0.6843\n",
      "Epoch 128/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1307e-05 - acc: 1.0000 - val_loss: 2.9395 - val_acc: 0.6827\n",
      "Epoch 129/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1505e-04 - acc: 1.0000 - val_loss: 2.9705 - val_acc: 0.6843\n",
      "Epoch 130/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4414e-05 - acc: 1.0000 - val_loss: 2.7952 - val_acc: 0.6939\n",
      "Epoch 131/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7294e-05 - acc: 1.0000 - val_loss: 2.7803 - val_acc: 0.6955\n",
      "Epoch 132/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7131e-05 - acc: 1.0000 - val_loss: 2.8895 - val_acc: 0.6923\n",
      "Epoch 133/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1608e-05 - acc: 1.0000 - val_loss: 2.8749 - val_acc: 0.6923\n",
      "Epoch 134/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5855e-05 - acc: 1.0000 - val_loss: 3.0696 - val_acc: 0.6859\n",
      "Epoch 135/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4122e-05 - acc: 1.0000 - val_loss: 2.9227 - val_acc: 0.6891\n",
      "Epoch 136/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5811e-06 - acc: 1.0000 - val_loss: 2.9047 - val_acc: 0.6923\n",
      "Epoch 137/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5269e-06 - acc: 1.0000 - val_loss: 2.9557 - val_acc: 0.6907\n",
      "Epoch 138/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1275e-05 - acc: 1.0000 - val_loss: 2.7859 - val_acc: 0.6987\n",
      "Epoch 139/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1651e-06 - acc: 1.0000 - val_loss: 2.8589 - val_acc: 0.6971\n",
      "Epoch 140/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0248 - acc: 0.9929 - val_loss: 4.7344 - val_acc: 0.5673\n",
      "Epoch 141/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0609 - acc: 0.9795 - val_loss: 1.6323 - val_acc: 0.7083\n",
      "Epoch 142/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0168 - acc: 0.9933 - val_loss: 1.9660 - val_acc: 0.6907\n",
      "Epoch 143/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 2.6510 - val_acc: 0.6571\n",
      "Epoch 144/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 2.4246 - val_acc: 0.6699\n",
      "Epoch 145/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7904e-04 - acc: 1.0000 - val_loss: 2.4836 - val_acc: 0.6699\n",
      "Epoch 146/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4849e-04 - acc: 0.9996 - val_loss: 2.5405 - val_acc: 0.6731\n",
      "Epoch 147/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1325e-04 - acc: 1.0000 - val_loss: 2.6586 - val_acc: 0.6731\n",
      "Epoch 148/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2919e-04 - acc: 1.0000 - val_loss: 2.6349 - val_acc: 0.6763\n",
      "Epoch 149/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2955e-05 - acc: 1.0000 - val_loss: 2.7188 - val_acc: 0.6747\n",
      "Epoch 150/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2102e-05 - acc: 1.0000 - val_loss: 2.6948 - val_acc: 0.6795\n",
      "Epoch 151/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6348e-05 - acc: 1.0000 - val_loss: 2.7600 - val_acc: 0.6795\n",
      "Epoch 152/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1120e-05 - acc: 1.0000 - val_loss: 2.9096 - val_acc: 0.6763\n",
      "Epoch 153/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3769e-05 - acc: 1.0000 - val_loss: 2.8958 - val_acc: 0.6763\n",
      "Epoch 154/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9470e-05 - acc: 1.0000 - val_loss: 2.8799 - val_acc: 0.6763\n",
      "Epoch 155/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4162e-05 - acc: 1.0000 - val_loss: 2.9696 - val_acc: 0.6763\n",
      "Epoch 156/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8744e-05 - acc: 1.0000 - val_loss: 2.9879 - val_acc: 0.6763\n",
      "Epoch 157/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3507e-05 - acc: 1.0000 - val_loss: 3.0689 - val_acc: 0.6747\n",
      "Epoch 158/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5296e-05 - acc: 1.0000 - val_loss: 3.0720 - val_acc: 0.6731\n",
      "Epoch 159/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1637e-05 - acc: 1.0000 - val_loss: 3.0259 - val_acc: 0.6779\n",
      "Epoch 160/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7446e-05 - acc: 1.0000 - val_loss: 3.0113 - val_acc: 0.6811\n",
      "Epoch 161/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0038e-05 - acc: 1.0000 - val_loss: 3.0251 - val_acc: 0.6827\n",
      "Epoch 162/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2214e-05 - acc: 1.0000 - val_loss: 3.0385 - val_acc: 0.6859\n",
      "Epoch 163/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9584e-06 - acc: 1.0000 - val_loss: 3.0050 - val_acc: 0.6875\n",
      "Epoch 164/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5759e-06 - acc: 1.0000 - val_loss: 2.9742 - val_acc: 0.6891\n",
      "Epoch 165/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7380e-05 - acc: 1.0000 - val_loss: 3.0362 - val_acc: 0.6795\n",
      "Epoch 166/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7367e-05 - acc: 1.0000 - val_loss: 3.3150 - val_acc: 0.6667\n",
      "Epoch 167/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8758e-05 - acc: 1.0000 - val_loss: 3.1451 - val_acc: 0.6747\n",
      "Epoch 168/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1219e-05 - acc: 1.0000 - val_loss: 3.0221 - val_acc: 0.6795\n",
      "Epoch 169/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4609e-06 - acc: 1.0000 - val_loss: 3.0838 - val_acc: 0.6795\n",
      "Epoch 170/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8812e-06 - acc: 1.0000 - val_loss: 3.0505 - val_acc: 0.6827\n",
      "Epoch 171/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3282e-05 - acc: 1.0000 - val_loss: 3.0263 - val_acc: 0.6843\n",
      "Epoch 172/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3307e-05 - acc: 1.0000 - val_loss: 3.1925 - val_acc: 0.6811\n",
      "Epoch 173/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9501e-06 - acc: 1.0000 - val_loss: 3.0348 - val_acc: 0.6875\n",
      "Epoch 174/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8256e-06 - acc: 1.0000 - val_loss: 2.9858 - val_acc: 0.6955\n",
      "Epoch 175/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0626e-06 - acc: 1.0000 - val_loss: 3.0580 - val_acc: 0.6907\n",
      "Epoch 176/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5648e-06 - acc: 1.0000 - val_loss: 2.9237 - val_acc: 0.7003\n",
      "Epoch 177/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6687e-06 - acc: 1.0000 - val_loss: 3.0480 - val_acc: 0.6955\n",
      "Epoch 178/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5631e-06 - acc: 1.0000 - val_loss: 3.1599 - val_acc: 0.6907\n",
      "Epoch 179/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2141e-06 - acc: 1.0000 - val_loss: 3.1338 - val_acc: 0.6939\n",
      "Epoch 180/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8625e-06 - acc: 1.0000 - val_loss: 3.1585 - val_acc: 0.6939\n",
      "Epoch 181/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8268e-07 - acc: 1.0000 - val_loss: 3.1361 - val_acc: 0.6939\n",
      "Epoch 182/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1635e-06 - acc: 1.0000 - val_loss: 3.1343 - val_acc: 0.6939\n",
      "Epoch 183/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.2851e-07 - acc: 1.0000 - val_loss: 3.1863 - val_acc: 0.6939\n",
      "Epoch 184/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7687e-07 - acc: 1.0000 - val_loss: 3.1662 - val_acc: 0.6939\n",
      "Epoch 185/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0258e-06 - acc: 1.0000 - val_loss: 3.2113 - val_acc: 0.6939\n",
      "Epoch 186/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8708e-07 - acc: 1.0000 - val_loss: 3.1947 - val_acc: 0.6939\n",
      "Epoch 187/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0443e-06 - acc: 1.0000 - val_loss: 2.7599 - val_acc: 0.7115\n",
      "Epoch 188/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0434 - acc: 0.9907 - val_loss: 3.8406 - val_acc: 0.7003\n",
      "Epoch 189/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0725 - acc: 0.9787 - val_loss: 2.0237 - val_acc: 0.6619\n",
      "Epoch 190/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 3.2870 - val_acc: 0.6506\n",
      "Epoch 191/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 3.1865 - val_acc: 0.6554\n",
      "Epoch 192/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0071 - acc: 0.9985 - val_loss: 2.7721 - val_acc: 0.6635\n",
      "Epoch 193/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 2.5641 - val_acc: 0.6779\n",
      "Epoch 194/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5029e-04 - acc: 1.0000 - val_loss: 2.7248 - val_acc: 0.6779\n",
      "Epoch 195/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4257e-04 - acc: 1.0000 - val_loss: 2.6956 - val_acc: 0.6859\n",
      "Epoch 196/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7611e-04 - acc: 1.0000 - val_loss: 2.6338 - val_acc: 0.6923\n",
      "Epoch 197/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5151e-04 - acc: 1.0000 - val_loss: 2.7870 - val_acc: 0.6875\n",
      "Epoch 198/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6092e-05 - acc: 1.0000 - val_loss: 2.7771 - val_acc: 0.6923\n",
      "Epoch 199/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0102 - acc: 0.9955 - val_loss: 2.7147 - val_acc: 0.6971\n",
      "Epoch 200/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0181 - acc: 0.9940 - val_loss: 2.4069 - val_acc: 0.6923\n",
      "Epoch 201/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 2.9581 - val_acc: 0.6779\n",
      "Epoch 202/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4793e-04 - acc: 1.0000 - val_loss: 3.3829 - val_acc: 0.6651\n",
      "Epoch 203/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1491e-04 - acc: 1.0000 - val_loss: 3.3090 - val_acc: 0.6699\n",
      "Epoch 204/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1515e-04 - acc: 0.9996 - val_loss: 3.1425 - val_acc: 0.6651\n",
      "Epoch 205/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1520e-04 - acc: 1.0000 - val_loss: 2.9882 - val_acc: 0.6875\n",
      "Epoch 206/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1346e-05 - acc: 1.0000 - val_loss: 3.0898 - val_acc: 0.6891\n",
      "Epoch 207/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1956e-05 - acc: 1.0000 - val_loss: 3.2577 - val_acc: 0.6795\n",
      "Epoch 208/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8841e-05 - acc: 1.0000 - val_loss: 3.2740 - val_acc: 0.6827\n",
      "Epoch 209/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0163e-04 - acc: 1.0000 - val_loss: 3.5494 - val_acc: 0.6683\n",
      "Epoch 210/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8318e-05 - acc: 1.0000 - val_loss: 3.4682 - val_acc: 0.6763\n",
      "Epoch 211/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.2265 - val_acc: 0.6490\n",
      "Epoch 212/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 3.1131 - val_acc: 0.7051\n",
      "Epoch 213/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0294 - acc: 0.9892 - val_loss: 2.8325 - val_acc: 0.6859\n",
      "Epoch 214/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0047 - acc: 0.9978 - val_loss: 3.1511 - val_acc: 0.6506\n",
      "Epoch 215/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0065 - acc: 0.9981 - val_loss: 3.0161 - val_acc: 0.6843\n",
      "Epoch 216/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4512e-04 - acc: 1.0000 - val_loss: 2.8885 - val_acc: 0.6859\n",
      "Epoch 217/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8088e-04 - acc: 1.0000 - val_loss: 2.6494 - val_acc: 0.7067\n",
      "Epoch 218/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2449e-04 - acc: 1.0000 - val_loss: 2.5383 - val_acc: 0.7163\n",
      "Epoch 219/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 4.1008 - val_acc: 0.6474\n",
      "Epoch 220/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0136 - acc: 0.9978 - val_loss: 2.5874 - val_acc: 0.7179\n",
      "Epoch 221/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 3.5056 - val_acc: 0.6554\n",
      "Epoch 222/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 0.9989 - val_loss: 2.9754 - val_acc: 0.6715\n",
      "Epoch 223/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2851e-04 - acc: 1.0000 - val_loss: 3.1843 - val_acc: 0.6683\n",
      "Epoch 224/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2389e-04 - acc: 1.0000 - val_loss: 3.4145 - val_acc: 0.6587\n",
      "Epoch 225/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0193e-04 - acc: 1.0000 - val_loss: 3.5383 - val_acc: 0.6571\n",
      "Epoch 226/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1030e-05 - acc: 1.0000 - val_loss: 3.2652 - val_acc: 0.6667\n",
      "Epoch 227/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8628e-05 - acc: 1.0000 - val_loss: 3.3480 - val_acc: 0.6667\n",
      "Epoch 228/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8823e-05 - acc: 1.0000 - val_loss: 3.3483 - val_acc: 0.6667\n",
      "Epoch 229/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7092e-05 - acc: 1.0000 - val_loss: 3.3280 - val_acc: 0.6683\n",
      "Epoch 230/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2591e-05 - acc: 1.0000 - val_loss: 3.3796 - val_acc: 0.6651\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8182e-05 - acc: 1.0000 - val_loss: 3.3712 - val_acc: 0.6683\n",
      "Epoch 232/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5376e-05 - acc: 1.0000 - val_loss: 3.7594 - val_acc: 0.6587\n",
      "Epoch 233/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0754e-05 - acc: 1.0000 - val_loss: 3.6174 - val_acc: 0.6587\n",
      "Epoch 234/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.3853e-06 - acc: 1.0000 - val_loss: 3.6085 - val_acc: 0.6587\n",
      "Epoch 235/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0571e-05 - acc: 1.0000 - val_loss: 3.6028 - val_acc: 0.6587\n",
      "Epoch 236/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3119e-05 - acc: 1.0000 - val_loss: 3.4916 - val_acc: 0.6619\n",
      "Epoch 237/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0165e-05 - acc: 1.0000 - val_loss: 3.5983 - val_acc: 0.6587\n",
      "Epoch 238/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1460e-05 - acc: 1.0000 - val_loss: 3.6150 - val_acc: 0.6603\n",
      "Epoch 239/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3396e-06 - acc: 1.0000 - val_loss: 3.6122 - val_acc: 0.6603\n",
      "Epoch 240/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0776e-06 - acc: 1.0000 - val_loss: 3.5946 - val_acc: 0.6619\n",
      "Epoch 241/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3803e-05 - acc: 1.0000 - val_loss: 3.5089 - val_acc: 0.6651\n",
      "Epoch 242/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5220e-06 - acc: 1.0000 - val_loss: 3.5569 - val_acc: 0.6635\n",
      "Epoch 243/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1887e-06 - acc: 1.0000 - val_loss: 3.5939 - val_acc: 0.6619\n",
      "Epoch 244/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6748e-06 - acc: 1.0000 - val_loss: 3.6799 - val_acc: 0.6603\n",
      "Epoch 245/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4936e-06 - acc: 1.0000 - val_loss: 3.6562 - val_acc: 0.6619\n",
      "Epoch 246/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5591e-06 - acc: 1.0000 - val_loss: 3.6221 - val_acc: 0.6619\n",
      "Epoch 247/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3175e-06 - acc: 1.0000 - val_loss: 3.6804 - val_acc: 0.6619\n",
      "Epoch 248/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2459e-06 - acc: 1.0000 - val_loss: 3.5296 - val_acc: 0.6683\n",
      "Epoch 249/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9439e-06 - acc: 1.0000 - val_loss: 3.5217 - val_acc: 0.6699\n",
      "Epoch 250/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8083e-06 - acc: 1.0000 - val_loss: 3.5977 - val_acc: 0.6683\n",
      "Epoch 251/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6719e-06 - acc: 1.0000 - val_loss: 3.6148 - val_acc: 0.6683\n",
      "Epoch 252/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.4996e-06 - acc: 1.0000 - val_loss: 3.5464 - val_acc: 0.6699\n",
      "Epoch 253/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9549e-06 - acc: 1.0000 - val_loss: 3.3217 - val_acc: 0.6875\n",
      "Epoch 254/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5401e-06 - acc: 1.0000 - val_loss: 3.4158 - val_acc: 0.6811\n",
      "Epoch 255/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0222e-06 - acc: 1.0000 - val_loss: 3.4237 - val_acc: 0.6827\n",
      "Epoch 256/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1298e-06 - acc: 1.0000 - val_loss: 3.5311 - val_acc: 0.6779\n",
      "Epoch 257/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6242e-06 - acc: 1.0000 - val_loss: 3.6059 - val_acc: 0.6667\n",
      "Epoch 258/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9718e-06 - acc: 1.0000 - val_loss: 3.6408 - val_acc: 0.6667\n",
      "Epoch 259/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2965e-06 - acc: 1.0000 - val_loss: 3.6193 - val_acc: 0.6667\n",
      "Epoch 260/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8730e-07 - acc: 1.0000 - val_loss: 3.6311 - val_acc: 0.6667\n",
      "Epoch 261/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1391e-07 - acc: 1.0000 - val_loss: 3.6471 - val_acc: 0.6667\n",
      "Epoch 262/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3889e-07 - acc: 1.0000 - val_loss: 3.6408 - val_acc: 0.6667\n",
      "Epoch 263/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8327e-07 - acc: 1.0000 - val_loss: 3.6431 - val_acc: 0.6667\n",
      "Epoch 264/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0721e-06 - acc: 1.0000 - val_loss: 3.6069 - val_acc: 0.6747\n",
      "Epoch 265/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1420e-06 - acc: 1.0000 - val_loss: 3.6378 - val_acc: 0.6699\n",
      "Epoch 266/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0532e-06 - acc: 1.0000 - val_loss: 3.6429 - val_acc: 0.6699\n",
      "Epoch 267/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5803e-07 - acc: 1.0000 - val_loss: 3.6521 - val_acc: 0.6715\n",
      "Epoch 268/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.5660e-07 - acc: 1.0000 - val_loss: 3.7074 - val_acc: 0.6667\n",
      "Epoch 269/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0485 - acc: 0.9870 - val_loss: 2.6345 - val_acc: 0.7131\n",
      "Epoch 270/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0188 - acc: 0.9918 - val_loss: 3.1984 - val_acc: 0.6811\n",
      "Epoch 271/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0076 - acc: 0.9974 - val_loss: 3.7383 - val_acc: 0.6474\n",
      "Epoch 272/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 2.5958 - val_acc: 0.7115\n",
      "Epoch 273/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 2.9432 - val_acc: 0.6907\n",
      "Epoch 274/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.7070 - val_acc: 0.7003\n",
      "Epoch 275/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9716e-04 - acc: 1.0000 - val_loss: 2.6409 - val_acc: 0.7051\n",
      "Epoch 276/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4313e-04 - acc: 1.0000 - val_loss: 2.8595 - val_acc: 0.6955\n",
      "Epoch 277/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4264e-05 - acc: 1.0000 - val_loss: 2.9347 - val_acc: 0.6923\n",
      "Epoch 278/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0110e-05 - acc: 1.0000 - val_loss: 2.9897 - val_acc: 0.6907\n",
      "Epoch 279/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4646e-05 - acc: 1.0000 - val_loss: 3.1712 - val_acc: 0.6875\n",
      "Epoch 280/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1761e-05 - acc: 1.0000 - val_loss: 3.1223 - val_acc: 0.6891\n",
      "Epoch 281/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2411e-05 - acc: 1.0000 - val_loss: 3.1108 - val_acc: 0.6923\n",
      "Epoch 282/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.4587e-05 - acc: 1.0000 - val_loss: 3.1397 - val_acc: 0.6891\n",
      "Epoch 283/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3378e-05 - acc: 1.0000 - val_loss: 3.2051 - val_acc: 0.6891\n",
      "Epoch 284/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9425e-05 - acc: 1.0000 - val_loss: 3.1425 - val_acc: 0.6923\n",
      "Epoch 285/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9355e-05 - acc: 1.0000 - val_loss: 3.1358 - val_acc: 0.6939\n",
      "Epoch 286/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9630e-05 - acc: 1.0000 - val_loss: 3.2170 - val_acc: 0.6923\n",
      "Epoch 287/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6818e-05 - acc: 1.0000 - val_loss: 3.2678 - val_acc: 0.6907\n",
      "Epoch 288/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0426e-04 - acc: 1.0000 - val_loss: 3.1874 - val_acc: 0.6923\n",
      "Epoch 289/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4212e-05 - acc: 1.0000 - val_loss: 3.1835 - val_acc: 0.6987\n",
      "Epoch 290/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0241e-05 - acc: 1.0000 - val_loss: 3.2716 - val_acc: 0.6923\n",
      "Epoch 291/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0816e-06 - acc: 1.0000 - val_loss: 3.2774 - val_acc: 0.6923\n",
      "Epoch 292/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8461e-05 - acc: 1.0000 - val_loss: 3.3177 - val_acc: 0.6907\n",
      "Epoch 293/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0065e-05 - acc: 1.0000 - val_loss: 3.2628 - val_acc: 0.6939\n",
      "Epoch 294/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5183e-06 - acc: 1.0000 - val_loss: 3.2960 - val_acc: 0.6955\n",
      "Epoch 295/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8957e-06 - acc: 1.0000 - val_loss: 3.3252 - val_acc: 0.6907\n",
      "Epoch 296/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5306e-05 - acc: 1.0000 - val_loss: 3.7505 - val_acc: 0.6779\n",
      "Epoch 297/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3146e-05 - acc: 1.0000 - val_loss: 3.6922 - val_acc: 0.6843\n",
      "Epoch 298/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1464e-06 - acc: 1.0000 - val_loss: 3.7395 - val_acc: 0.6811\n",
      "Epoch 299/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4990e-05 - acc: 1.0000 - val_loss: 3.2571 - val_acc: 0.7099\n",
      "Epoch 300/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8048e-04 - acc: 1.0000 - val_loss: 3.6948 - val_acc: 0.6939\n",
      "Epoch 301/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4893e-05 - acc: 1.0000 - val_loss: 3.5478 - val_acc: 0.6891\n",
      "Epoch 302/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2150e-05 - acc: 1.0000 - val_loss: 3.4479 - val_acc: 0.7083\n",
      "Epoch 303/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2379e-05 - acc: 1.0000 - val_loss: 3.4857 - val_acc: 0.7051\n",
      "Epoch 304/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0454 - acc: 0.9877 - val_loss: 1.6888 - val_acc: 0.8157\n",
      "Epoch 305/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0093 - acc: 0.9955 - val_loss: 2.7758 - val_acc: 0.7019\n",
      "Epoch 306/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 2.9170 - val_acc: 0.6923\n",
      "Epoch 307/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7117e-04 - acc: 1.0000 - val_loss: 3.0639 - val_acc: 0.6891\n",
      "Epoch 308/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3226e-04 - acc: 1.0000 - val_loss: 3.1105 - val_acc: 0.6875\n",
      "Epoch 309/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5680e-04 - acc: 1.0000 - val_loss: 3.1730 - val_acc: 0.6859\n",
      "Epoch 310/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5387e-04 - acc: 1.0000 - val_loss: 3.3249 - val_acc: 0.6827\n",
      "Epoch 311/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2735e-04 - acc: 0.9993 - val_loss: 2.7127 - val_acc: 0.7147\n",
      "Epoch 312/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 3.1485 - val_acc: 0.6843\n",
      "Epoch 313/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7491e-04 - acc: 1.0000 - val_loss: 3.6418 - val_acc: 0.6699\n",
      "Epoch 314/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2471e-04 - acc: 1.0000 - val_loss: 3.6835 - val_acc: 0.6699\n",
      "Epoch 315/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 3.9334 - val_acc: 0.6619\n",
      "Epoch 316/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5441e-04 - acc: 0.9996 - val_loss: 3.5894 - val_acc: 0.6715\n",
      "Epoch 317/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7447e-04 - acc: 1.0000 - val_loss: 3.6568 - val_acc: 0.6715\n",
      "Epoch 318/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 3.8719 - val_acc: 0.6667\n",
      "Epoch 319/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 4.5017 - val_acc: 0.6458\n",
      "Epoch 320/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 3.4633 - val_acc: 0.6731\n",
      "Epoch 321/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3250e-04 - acc: 1.0000 - val_loss: 2.9289 - val_acc: 0.7083\n",
      "Epoch 322/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 4.2995 - val_acc: 0.6571\n",
      "Epoch 323/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 4.3434 - val_acc: 0.6538\n",
      "Epoch 324/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 3.4636 - val_acc: 0.6923\n",
      "Epoch 325/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6248e-05 - acc: 1.0000 - val_loss: 3.4703 - val_acc: 0.6971\n",
      "Epoch 326/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0161 - acc: 0.9963 - val_loss: 2.0738 - val_acc: 0.7292\n",
      "Epoch 327/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8310e-04 - acc: 1.0000 - val_loss: 2.4958 - val_acc: 0.7163\n",
      "Epoch 328/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2075e-04 - acc: 1.0000 - val_loss: 2.5667 - val_acc: 0.7131\n",
      "Epoch 329/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0954e-05 - acc: 1.0000 - val_loss: 2.6594 - val_acc: 0.7131\n",
      "Epoch 330/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3980e-05 - acc: 1.0000 - val_loss: 2.6944 - val_acc: 0.7147\n",
      "Epoch 331/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5601e-05 - acc: 1.0000 - val_loss: 2.6858 - val_acc: 0.7179\n",
      "Epoch 332/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7331e-05 - acc: 1.0000 - val_loss: 2.9138 - val_acc: 0.7067\n",
      "Epoch 333/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3019e-05 - acc: 1.0000 - val_loss: 2.8705 - val_acc: 0.7115\n",
      "Epoch 334/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7224e-05 - acc: 1.0000 - val_loss: 2.9086 - val_acc: 0.7099\n",
      "Epoch 335/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4598e-05 - acc: 1.0000 - val_loss: 2.8544 - val_acc: 0.7179\n",
      "Epoch 336/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3690e-05 - acc: 1.0000 - val_loss: 3.0268 - val_acc: 0.7115\n",
      "Epoch 337/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9228e-05 - acc: 1.0000 - val_loss: 2.8632 - val_acc: 0.7196\n",
      "Epoch 338/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8054e-06 - acc: 1.0000 - val_loss: 2.8511 - val_acc: 0.7212\n",
      "Epoch 339/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3409e-05 - acc: 1.0000 - val_loss: 2.9694 - val_acc: 0.7147\n",
      "Epoch 340/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3731e-06 - acc: 1.0000 - val_loss: 2.9455 - val_acc: 0.7179\n",
      "Epoch 341/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9838e-06 - acc: 1.0000 - val_loss: 2.9446 - val_acc: 0.7179\n",
      "Epoch 342/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7149e-06 - acc: 1.0000 - val_loss: 2.9707 - val_acc: 0.7179\n",
      "Epoch 343/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6737e-06 - acc: 1.0000 - val_loss: 2.9633 - val_acc: 0.7179\n",
      "Epoch 344/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7245e-06 - acc: 1.0000 - val_loss: 2.9810 - val_acc: 0.7179\n",
      "Epoch 345/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3013e-06 - acc: 1.0000 - val_loss: 2.9992 - val_acc: 0.7196\n",
      "Epoch 346/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2212e-06 - acc: 1.0000 - val_loss: 2.9817 - val_acc: 0.7179\n",
      "Epoch 347/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7963e-06 - acc: 1.0000 - val_loss: 3.0010 - val_acc: 0.7196\n",
      "Epoch 348/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3025e-06 - acc: 1.0000 - val_loss: 2.9737 - val_acc: 0.7179\n",
      "Epoch 349/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1593e-06 - acc: 1.0000 - val_loss: 3.0425 - val_acc: 0.7196\n",
      "Epoch 350/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9965e-06 - acc: 1.0000 - val_loss: 3.0212 - val_acc: 0.7196\n",
      "Epoch 351/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8379e-06 - acc: 1.0000 - val_loss: 3.0740 - val_acc: 0.7196\n",
      "Epoch 352/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3146e-06 - acc: 1.0000 - val_loss: 3.0533 - val_acc: 0.7196\n",
      "Epoch 353/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5413e-06 - acc: 1.0000 - val_loss: 3.0646 - val_acc: 0.7196\n",
      "Epoch 354/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7896e-06 - acc: 1.0000 - val_loss: 3.0766 - val_acc: 0.7196\n",
      "Epoch 355/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5251e-06 - acc: 1.0000 - val_loss: 3.0284 - val_acc: 0.7196\n",
      "Epoch 356/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3607e-06 - acc: 1.0000 - val_loss: 3.0299 - val_acc: 0.7196\n",
      "Epoch 357/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0252e-06 - acc: 1.0000 - val_loss: 3.0781 - val_acc: 0.7196\n",
      "Epoch 358/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5014e-06 - acc: 1.0000 - val_loss: 3.0774 - val_acc: 0.7196\n",
      "Epoch 359/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2069e-06 - acc: 1.0000 - val_loss: 3.0916 - val_acc: 0.7196\n",
      "Epoch 360/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9081e-06 - acc: 1.0000 - val_loss: 3.1083 - val_acc: 0.7196\n",
      "Epoch 361/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3158e-06 - acc: 1.0000 - val_loss: 3.1455 - val_acc: 0.7196\n",
      "Epoch 362/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3966e-06 - acc: 1.0000 - val_loss: 3.0990 - val_acc: 0.7196\n",
      "Epoch 363/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2659e-07 - acc: 1.0000 - val_loss: 3.0882 - val_acc: 0.7196\n",
      "Epoch 364/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3179e-07 - acc: 1.0000 - val_loss: 3.1219 - val_acc: 0.7196\n",
      "Epoch 365/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0627e-06 - acc: 1.0000 - val_loss: 3.1014 - val_acc: 0.7196\n",
      "Epoch 366/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.8938e-07 - acc: 1.0000 - val_loss: 3.1007 - val_acc: 0.7196\n",
      "Epoch 367/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6616e-07 - acc: 1.0000 - val_loss: 3.1146 - val_acc: 0.7196\n",
      "Epoch 368/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0782e-06 - acc: 1.0000 - val_loss: 3.1685 - val_acc: 0.7196\n",
      "Epoch 369/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8107e-06 - acc: 1.0000 - val_loss: 3.2684 - val_acc: 0.7067\n",
      "Epoch 370/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.7657e-07 - acc: 1.0000 - val_loss: 3.2505 - val_acc: 0.7067\n",
      "Epoch 371/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9806e-07 - acc: 1.0000 - val_loss: 3.2152 - val_acc: 0.7179\n",
      "Epoch 372/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0528e-06 - acc: 1.0000 - val_loss: 3.3080 - val_acc: 0.7083\n",
      "Epoch 373/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1366e-07 - acc: 1.0000 - val_loss: 3.2902 - val_acc: 0.7115\n",
      "Epoch 374/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6630e-07 - acc: 1.0000 - val_loss: 3.2420 - val_acc: 0.7163\n",
      "Epoch 375/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4819e-07 - acc: 1.0000 - val_loss: 3.2438 - val_acc: 0.7163\n",
      "Epoch 376/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7022e-07 - acc: 1.0000 - val_loss: 3.2917 - val_acc: 0.7147\n",
      "Epoch 377/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2608e-07 - acc: 1.0000 - val_loss: 3.2551 - val_acc: 0.7163\n",
      "Epoch 378/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7787e-07 - acc: 1.0000 - val_loss: 3.2743 - val_acc: 0.7147\n",
      "Epoch 379/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6172e-07 - acc: 1.0000 - val_loss: 3.2628 - val_acc: 0.7163\n",
      "Epoch 380/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0949e-07 - acc: 1.0000 - val_loss: 3.2471 - val_acc: 0.7163\n",
      "Epoch 381/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6480e-07 - acc: 1.0000 - val_loss: 3.2352 - val_acc: 0.7179\n",
      "Epoch 382/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9694e-07 - acc: 1.0000 - val_loss: 3.2267 - val_acc: 0.7196\n",
      "Epoch 383/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7352e-07 - acc: 1.0000 - val_loss: 3.3255 - val_acc: 0.7163\n",
      "Epoch 384/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5802e-07 - acc: 1.0000 - val_loss: 3.3180 - val_acc: 0.7163\n",
      "Epoch 385/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7098e-07 - acc: 1.0000 - val_loss: 3.3547 - val_acc: 0.7131\n",
      "Epoch 386/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0465e-06 - acc: 1.0000 - val_loss: 2.4435 - val_acc: 0.7420\n",
      "Epoch 387/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0466 - acc: 0.9877 - val_loss: 1.6377 - val_acc: 0.7260\n",
      "Epoch 388/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0138 - acc: 0.9955 - val_loss: 3.1937 - val_acc: 0.6683\n",
      "Epoch 389/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9792e-04 - acc: 1.0000 - val_loss: 3.4998 - val_acc: 0.6619\n",
      "Epoch 390/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9218e-04 - acc: 1.0000 - val_loss: 3.3297 - val_acc: 0.6731\n",
      "Epoch 391/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1930e-04 - acc: 1.0000 - val_loss: 3.2705 - val_acc: 0.6763\n",
      "Epoch 392/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9373e-04 - acc: 0.9996 - val_loss: 3.0383 - val_acc: 0.6923\n",
      "Epoch 393/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 2.7486 - val_acc: 0.7003\n",
      "Epoch 394/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 3.0103 - val_acc: 0.7003\n",
      "Epoch 395/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0078e-04 - acc: 1.0000 - val_loss: 2.9101 - val_acc: 0.7035\n",
      "Epoch 396/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.2427e-05 - acc: 1.0000 - val_loss: 3.0803 - val_acc: 0.6955\n",
      "Epoch 397/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0047e-05 - acc: 1.0000 - val_loss: 3.0499 - val_acc: 0.6955\n",
      "Epoch 398/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8170e-05 - acc: 1.0000 - val_loss: 3.0534 - val_acc: 0.6955\n",
      "Epoch 399/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7890e-05 - acc: 1.0000 - val_loss: 3.0478 - val_acc: 0.6971\n",
      "Epoch 400/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2410e-05 - acc: 1.0000 - val_loss: 3.0326 - val_acc: 0.7003\n",
      "Epoch 401/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0020e-04 - acc: 1.0000 - val_loss: 3.2518 - val_acc: 0.6875\n",
      "Epoch 402/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5530e-05 - acc: 1.0000 - val_loss: 3.1988 - val_acc: 0.6907\n",
      "Epoch 403/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1298e-05 - acc: 1.0000 - val_loss: 3.0728 - val_acc: 0.6939\n",
      "Epoch 404/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3608e-05 - acc: 1.0000 - val_loss: 3.1382 - val_acc: 0.6923\n",
      "Epoch 405/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1276e-05 - acc: 1.0000 - val_loss: 3.1152 - val_acc: 0.6923\n",
      "Epoch 406/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0267e-05 - acc: 1.0000 - val_loss: 3.1759 - val_acc: 0.6923\n",
      "Epoch 407/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0235 - acc: 0.9918 - val_loss: 2.6983 - val_acc: 0.6891\n",
      "Epoch 408/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 4.0773 - val_acc: 0.6619\n",
      "Epoch 409/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 3.0112 - val_acc: 0.6875\n",
      "Epoch 410/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.4926 - val_acc: 0.6731\n",
      "Epoch 411/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4231e-04 - acc: 1.0000 - val_loss: 3.2966 - val_acc: 0.6827\n",
      "Epoch 412/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5608e-04 - acc: 1.0000 - val_loss: 3.4893 - val_acc: 0.6747\n",
      "Epoch 413/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0186e-05 - acc: 1.0000 - val_loss: 3.4596 - val_acc: 0.6795\n",
      "Epoch 414/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2703e-05 - acc: 1.0000 - val_loss: 3.4904 - val_acc: 0.6795\n",
      "Epoch 415/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4679e-05 - acc: 1.0000 - val_loss: 3.4272 - val_acc: 0.6827\n",
      "Epoch 416/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1283e-05 - acc: 1.0000 - val_loss: 3.4933 - val_acc: 0.6795\n",
      "Epoch 417/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0178e-05 - acc: 1.0000 - val_loss: 3.5755 - val_acc: 0.6779\n",
      "Epoch 418/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4198e-05 - acc: 1.0000 - val_loss: 3.5840 - val_acc: 0.6779\n",
      "Epoch 419/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2080e-05 - acc: 1.0000 - val_loss: 3.5797 - val_acc: 0.6779\n",
      "Epoch 420/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2577e-05 - acc: 1.0000 - val_loss: 3.5806 - val_acc: 0.6779\n",
      "Epoch 421/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3971e-05 - acc: 1.0000 - val_loss: 3.4886 - val_acc: 0.6875\n",
      "Epoch 422/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4565e-05 - acc: 1.0000 - val_loss: 3.5041 - val_acc: 0.6875\n",
      "Epoch 423/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8471e-06 - acc: 1.0000 - val_loss: 3.5500 - val_acc: 0.6859\n",
      "Epoch 424/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5388e-05 - acc: 1.0000 - val_loss: 3.5436 - val_acc: 0.6859\n",
      "Epoch 425/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.0493e-06 - acc: 1.0000 - val_loss: 3.5519 - val_acc: 0.6875\n",
      "Epoch 426/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7683e-06 - acc: 1.0000 - val_loss: 3.5786 - val_acc: 0.6859\n",
      "Epoch 427/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0701e-06 - acc: 1.0000 - val_loss: 3.5619 - val_acc: 0.6875\n",
      "Epoch 428/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3529e-06 - acc: 1.0000 - val_loss: 3.6541 - val_acc: 0.6827\n",
      "Epoch 429/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2576e-05 - acc: 1.0000 - val_loss: 3.7121 - val_acc: 0.6827\n",
      "Epoch 430/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0200e-06 - acc: 1.0000 - val_loss: 3.6072 - val_acc: 0.6875\n",
      "Epoch 431/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8034e-06 - acc: 1.0000 - val_loss: 3.6252 - val_acc: 0.6859\n",
      "Epoch 432/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5777e-06 - acc: 1.0000 - val_loss: 3.6014 - val_acc: 0.6891\n",
      "Epoch 433/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8292e-06 - acc: 1.0000 - val_loss: 3.6034 - val_acc: 0.6891\n",
      "Epoch 434/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2592e-06 - acc: 1.0000 - val_loss: 3.6812 - val_acc: 0.6843\n",
      "Epoch 435/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2970e-06 - acc: 1.0000 - val_loss: 3.6422 - val_acc: 0.6891\n",
      "Epoch 436/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8841e-06 - acc: 1.0000 - val_loss: 3.5822 - val_acc: 0.6923\n",
      "Epoch 437/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8767e-06 - acc: 1.0000 - val_loss: 3.5614 - val_acc: 0.6955\n",
      "Epoch 438/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9681e-06 - acc: 1.0000 - val_loss: 3.6388 - val_acc: 0.6907\n",
      "Epoch 439/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0948e-06 - acc: 1.0000 - val_loss: 3.5775 - val_acc: 0.6971\n",
      "Epoch 440/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6367e-06 - acc: 1.0000 - val_loss: 3.5529 - val_acc: 0.6987\n",
      "Epoch 441/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6776e-06 - acc: 1.0000 - val_loss: 3.5675 - val_acc: 0.6987\n",
      "Epoch 442/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6254e-06 - acc: 1.0000 - val_loss: 3.6096 - val_acc: 0.6987\n",
      "Epoch 443/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4077e-06 - acc: 1.0000 - val_loss: 3.5983 - val_acc: 0.6987\n",
      "Epoch 444/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3898e-06 - acc: 1.0000 - val_loss: 3.5930 - val_acc: 0.6987\n",
      "Epoch 445/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1113e-06 - acc: 1.0000 - val_loss: 3.6073 - val_acc: 0.6987\n",
      "Epoch 446/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1287e-06 - acc: 1.0000 - val_loss: 3.6050 - val_acc: 0.6987\n",
      "Epoch 447/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1143e-06 - acc: 1.0000 - val_loss: 3.5851 - val_acc: 0.6987\n",
      "Epoch 448/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5171e-06 - acc: 1.0000 - val_loss: 3.6185 - val_acc: 0.6987\n",
      "Epoch 449/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1881e-06 - acc: 1.0000 - val_loss: 3.6366 - val_acc: 0.6987\n",
      "Epoch 450/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8332e-06 - acc: 1.0000 - val_loss: 3.6753 - val_acc: 0.6955\n",
      "Epoch 451/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2070e-06 - acc: 1.0000 - val_loss: 3.5941 - val_acc: 0.6987\n",
      "Epoch 452/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2560e-07 - acc: 1.0000 - val_loss: 3.6286 - val_acc: 0.6987\n",
      "Epoch 453/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7574e-07 - acc: 1.0000 - val_loss: 3.6056 - val_acc: 0.6987\n",
      "Epoch 454/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5238e-07 - acc: 1.0000 - val_loss: 3.5981 - val_acc: 0.6987\n",
      "Epoch 455/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6703e-06 - acc: 1.0000 - val_loss: 3.9388 - val_acc: 0.6779\n",
      "Epoch 456/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7612e-06 - acc: 1.0000 - val_loss: 3.7055 - val_acc: 0.6955\n",
      "Epoch 457/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9658e-06 - acc: 1.0000 - val_loss: 3.7774 - val_acc: 0.6907\n",
      "Epoch 458/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1648e-07 - acc: 1.0000 - val_loss: 3.7307 - val_acc: 0.6955\n",
      "Epoch 459/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7878e-07 - acc: 1.0000 - val_loss: 3.7200 - val_acc: 0.6971\n",
      "Epoch 460/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2556e-07 - acc: 1.0000 - val_loss: 3.7173 - val_acc: 0.6971\n",
      "Epoch 461/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3104e-07 - acc: 1.0000 - val_loss: 3.7466 - val_acc: 0.6971\n",
      "Epoch 462/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0341e-06 - acc: 1.0000 - val_loss: 3.6754 - val_acc: 0.7019\n",
      "Epoch 463/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8046e-07 - acc: 1.0000 - val_loss: 3.6814 - val_acc: 0.6987\n",
      "Epoch 464/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9497e-07 - acc: 1.0000 - val_loss: 3.6441 - val_acc: 0.7019\n",
      "Epoch 465/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8329e-07 - acc: 1.0000 - val_loss: 3.6467 - val_acc: 0.7019\n",
      "Epoch 466/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2933e-07 - acc: 1.0000 - val_loss: 3.7290 - val_acc: 0.6987\n",
      "Epoch 467/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4893e-07 - acc: 1.0000 - val_loss: 3.7086 - val_acc: 0.6987\n",
      "Epoch 468/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9581e-07 - acc: 1.0000 - val_loss: 3.6717 - val_acc: 0.7019\n",
      "Epoch 469/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1830e-07 - acc: 1.0000 - val_loss: 3.7270 - val_acc: 0.6987\n",
      "Epoch 470/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3241e-07 - acc: 1.0000 - val_loss: 3.8198 - val_acc: 0.6971\n",
      "Epoch 471/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0267 - acc: 0.9955 - val_loss: 2.5048 - val_acc: 0.6763\n",
      "Epoch 472/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0122 - acc: 0.9944 - val_loss: 3.1067 - val_acc: 0.6827\n",
      "Epoch 473/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 3.5319 - val_acc: 0.6651\n",
      "Epoch 474/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 3.4286 - val_acc: 0.6731\n",
      "Epoch 475/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0337e-04 - acc: 1.0000 - val_loss: 3.5805 - val_acc: 0.6699\n",
      "Epoch 476/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6838e-05 - acc: 1.0000 - val_loss: 3.6396 - val_acc: 0.6731\n",
      "Epoch 477/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5318e-04 - acc: 1.0000 - val_loss: 3.5737 - val_acc: 0.6715\n",
      "Epoch 478/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 4.5714 - val_acc: 0.6522\n",
      "Epoch 479/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1366e-04 - acc: 1.0000 - val_loss: 4.0452 - val_acc: 0.6651\n",
      "Epoch 480/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7307e-05 - acc: 1.0000 - val_loss: 3.9963 - val_acc: 0.6699\n",
      "Epoch 481/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5730e-05 - acc: 1.0000 - val_loss: 3.9301 - val_acc: 0.6747\n",
      "Epoch 482/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 4.4847 - val_acc: 0.6571\n",
      "Epoch 483/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.2049 - val_acc: 0.6667\n",
      "Epoch 484/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 3.0270 - val_acc: 0.7131\n",
      "Epoch 485/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 3.5981 - val_acc: 0.6715\n",
      "Epoch 486/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4619e-04 - acc: 1.0000 - val_loss: 3.3466 - val_acc: 0.6827\n",
      "Epoch 487/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9935e-05 - acc: 1.0000 - val_loss: 3.3413 - val_acc: 0.6843\n",
      "Epoch 488/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 3.1682 - val_acc: 0.6907\n",
      "Epoch 489/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0052 - acc: 0.9974 - val_loss: 3.7035 - val_acc: 0.6763\n",
      "Epoch 490/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9861e-04 - acc: 0.9996 - val_loss: 3.9316 - val_acc: 0.6651\n",
      "Epoch 491/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4241e-05 - acc: 1.0000 - val_loss: 4.0240 - val_acc: 0.6635\n",
      "Epoch 492/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4188e-05 - acc: 1.0000 - val_loss: 3.9968 - val_acc: 0.6635\n",
      "Epoch 493/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5160e-05 - acc: 1.0000 - val_loss: 3.9156 - val_acc: 0.6651\n",
      "Epoch 494/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6934e-05 - acc: 1.0000 - val_loss: 3.9911 - val_acc: 0.6635\n",
      "Epoch 495/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5383e-06 - acc: 1.0000 - val_loss: 3.9505 - val_acc: 0.6651\n",
      "Epoch 496/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0305e-05 - acc: 1.0000 - val_loss: 3.9421 - val_acc: 0.6635\n",
      "Epoch 497/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9545e-06 - acc: 1.0000 - val_loss: 3.9711 - val_acc: 0.6635\n",
      "Epoch 498/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9459e-06 - acc: 1.0000 - val_loss: 3.9818 - val_acc: 0.6635\n",
      "Epoch 499/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4153e-05 - acc: 1.0000 - val_loss: 3.9705 - val_acc: 0.6635\n",
      "Epoch 500/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.1046e-06 - acc: 1.0000 - val_loss: 3.9061 - val_acc: 0.6651\n",
      "Epoch 501/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4833e-06 - acc: 1.0000 - val_loss: 3.9026 - val_acc: 0.6651\n",
      "Epoch 502/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5361e-06 - acc: 1.0000 - val_loss: 3.9530 - val_acc: 0.6651\n",
      "Epoch 503/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0702e-06 - acc: 1.0000 - val_loss: 3.8853 - val_acc: 0.6699\n",
      "Epoch 504/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1775e-05 - acc: 1.0000 - val_loss: 3.8616 - val_acc: 0.6715\n",
      "Epoch 505/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7726e-06 - acc: 1.0000 - val_loss: 3.8823 - val_acc: 0.6683\n",
      "Epoch 506/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6664e-06 - acc: 1.0000 - val_loss: 3.9080 - val_acc: 0.6683\n",
      "Epoch 507/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6691e-06 - acc: 1.0000 - val_loss: 3.9175 - val_acc: 0.6683\n",
      "Epoch 508/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1317e-06 - acc: 1.0000 - val_loss: 3.9477 - val_acc: 0.6667\n",
      "Epoch 509/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6059e-06 - acc: 1.0000 - val_loss: 3.9799 - val_acc: 0.6635\n",
      "Epoch 510/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9768e-06 - acc: 1.0000 - val_loss: 3.9327 - val_acc: 0.6683\n",
      "Epoch 511/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4063e-06 - acc: 1.0000 - val_loss: 3.9909 - val_acc: 0.6651\n",
      "Epoch 512/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6664e-06 - acc: 1.0000 - val_loss: 3.9973 - val_acc: 0.6635\n",
      "Epoch 513/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8296e-06 - acc: 1.0000 - val_loss: 3.9475 - val_acc: 0.6699\n",
      "Epoch 514/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3370e-06 - acc: 1.0000 - val_loss: 3.9643 - val_acc: 0.6667\n",
      "Epoch 515/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3095e-06 - acc: 1.0000 - val_loss: 3.9783 - val_acc: 0.6651\n",
      "Epoch 516/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6403e-06 - acc: 1.0000 - val_loss: 3.9647 - val_acc: 0.6699\n",
      "Epoch 517/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0075e-06 - acc: 1.0000 - val_loss: 3.9435 - val_acc: 0.6715\n",
      "Epoch 518/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2209e-06 - acc: 1.0000 - val_loss: 3.9697 - val_acc: 0.6683\n",
      "Epoch 519/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3094e-06 - acc: 1.0000 - val_loss: 3.9843 - val_acc: 0.6683\n",
      "Epoch 520/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.8300e-07 - acc: 1.0000 - val_loss: 3.9617 - val_acc: 0.6715\n",
      "Epoch 521/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3624e-06 - acc: 1.0000 - val_loss: 3.9733 - val_acc: 0.6715\n",
      "Epoch 522/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9497e-06 - acc: 1.0000 - val_loss: 4.0432 - val_acc: 0.6635\n",
      "Epoch 523/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7624e-06 - acc: 1.0000 - val_loss: 4.0397 - val_acc: 0.6635\n",
      "Epoch 524/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7505e-06 - acc: 1.0000 - val_loss: 3.9962 - val_acc: 0.6699\n",
      "Epoch 525/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0035e-06 - acc: 1.0000 - val_loss: 3.9888 - val_acc: 0.6715\n",
      "Epoch 526/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4452e-06 - acc: 1.0000 - val_loss: 4.0176 - val_acc: 0.6683\n",
      "Epoch 527/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2864e-06 - acc: 1.0000 - val_loss: 4.1076 - val_acc: 0.6635\n",
      "Epoch 528/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1534e-06 - acc: 1.0000 - val_loss: 4.0680 - val_acc: 0.6651\n",
      "Epoch 529/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4210e-07 - acc: 1.0000 - val_loss: 3.9996 - val_acc: 0.6699\n",
      "Epoch 530/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6083e-07 - acc: 1.0000 - val_loss: 4.0136 - val_acc: 0.6699\n",
      "Epoch 531/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2176e-06 - acc: 1.0000 - val_loss: 3.8024 - val_acc: 0.6795\n",
      "Epoch 532/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3590e-07 - acc: 1.0000 - val_loss: 3.8269 - val_acc: 0.6795\n",
      "Epoch 533/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4697e-07 - acc: 1.0000 - val_loss: 3.8590 - val_acc: 0.6763\n",
      "Epoch 534/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2647e-07 - acc: 1.0000 - val_loss: 3.9193 - val_acc: 0.6747\n",
      "Epoch 535/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.9049e-07 - acc: 1.0000 - val_loss: 3.9507 - val_acc: 0.6683\n",
      "Epoch 536/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6701e-07 - acc: 1.0000 - val_loss: 3.9537 - val_acc: 0.6683\n",
      "Epoch 537/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1782e-06 - acc: 1.0000 - val_loss: 3.9022 - val_acc: 0.6763\n",
      "Epoch 538/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7981e-07 - acc: 1.0000 - val_loss: 3.8944 - val_acc: 0.6779\n",
      "Epoch 539/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2883e-07 - acc: 1.0000 - val_loss: 3.9474 - val_acc: 0.6715\n",
      "Epoch 540/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9743e-07 - acc: 1.0000 - val_loss: 3.9359 - val_acc: 0.6731\n",
      "Epoch 541/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.3000e-07 - acc: 1.0000 - val_loss: 3.9457 - val_acc: 0.6715\n",
      "Epoch 542/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0404e-07 - acc: 1.0000 - val_loss: 3.9615 - val_acc: 0.6731\n",
      "Epoch 543/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7904e-07 - acc: 1.0000 - val_loss: 3.9874 - val_acc: 0.6731\n",
      "Epoch 544/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0031e-07 - acc: 1.0000 - val_loss: 3.9941 - val_acc: 0.6731\n",
      "Epoch 545/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.0162e-07 - acc: 1.0000 - val_loss: 3.9921 - val_acc: 0.6731\n",
      "Epoch 546/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6076e-07 - acc: 1.0000 - val_loss: 3.9722 - val_acc: 0.6731\n",
      "Epoch 547/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1171e-07 - acc: 1.0000 - val_loss: 3.9948 - val_acc: 0.6731\n",
      "Epoch 548/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0558e-07 - acc: 1.0000 - val_loss: 3.9900 - val_acc: 0.6731\n",
      "Epoch 549/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1370e-07 - acc: 1.0000 - val_loss: 3.9636 - val_acc: 0.6731\n",
      "Epoch 550/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0145e-07 - acc: 1.0000 - val_loss: 3.9833 - val_acc: 0.6731\n",
      "Epoch 551/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5362e-07 - acc: 1.0000 - val_loss: 4.0046 - val_acc: 0.6731\n",
      "Epoch 552/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7589e-07 - acc: 1.0000 - val_loss: 3.9709 - val_acc: 0.6731\n",
      "Epoch 553/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4186e-07 - acc: 1.0000 - val_loss: 4.1342 - val_acc: 0.6683\n",
      "Epoch 554/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8583e-07 - acc: 1.0000 - val_loss: 4.1221 - val_acc: 0.6699\n",
      "Epoch 555/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0323 - acc: 0.9929 - val_loss: 3.6561 - val_acc: 0.6859\n",
      "Epoch 556/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0058 - acc: 0.9989 - val_loss: 3.8360 - val_acc: 0.6651\n",
      "Epoch 557/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 3.8122 - val_acc: 0.6763\n",
      "Epoch 558/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2080e-04 - acc: 1.0000 - val_loss: 3.5129 - val_acc: 0.6875\n",
      "Epoch 559/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0919e-05 - acc: 1.0000 - val_loss: 3.5029 - val_acc: 0.6907\n",
      "Epoch 560/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6664e-04 - acc: 1.0000 - val_loss: 3.3573 - val_acc: 0.6955\n",
      "Epoch 561/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6150e-05 - acc: 1.0000 - val_loss: 3.4770 - val_acc: 0.6923\n",
      "Epoch 562/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3396e-04 - acc: 1.0000 - val_loss: 3.4337 - val_acc: 0.6939\n",
      "Epoch 563/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 2.3539 - val_acc: 0.7356\n",
      "Epoch 564/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1061e-04 - acc: 1.0000 - val_loss: 3.8319 - val_acc: 0.6779\n",
      "Epoch 565/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0035e-05 - acc: 1.0000 - val_loss: 3.7682 - val_acc: 0.6811\n",
      "Epoch 566/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6875e-04 - acc: 1.0000 - val_loss: 4.1031 - val_acc: 0.6683\n",
      "Epoch 567/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.5121e-05 - acc: 1.0000 - val_loss: 3.6762 - val_acc: 0.6891\n",
      "Epoch 568/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7951e-06 - acc: 1.0000 - val_loss: 3.7050 - val_acc: 0.6891\n",
      "Epoch 569/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8126e-05 - acc: 1.0000 - val_loss: 3.5211 - val_acc: 0.6971\n",
      "Epoch 570/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1183e-06 - acc: 1.0000 - val_loss: 3.5566 - val_acc: 0.6955\n",
      "Epoch 571/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2894e-05 - acc: 1.0000 - val_loss: 3.6256 - val_acc: 0.6907\n",
      "Epoch 572/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7233e-06 - acc: 1.0000 - val_loss: 3.6952 - val_acc: 0.6875\n",
      "Epoch 573/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9933e-06 - acc: 1.0000 - val_loss: 3.6927 - val_acc: 0.6875\n",
      "Epoch 574/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9694e-06 - acc: 1.0000 - val_loss: 3.6938 - val_acc: 0.6875\n",
      "Epoch 575/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1093e-05 - acc: 1.0000 - val_loss: 3.6497 - val_acc: 0.6923\n",
      "Epoch 576/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.8488e-06 - acc: 1.0000 - val_loss: 3.7095 - val_acc: 0.6875\n",
      "Epoch 577/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.3373e-06 - acc: 1.0000 - val_loss: 3.6493 - val_acc: 0.6923\n",
      "Epoch 578/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0831e-06 - acc: 1.0000 - val_loss: 3.6421 - val_acc: 0.6923\n",
      "Epoch 579/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5055e-06 - acc: 1.0000 - val_loss: 3.6903 - val_acc: 0.6907\n",
      "Epoch 580/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7755e-06 - acc: 1.0000 - val_loss: 3.7524 - val_acc: 0.6843\n",
      "Epoch 581/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6131e-06 - acc: 1.0000 - val_loss: 3.7383 - val_acc: 0.6859\n",
      "Epoch 582/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.7486e-06 - acc: 1.0000 - val_loss: 3.7495 - val_acc: 0.6859\n",
      "Epoch 583/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8014e-06 - acc: 1.0000 - val_loss: 3.7577 - val_acc: 0.6859\n",
      "Epoch 584/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1254e-06 - acc: 1.0000 - val_loss: 3.6698 - val_acc: 0.6939\n",
      "Epoch 585/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4466e-06 - acc: 1.0000 - val_loss: 3.6908 - val_acc: 0.6923\n",
      "Epoch 586/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8455e-06 - acc: 1.0000 - val_loss: 3.6811 - val_acc: 0.6955\n",
      "Epoch 587/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9195e-06 - acc: 1.0000 - val_loss: 3.7031 - val_acc: 0.6923\n",
      "Epoch 588/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7442e-06 - acc: 1.0000 - val_loss: 3.7430 - val_acc: 0.6891\n",
      "Epoch 589/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3461e-05 - acc: 1.0000 - val_loss: 3.5066 - val_acc: 0.6987\n",
      "Epoch 590/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9809e-06 - acc: 1.0000 - val_loss: 3.5409 - val_acc: 0.6971\n",
      "Epoch 591/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3193e-06 - acc: 1.0000 - val_loss: 3.4669 - val_acc: 0.7019\n",
      "Epoch 592/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8538e-06 - acc: 1.0000 - val_loss: 3.4033 - val_acc: 0.7051\n",
      "Epoch 593/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5163e-06 - acc: 1.0000 - val_loss: 3.4994 - val_acc: 0.7019\n",
      "Epoch 594/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1407e-06 - acc: 1.0000 - val_loss: 3.5279 - val_acc: 0.7019\n",
      "Epoch 595/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3169e-06 - acc: 1.0000 - val_loss: 3.4976 - val_acc: 0.7019\n",
      "Epoch 596/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5356e-07 - acc: 1.0000 - val_loss: 3.4807 - val_acc: 0.7035\n",
      "Epoch 597/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5497e-07 - acc: 1.0000 - val_loss: 3.4932 - val_acc: 0.7035\n",
      "Epoch 598/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6498e-06 - acc: 1.0000 - val_loss: 3.5995 - val_acc: 0.6987\n",
      "Epoch 599/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4849e-07 - acc: 1.0000 - val_loss: 3.6224 - val_acc: 0.6955\n",
      "Epoch 600/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4442e-06 - acc: 1.0000 - val_loss: 3.6595 - val_acc: 0.6955\n",
      "Epoch 601/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0726e-07 - acc: 1.0000 - val_loss: 3.6371 - val_acc: 0.6955\n",
      "Epoch 602/1000\n",
      "2682/2682 [==============================] - 47s 17ms/step - loss: 2.5862e-06 - acc: 1.0000 - val_loss: 3.8863 - val_acc: 0.6811\n",
      "Epoch 603/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0724e-06 - acc: 1.0000 - val_loss: 3.8013 - val_acc: 0.6827\n",
      "Epoch 604/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.5114e-07 - acc: 1.0000 - val_loss: 3.7583 - val_acc: 0.6907\n",
      "Epoch 605/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7366e-07 - acc: 1.0000 - val_loss: 3.7423 - val_acc: 0.6907\n",
      "Epoch 606/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8475e-07 - acc: 1.0000 - val_loss: 3.7385 - val_acc: 0.6907\n",
      "Epoch 607/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9756e-07 - acc: 1.0000 - val_loss: 3.7061 - val_acc: 0.6939\n",
      "Epoch 608/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5460e-07 - acc: 1.0000 - val_loss: 3.7072 - val_acc: 0.6939\n",
      "Epoch 609/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5376e-07 - acc: 1.0000 - val_loss: 3.7014 - val_acc: 0.6955\n",
      "Epoch 610/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0690e-07 - acc: 1.0000 - val_loss: 3.7653 - val_acc: 0.6891\n",
      "Epoch 611/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6418e-07 - acc: 1.0000 - val_loss: 3.7454 - val_acc: 0.6907\n",
      "Epoch 612/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8396e-07 - acc: 1.0000 - val_loss: 3.7264 - val_acc: 0.6923\n",
      "Epoch 613/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2052e-07 - acc: 1.0000 - val_loss: 3.7376 - val_acc: 0.6907\n",
      "Epoch 614/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6478e-07 - acc: 1.0000 - val_loss: 3.7592 - val_acc: 0.6907\n",
      "Epoch 615/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2357e-07 - acc: 1.0000 - val_loss: 3.7561 - val_acc: 0.6923\n",
      "Epoch 616/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1311e-07 - acc: 1.0000 - val_loss: 3.7259 - val_acc: 0.6955\n",
      "Epoch 617/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5528e-07 - acc: 1.0000 - val_loss: 3.7445 - val_acc: 0.6923\n",
      "Epoch 618/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6954e-07 - acc: 1.0000 - val_loss: 3.7519 - val_acc: 0.6939\n",
      "Epoch 619/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4004e-07 - acc: 1.0000 - val_loss: 3.7620 - val_acc: 0.6939\n",
      "Epoch 620/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2631e-07 - acc: 1.0000 - val_loss: 3.7521 - val_acc: 0.6939\n",
      "Epoch 621/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2024e-07 - acc: 1.0000 - val_loss: 3.8028 - val_acc: 0.6907\n",
      "Epoch 622/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4189e-07 - acc: 1.0000 - val_loss: 3.7648 - val_acc: 0.6939\n",
      "Epoch 623/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6672e-07 - acc: 1.0000 - val_loss: 3.7892 - val_acc: 0.6907\n",
      "Epoch 624/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6924e-07 - acc: 1.0000 - val_loss: 3.7953 - val_acc: 0.6907\n",
      "Epoch 625/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7908e-07 - acc: 1.0000 - val_loss: 3.7665 - val_acc: 0.6955\n",
      "Epoch 626/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9472e-07 - acc: 1.0000 - val_loss: 3.8010 - val_acc: 0.6923\n",
      "Epoch 627/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3155e-07 - acc: 1.0000 - val_loss: 3.7683 - val_acc: 0.6955\n",
      "Epoch 628/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7775e-07 - acc: 1.0000 - val_loss: 3.7632 - val_acc: 0.6955\n",
      "Epoch 629/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3144e-07 - acc: 1.0000 - val_loss: 3.8379 - val_acc: 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 630/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8284e-07 - acc: 1.0000 - val_loss: 3.8613 - val_acc: 0.6891\n",
      "Epoch 631/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5857e-07 - acc: 1.0000 - val_loss: 4.0636 - val_acc: 0.6811\n",
      "Epoch 632/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1105e-07 - acc: 1.0000 - val_loss: 3.9551 - val_acc: 0.6843\n",
      "Epoch 633/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1579e-06 - acc: 1.0000 - val_loss: 3.7602 - val_acc: 0.6923\n",
      "Epoch 634/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0246 - acc: 0.9948 - val_loss: 2.4922 - val_acc: 0.7468\n",
      "Epoch 635/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0066 - acc: 0.9974 - val_loss: 2.6168 - val_acc: 0.7276\n",
      "Epoch 636/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 3.7807 - val_acc: 0.6715\n",
      "Epoch 637/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8069e-04 - acc: 1.0000 - val_loss: 3.7637 - val_acc: 0.6731\n",
      "Epoch 638/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3199e-04 - acc: 1.0000 - val_loss: 3.7671 - val_acc: 0.6731\n",
      "Epoch 639/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4945e-05 - acc: 1.0000 - val_loss: 3.6977 - val_acc: 0.6795\n",
      "Epoch 640/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8287e-05 - acc: 1.0000 - val_loss: 3.7004 - val_acc: 0.6795\n",
      "Epoch 641/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2110e-05 - acc: 1.0000 - val_loss: 3.7103 - val_acc: 0.6795\n",
      "Epoch 642/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1803e-05 - acc: 1.0000 - val_loss: 3.7630 - val_acc: 0.6763\n",
      "Epoch 643/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1093e-05 - acc: 1.0000 - val_loss: 3.8191 - val_acc: 0.6747\n",
      "Epoch 644/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1642e-06 - acc: 1.0000 - val_loss: 3.7797 - val_acc: 0.6779\n",
      "Epoch 645/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0788e-05 - acc: 1.0000 - val_loss: 3.7773 - val_acc: 0.6779\n",
      "Epoch 646/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4488e-05 - acc: 1.0000 - val_loss: 3.7508 - val_acc: 0.6811\n",
      "Epoch 647/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7374e-05 - acc: 1.0000 - val_loss: 3.7310 - val_acc: 0.6811\n",
      "Epoch 648/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2202e-05 - acc: 1.0000 - val_loss: 3.9677 - val_acc: 0.6763\n",
      "Epoch 649/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3335e-05 - acc: 1.0000 - val_loss: 3.9277 - val_acc: 0.6795\n",
      "Epoch 650/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.6520e-06 - acc: 1.0000 - val_loss: 3.9351 - val_acc: 0.6795\n",
      "Epoch 651/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1321e-05 - acc: 1.0000 - val_loss: 3.9866 - val_acc: 0.6763\n",
      "Epoch 652/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4386e-05 - acc: 1.0000 - val_loss: 3.9309 - val_acc: 0.6795\n",
      "Epoch 653/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1594e-06 - acc: 1.0000 - val_loss: 3.7137 - val_acc: 0.6827\n",
      "Epoch 654/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5389e-05 - acc: 1.0000 - val_loss: 3.8459 - val_acc: 0.6811\n",
      "Epoch 655/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6941e-06 - acc: 1.0000 - val_loss: 3.8397 - val_acc: 0.6811\n",
      "Epoch 656/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9305e-06 - acc: 1.0000 - val_loss: 3.9607 - val_acc: 0.6763\n",
      "Epoch 657/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5327e-06 - acc: 1.0000 - val_loss: 3.9191 - val_acc: 0.6795\n",
      "Epoch 658/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8569e-06 - acc: 1.0000 - val_loss: 3.9345 - val_acc: 0.6795\n",
      "Epoch 659/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8164e-06 - acc: 1.0000 - val_loss: 3.8928 - val_acc: 0.6795\n",
      "Epoch 660/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6812e-06 - acc: 1.0000 - val_loss: 3.8927 - val_acc: 0.6795\n",
      "Epoch 661/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3729e-04 - acc: 1.0000 - val_loss: 5.2182 - val_acc: 0.6410\n",
      "Epoch 662/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0467 - acc: 0.9888 - val_loss: 2.0436 - val_acc: 0.7340\n",
      "Epoch 663/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 2.0334 - val_acc: 0.7340\n",
      "Epoch 664/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 2.5257 - val_acc: 0.7035\n",
      "Epoch 665/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.4579e-05 - acc: 1.0000 - val_loss: 2.5322 - val_acc: 0.7099\n",
      "Epoch 666/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4302e-05 - acc: 1.0000 - val_loss: 2.5953 - val_acc: 0.7019\n",
      "Epoch 667/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1270e-05 - acc: 1.0000 - val_loss: 2.7202 - val_acc: 0.6987\n",
      "Epoch 668/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0869e-05 - acc: 1.0000 - val_loss: 2.6743 - val_acc: 0.6987\n",
      "Epoch 669/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5050e-05 - acc: 1.0000 - val_loss: 2.7538 - val_acc: 0.6987\n",
      "Epoch 670/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2965e-05 - acc: 1.0000 - val_loss: 2.7735 - val_acc: 0.6987\n",
      "Epoch 671/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0688e-05 - acc: 1.0000 - val_loss: 2.7698 - val_acc: 0.6987\n",
      "Epoch 672/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9533e-05 - acc: 1.0000 - val_loss: 2.8409 - val_acc: 0.6987\n",
      "Epoch 673/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3368e-05 - acc: 1.0000 - val_loss: 2.9086 - val_acc: 0.6987\n",
      "Epoch 674/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9504e-05 - acc: 1.0000 - val_loss: 2.9061 - val_acc: 0.7003\n",
      "Epoch 675/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7129e-06 - acc: 1.0000 - val_loss: 2.9063 - val_acc: 0.7003\n",
      "Epoch 676/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5640e-06 - acc: 1.0000 - val_loss: 2.9327 - val_acc: 0.6987\n",
      "Epoch 677/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2830e-06 - acc: 1.0000 - val_loss: 2.9036 - val_acc: 0.7003\n",
      "Epoch 678/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0666e-06 - acc: 1.0000 - val_loss: 2.8842 - val_acc: 0.7035\n",
      "Epoch 679/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3541e-05 - acc: 1.0000 - val_loss: 2.9156 - val_acc: 0.7019\n",
      "Epoch 680/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0345e-05 - acc: 1.0000 - val_loss: 2.9528 - val_acc: 0.7003\n",
      "Epoch 681/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3199e-06 - acc: 1.0000 - val_loss: 2.9600 - val_acc: 0.7003\n",
      "Epoch 682/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3837e-06 - acc: 1.0000 - val_loss: 2.9961 - val_acc: 0.7003\n",
      "Epoch 683/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6151e-06 - acc: 1.0000 - val_loss: 3.0106 - val_acc: 0.7003\n",
      "Epoch 684/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7295e-06 - acc: 1.0000 - val_loss: 2.9885 - val_acc: 0.7019\n",
      "Epoch 685/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6592e-06 - acc: 1.0000 - val_loss: 3.0297 - val_acc: 0.6987\n",
      "Epoch 686/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0863e-06 - acc: 1.0000 - val_loss: 3.0078 - val_acc: 0.7019\n",
      "Epoch 687/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2645e-05 - acc: 1.0000 - val_loss: 3.1859 - val_acc: 0.6971\n",
      "Epoch 688/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2362e-04 - acc: 1.0000 - val_loss: 3.2571 - val_acc: 0.7003\n",
      "Epoch 689/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5739e-06 - acc: 1.0000 - val_loss: 3.2939 - val_acc: 0.7019\n",
      "Epoch 690/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6909e-06 - acc: 1.0000 - val_loss: 3.3114 - val_acc: 0.7019\n",
      "Epoch 691/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3911e-06 - acc: 1.0000 - val_loss: 3.3348 - val_acc: 0.7019\n",
      "Epoch 692/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0575e-06 - acc: 1.0000 - val_loss: 3.3370 - val_acc: 0.7019\n",
      "Epoch 693/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0141e-06 - acc: 1.0000 - val_loss: 3.3515 - val_acc: 0.7019\n",
      "Epoch 694/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6872e-06 - acc: 1.0000 - val_loss: 3.3426 - val_acc: 0.7019\n",
      "Epoch 695/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8580e-06 - acc: 1.0000 - val_loss: 3.3796 - val_acc: 0.7019\n",
      "Epoch 696/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3835e-06 - acc: 1.0000 - val_loss: 3.3877 - val_acc: 0.7019\n",
      "Epoch 697/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8335e-06 - acc: 1.0000 - val_loss: 3.4409 - val_acc: 0.7003\n",
      "Epoch 698/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2918e-06 - acc: 1.0000 - val_loss: 3.4416 - val_acc: 0.7003\n",
      "Epoch 699/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0828e-06 - acc: 1.0000 - val_loss: 3.4885 - val_acc: 0.6971\n",
      "Epoch 700/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0773e-06 - acc: 1.0000 - val_loss: 3.4805 - val_acc: 0.7019\n",
      "Epoch 701/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0795e-06 - acc: 1.0000 - val_loss: 3.4858 - val_acc: 0.7003\n",
      "Epoch 702/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0572e-06 - acc: 1.0000 - val_loss: 3.4605 - val_acc: 0.7019\n",
      "Epoch 703/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0107e-07 - acc: 1.0000 - val_loss: 3.4828 - val_acc: 0.7019\n",
      "Epoch 704/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5872e-06 - acc: 1.0000 - val_loss: 3.5011 - val_acc: 0.7003\n",
      "Epoch 705/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6103e-07 - acc: 1.0000 - val_loss: 3.4740 - val_acc: 0.7035\n",
      "Epoch 706/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6085e-06 - acc: 1.0000 - val_loss: 3.6583 - val_acc: 0.6939\n",
      "Epoch 707/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9306e-07 - acc: 1.0000 - val_loss: 3.6307 - val_acc: 0.6939\n",
      "Epoch 708/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.3330e-07 - acc: 1.0000 - val_loss: 3.6335 - val_acc: 0.6939\n",
      "Epoch 709/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2140e-07 - acc: 1.0000 - val_loss: 3.5752 - val_acc: 0.6955\n",
      "Epoch 710/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9961e-07 - acc: 1.0000 - val_loss: 3.5734 - val_acc: 0.6971\n",
      "Epoch 711/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0839e-07 - acc: 1.0000 - val_loss: 3.6105 - val_acc: 0.6939\n",
      "Epoch 712/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6540e-06 - acc: 1.0000 - val_loss: 3.4583 - val_acc: 0.7019\n",
      "Epoch 713/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5546e-07 - acc: 1.0000 - val_loss: 3.4802 - val_acc: 0.7019\n",
      "Epoch 714/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8157e-07 - acc: 1.0000 - val_loss: 3.5131 - val_acc: 0.7019\n",
      "Epoch 715/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.3430e-07 - acc: 1.0000 - val_loss: 3.6267 - val_acc: 0.6971\n",
      "Epoch 716/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5914e-07 - acc: 1.0000 - val_loss: 3.6294 - val_acc: 0.6955\n",
      "Epoch 717/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5931e-07 - acc: 1.0000 - val_loss: 3.6088 - val_acc: 0.6971\n",
      "Epoch 718/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.1707e-07 - acc: 1.0000 - val_loss: 3.6134 - val_acc: 0.6971\n",
      "Epoch 719/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.5368e-07 - acc: 1.0000 - val_loss: 3.6254 - val_acc: 0.6971\n",
      "Epoch 720/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1702e-06 - acc: 1.0000 - val_loss: 3.6755 - val_acc: 0.6955\n",
      "Epoch 721/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6350e-07 - acc: 1.0000 - val_loss: 3.7536 - val_acc: 0.6939\n",
      "Epoch 722/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2224e-07 - acc: 1.0000 - val_loss: 3.6943 - val_acc: 0.6955\n",
      "Epoch 723/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2267e-06 - acc: 1.0000 - val_loss: 3.8523 - val_acc: 0.6891\n",
      "Epoch 724/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.1024e-07 - acc: 1.0000 - val_loss: 3.8598 - val_acc: 0.6891\n",
      "Epoch 725/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8893e-07 - acc: 1.0000 - val_loss: 3.8775 - val_acc: 0.6891\n",
      "Epoch 726/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0461e-07 - acc: 1.0000 - val_loss: 3.8691 - val_acc: 0.6891\n",
      "Epoch 727/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2531e-07 - acc: 1.0000 - val_loss: 3.8507 - val_acc: 0.6907\n",
      "Epoch 728/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6748e-07 - acc: 1.0000 - val_loss: 3.8366 - val_acc: 0.6907\n",
      "Epoch 729/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6138e-07 - acc: 1.0000 - val_loss: 3.6517 - val_acc: 0.7019\n",
      "Epoch 730/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7741e-07 - acc: 1.0000 - val_loss: 3.6788 - val_acc: 0.7003\n",
      "Epoch 731/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5326e-07 - acc: 1.0000 - val_loss: 3.6385 - val_acc: 0.7019\n",
      "Epoch 732/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0397e-07 - acc: 1.0000 - val_loss: 3.6968 - val_acc: 0.7003\n",
      "Epoch 733/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9575e-07 - acc: 1.0000 - val_loss: 3.7044 - val_acc: 0.7003\n",
      "Epoch 734/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3576e-07 - acc: 1.0000 - val_loss: 3.7467 - val_acc: 0.6987\n",
      "Epoch 735/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7957e-07 - acc: 1.0000 - val_loss: 3.7228 - val_acc: 0.6987\n",
      "Epoch 736/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4988e-07 - acc: 1.0000 - val_loss: 3.7356 - val_acc: 0.6987\n",
      "Epoch 737/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1853e-07 - acc: 1.0000 - val_loss: 3.7985 - val_acc: 0.6971\n",
      "Epoch 738/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0111e-07 - acc: 1.0000 - val_loss: 3.7396 - val_acc: 0.6987\n",
      "Epoch 739/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4803e-07 - acc: 1.0000 - val_loss: 3.7500 - val_acc: 0.6987\n",
      "Epoch 740/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5361e-07 - acc: 1.0000 - val_loss: 3.7419 - val_acc: 0.7003\n",
      "Epoch 741/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6704e-07 - acc: 1.0000 - val_loss: 3.7737 - val_acc: 0.6987\n",
      "Epoch 742/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9606e-07 - acc: 1.0000 - val_loss: 3.7425 - val_acc: 0.6987\n",
      "Epoch 743/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6284e-07 - acc: 1.0000 - val_loss: 3.7534 - val_acc: 0.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 744/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4103e-07 - acc: 1.0000 - val_loss: 3.7647 - val_acc: 0.6987\n",
      "Epoch 745/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7775e-07 - acc: 1.0000 - val_loss: 3.7948 - val_acc: 0.6971\n",
      "Epoch 746/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7308e-07 - acc: 1.0000 - val_loss: 3.7680 - val_acc: 0.7003\n",
      "Epoch 747/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5337e-07 - acc: 1.0000 - val_loss: 3.7674 - val_acc: 0.7003\n",
      "Epoch 748/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4955e-07 - acc: 1.0000 - val_loss: 3.7866 - val_acc: 0.6971\n",
      "Epoch 749/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4463e-07 - acc: 1.0000 - val_loss: 3.7758 - val_acc: 0.6971\n",
      "Epoch 750/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4457e-07 - acc: 1.0000 - val_loss: 3.7494 - val_acc: 0.7019\n",
      "Epoch 751/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3083e-07 - acc: 1.0000 - val_loss: 3.7645 - val_acc: 0.6987\n",
      "Epoch 752/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4450e-07 - acc: 1.0000 - val_loss: 3.7775 - val_acc: 0.6987\n",
      "Epoch 753/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4826e-07 - acc: 1.0000 - val_loss: 3.8151 - val_acc: 0.6971\n",
      "Epoch 754/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3234e-07 - acc: 1.0000 - val_loss: 3.8367 - val_acc: 0.6955\n",
      "Epoch 755/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3679e-07 - acc: 1.0000 - val_loss: 3.8128 - val_acc: 0.6971\n",
      "Epoch 756/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6829e-06 - acc: 1.0000 - val_loss: 3.8783 - val_acc: 0.6907\n",
      "Epoch 757/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6043e-07 - acc: 1.0000 - val_loss: 3.6904 - val_acc: 0.6971\n",
      "Epoch 758/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1313e-07 - acc: 1.0000 - val_loss: 3.9739 - val_acc: 0.6891\n",
      "Epoch 759/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2338e-07 - acc: 1.0000 - val_loss: 4.7489 - val_acc: 0.6587\n",
      "Epoch 760/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0234 - acc: 0.9940 - val_loss: 4.2712 - val_acc: 0.6795\n",
      "Epoch 761/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0148 - acc: 0.9966 - val_loss: 3.5305 - val_acc: 0.7115\n",
      "Epoch 762/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7031e-04 - acc: 1.0000 - val_loss: 3.7269 - val_acc: 0.7003\n",
      "Epoch 763/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.4551e-05 - acc: 1.0000 - val_loss: 3.8708 - val_acc: 0.6923\n",
      "Epoch 764/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9340e-05 - acc: 1.0000 - val_loss: 3.8852 - val_acc: 0.6891\n",
      "Epoch 765/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5819e-05 - acc: 1.0000 - val_loss: 3.8493 - val_acc: 0.6955\n",
      "Epoch 766/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 4.0440 - val_acc: 0.6731\n",
      "Epoch 767/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0964e-04 - acc: 1.0000 - val_loss: 3.6150 - val_acc: 0.6987\n",
      "Epoch 768/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8181e-05 - acc: 1.0000 - val_loss: 3.6817 - val_acc: 0.6987\n",
      "Epoch 769/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0666e-06 - acc: 1.0000 - val_loss: 3.7230 - val_acc: 0.6955\n",
      "Epoch 770/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2285e-06 - acc: 1.0000 - val_loss: 3.7251 - val_acc: 0.6955\n",
      "Epoch 771/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5025e-05 - acc: 1.0000 - val_loss: 3.7691 - val_acc: 0.6923\n",
      "Epoch 772/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0338e-06 - acc: 1.0000 - val_loss: 3.7876 - val_acc: 0.6907\n",
      "Epoch 773/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1124e-05 - acc: 1.0000 - val_loss: 3.7261 - val_acc: 0.6939\n",
      "Epoch 774/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2971e-04 - acc: 1.0000 - val_loss: 4.2371 - val_acc: 0.6747\n",
      "Epoch 775/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5639e-05 - acc: 1.0000 - val_loss: 4.1533 - val_acc: 0.6763\n",
      "Epoch 776/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2048e-04 - acc: 1.0000 - val_loss: 3.5619 - val_acc: 0.7196\n",
      "Epoch 777/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3020e-05 - acc: 1.0000 - val_loss: 3.9582 - val_acc: 0.6859\n",
      "Epoch 778/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5328e-06 - acc: 1.0000 - val_loss: 3.9838 - val_acc: 0.6843\n",
      "Epoch 779/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3661e-06 - acc: 1.0000 - val_loss: 3.9529 - val_acc: 0.6843\n",
      "Epoch 780/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7729e-06 - acc: 1.0000 - val_loss: 3.9265 - val_acc: 0.6859\n",
      "Epoch 781/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6576e-06 - acc: 1.0000 - val_loss: 3.9468 - val_acc: 0.6859\n",
      "Epoch 782/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6191e-07 - acc: 1.0000 - val_loss: 3.9434 - val_acc: 0.6859\n",
      "Epoch 783/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.7295e-07 - acc: 1.0000 - val_loss: 3.9485 - val_acc: 0.6859\n",
      "Epoch 784/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6790e-07 - acc: 1.0000 - val_loss: 3.9378 - val_acc: 0.6859\n",
      "Epoch 785/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5969e-06 - acc: 1.0000 - val_loss: 3.9461 - val_acc: 0.6859\n",
      "Epoch 786/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6156e-06 - acc: 1.0000 - val_loss: 4.0209 - val_acc: 0.6827\n",
      "Epoch 787/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6318e-07 - acc: 1.0000 - val_loss: 3.9997 - val_acc: 0.6843\n",
      "Epoch 788/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.9197e-07 - acc: 1.0000 - val_loss: 3.9853 - val_acc: 0.6843\n",
      "Epoch 789/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2184e-06 - acc: 1.0000 - val_loss: 3.9631 - val_acc: 0.6859\n",
      "Epoch 790/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5438e-05 - acc: 1.0000 - val_loss: 3.7782 - val_acc: 0.6987\n",
      "Epoch 791/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.2721e-07 - acc: 1.0000 - val_loss: 3.8068 - val_acc: 0.6971\n",
      "Epoch 792/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4855e-07 - acc: 1.0000 - val_loss: 3.8054 - val_acc: 0.6971\n",
      "Epoch 793/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3188e-06 - acc: 1.0000 - val_loss: 3.8256 - val_acc: 0.6971\n",
      "Epoch 794/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.1371e-07 - acc: 1.0000 - val_loss: 3.8519 - val_acc: 0.6939\n",
      "Epoch 795/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6369e-07 - acc: 1.0000 - val_loss: 3.8490 - val_acc: 0.6939\n",
      "Epoch 796/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0445e-06 - acc: 1.0000 - val_loss: 3.8467 - val_acc: 0.6939\n",
      "Epoch 797/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4378e-07 - acc: 1.0000 - val_loss: 3.8527 - val_acc: 0.6939\n",
      "Epoch 798/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3651e-06 - acc: 1.0000 - val_loss: 3.8983 - val_acc: 0.6891\n",
      "Epoch 799/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.2468e-07 - acc: 1.0000 - val_loss: 3.8991 - val_acc: 0.6891\n",
      "Epoch 800/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4216e-06 - acc: 1.0000 - val_loss: 3.9452 - val_acc: 0.6859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 801/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.5327e-07 - acc: 1.0000 - val_loss: 3.9169 - val_acc: 0.6891\n",
      "Epoch 802/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3633e-07 - acc: 1.0000 - val_loss: 3.9434 - val_acc: 0.6859\n",
      "Epoch 803/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1309e-07 - acc: 1.0000 - val_loss: 3.9072 - val_acc: 0.6891\n",
      "Epoch 804/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.7221e-07 - acc: 1.0000 - val_loss: 3.9031 - val_acc: 0.6891\n",
      "Epoch 805/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5505e-06 - acc: 1.0000 - val_loss: 3.9828 - val_acc: 0.6827\n",
      "Epoch 806/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2844e-06 - acc: 1.0000 - val_loss: 3.9351 - val_acc: 0.6875\n",
      "Epoch 807/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4227e-06 - acc: 1.0000 - val_loss: 4.0226 - val_acc: 0.6843\n",
      "Epoch 808/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.4320e-07 - acc: 1.0000 - val_loss: 4.0077 - val_acc: 0.6843\n",
      "Epoch 809/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1782e-06 - acc: 1.0000 - val_loss: 3.9397 - val_acc: 0.6875\n",
      "Epoch 810/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8806e-07 - acc: 1.0000 - val_loss: 3.9188 - val_acc: 0.6875\n",
      "Epoch 811/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.2863e-07 - acc: 1.0000 - val_loss: 3.9553 - val_acc: 0.6875\n",
      "Epoch 812/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9372e-07 - acc: 1.0000 - val_loss: 3.9496 - val_acc: 0.6875\n",
      "Epoch 813/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6182e-07 - acc: 1.0000 - val_loss: 3.9477 - val_acc: 0.6859\n",
      "Epoch 814/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0628e-07 - acc: 1.0000 - val_loss: 3.9175 - val_acc: 0.6875\n",
      "Epoch 815/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5586e-07 - acc: 1.0000 - val_loss: 3.9208 - val_acc: 0.6875\n",
      "Epoch 816/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3656e-07 - acc: 1.0000 - val_loss: 3.9221 - val_acc: 0.6875\n",
      "Epoch 817/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0486e-07 - acc: 1.0000 - val_loss: 3.9224 - val_acc: 0.6875\n",
      "Epoch 818/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.0653e-07 - acc: 1.0000 - val_loss: 3.9357 - val_acc: 0.6875\n",
      "Epoch 819/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4862e-07 - acc: 1.0000 - val_loss: 3.9376 - val_acc: 0.6875\n",
      "Epoch 820/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9090e-07 - acc: 1.0000 - val_loss: 3.9341 - val_acc: 0.6875\n",
      "Epoch 821/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6246e-07 - acc: 1.0000 - val_loss: 3.9295 - val_acc: 0.6875\n",
      "Epoch 822/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6875e-07 - acc: 1.0000 - val_loss: 3.9135 - val_acc: 0.6875\n",
      "Epoch 823/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5751e-07 - acc: 1.0000 - val_loss: 3.9057 - val_acc: 0.6891\n",
      "Epoch 824/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5984e-07 - acc: 1.0000 - val_loss: 3.9300 - val_acc: 0.6875\n",
      "Epoch 825/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7950e-07 - acc: 1.0000 - val_loss: 3.9293 - val_acc: 0.6875\n",
      "Epoch 826/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6159e-07 - acc: 1.0000 - val_loss: 3.9651 - val_acc: 0.6859\n",
      "Epoch 827/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3698e-07 - acc: 1.0000 - val_loss: 3.9160 - val_acc: 0.6891\n",
      "Epoch 828/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7015e-07 - acc: 1.0000 - val_loss: 3.9269 - val_acc: 0.6875\n",
      "Epoch 829/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6788e-07 - acc: 1.0000 - val_loss: 3.9277 - val_acc: 0.6891\n",
      "Epoch 830/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8313e-07 - acc: 1.0000 - val_loss: 3.9121 - val_acc: 0.6891\n",
      "Epoch 831/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4886e-07 - acc: 1.0000 - val_loss: 3.8698 - val_acc: 0.6923\n",
      "Epoch 832/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7879e-07 - acc: 1.0000 - val_loss: 3.8691 - val_acc: 0.6923\n",
      "Epoch 833/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2147e-07 - acc: 1.0000 - val_loss: 3.9329 - val_acc: 0.6891\n",
      "Epoch 834/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5183e-07 - acc: 1.0000 - val_loss: 3.9196 - val_acc: 0.6891\n",
      "Epoch 835/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.8057e-07 - acc: 1.0000 - val_loss: 3.9663 - val_acc: 0.6875\n",
      "Epoch 836/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5730e-07 - acc: 1.0000 - val_loss: 3.9686 - val_acc: 0.6875\n",
      "Epoch 837/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0040 - acc: 0.9978 - val_loss: 2.8889 - val_acc: 0.7356\n",
      "Epoch 838/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0122 - acc: 0.9978 - val_loss: 5.3677 - val_acc: 0.6394\n",
      "Epoch 839/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0015 - acc: 0.9993 - val_loss: 4.3757 - val_acc: 0.6715\n",
      "Epoch 840/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 3.5795 - val_acc: 0.6859\n",
      "Epoch 841/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 2.4374 - val_acc: 0.7276\n",
      "Epoch 842/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0058 - acc: 0.9978 - val_loss: 2.7736 - val_acc: 0.7372\n",
      "Epoch 843/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5025e-04 - acc: 0.9996 - val_loss: 3.4941 - val_acc: 0.6891\n",
      "Epoch 844/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 4.5209 - val_acc: 0.6474\n",
      "Epoch 845/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 4.2855 - val_acc: 0.6651\n",
      "Epoch 846/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.4261e-04 - acc: 0.9996 - val_loss: 4.2546 - val_acc: 0.6667\n",
      "Epoch 847/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0062 - acc: 0.9996 - val_loss: 3.9887 - val_acc: 0.6859\n",
      "Epoch 848/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2336e-04 - acc: 1.0000 - val_loss: 4.0639 - val_acc: 0.6843\n",
      "Epoch 849/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 2.4393 - val_acc: 0.7452\n",
      "Epoch 850/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0106 - acc: 0.9974 - val_loss: 4.6557 - val_acc: 0.6538\n",
      "Epoch 851/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6276e-04 - acc: 1.0000 - val_loss: 3.9951 - val_acc: 0.6747\n",
      "Epoch 852/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.2244e-05 - acc: 1.0000 - val_loss: 3.9694 - val_acc: 0.6763\n",
      "Epoch 853/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.9737e-06 - acc: 1.0000 - val_loss: 3.9578 - val_acc: 0.6763\n",
      "Epoch 854/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4607e-05 - acc: 1.0000 - val_loss: 4.0025 - val_acc: 0.6747\n",
      "Epoch 855/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5214e-05 - acc: 1.0000 - val_loss: 4.0183 - val_acc: 0.6747\n",
      "Epoch 856/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2790e-05 - acc: 1.0000 - val_loss: 3.9144 - val_acc: 0.6811\n",
      "Epoch 857/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3018e-05 - acc: 1.0000 - val_loss: 4.1276 - val_acc: 0.6683\n",
      "Epoch 858/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.6246e-05 - acc: 1.0000 - val_loss: 3.9114 - val_acc: 0.6843\n",
      "Epoch 859/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7021e-04 - acc: 1.0000 - val_loss: 4.6550 - val_acc: 0.6571\n",
      "Epoch 860/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4192e-05 - acc: 1.0000 - val_loss: 4.5698 - val_acc: 0.6619\n",
      "Epoch 861/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7301e-05 - acc: 1.0000 - val_loss: 4.2781 - val_acc: 0.6699\n",
      "Epoch 862/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9659e-06 - acc: 1.0000 - val_loss: 4.2982 - val_acc: 0.6699\n",
      "Epoch 863/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3612e-06 - acc: 1.0000 - val_loss: 4.3162 - val_acc: 0.6699\n",
      "Epoch 864/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3408e-06 - acc: 1.0000 - val_loss: 4.3278 - val_acc: 0.6699\n",
      "Epoch 865/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.5726e-06 - acc: 1.0000 - val_loss: 4.3224 - val_acc: 0.6699\n",
      "Epoch 866/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.2664e-04 - acc: 0.9993 - val_loss: 3.7690 - val_acc: 0.7051\n",
      "Epoch 867/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0104 - acc: 0.9974 - val_loss: 3.7595 - val_acc: 0.7003\n",
      "Epoch 868/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0112 - acc: 0.9978 - val_loss: 3.7041 - val_acc: 0.7051\n",
      "Epoch 869/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5528e-05 - acc: 1.0000 - val_loss: 3.7482 - val_acc: 0.7019\n",
      "Epoch 870/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4108e-05 - acc: 1.0000 - val_loss: 3.8091 - val_acc: 0.6971\n",
      "Epoch 871/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2706e-04 - acc: 1.0000 - val_loss: 4.0445 - val_acc: 0.6795\n",
      "Epoch 872/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8041e-05 - acc: 1.0000 - val_loss: 3.9859 - val_acc: 0.6811\n",
      "Epoch 873/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6379e-06 - acc: 1.0000 - val_loss: 3.9580 - val_acc: 0.6843\n",
      "Epoch 874/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4007e-05 - acc: 1.0000 - val_loss: 4.0222 - val_acc: 0.6795\n",
      "Epoch 875/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0784e-05 - acc: 1.0000 - val_loss: 3.8084 - val_acc: 0.6971\n",
      "Epoch 876/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2354e-06 - acc: 1.0000 - val_loss: 3.8154 - val_acc: 0.7003\n",
      "Epoch 877/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7828e-06 - acc: 1.0000 - val_loss: 3.8086 - val_acc: 0.7019\n",
      "Epoch 878/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8913e-06 - acc: 1.0000 - val_loss: 3.8181 - val_acc: 0.7003\n",
      "Epoch 879/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5096e-06 - acc: 1.0000 - val_loss: 3.8397 - val_acc: 0.6987\n",
      "Epoch 880/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.1882e-07 - acc: 1.0000 - val_loss: 3.8356 - val_acc: 0.6987\n",
      "Epoch 881/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.2955e-06 - acc: 1.0000 - val_loss: 3.8373 - val_acc: 0.6987\n",
      "Epoch 882/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6885e-06 - acc: 1.0000 - val_loss: 3.8526 - val_acc: 0.6987\n",
      "Epoch 883/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3900e-06 - acc: 1.0000 - val_loss: 3.8390 - val_acc: 0.6987\n",
      "Epoch 884/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2704e-06 - acc: 1.0000 - val_loss: 3.8566 - val_acc: 0.6971\n",
      "Epoch 885/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.6137e-07 - acc: 1.0000 - val_loss: 3.8568 - val_acc: 0.6971\n",
      "Epoch 886/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1121e-06 - acc: 1.0000 - val_loss: 3.8503 - val_acc: 0.6971\n",
      "Epoch 887/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3037e-06 - acc: 1.0000 - val_loss: 3.8382 - val_acc: 0.6971\n",
      "Epoch 888/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8026e-06 - acc: 1.0000 - val_loss: 3.8838 - val_acc: 0.6939\n",
      "Epoch 889/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3472e-06 - acc: 1.0000 - val_loss: 3.8665 - val_acc: 0.6955\n",
      "Epoch 890/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5540e-07 - acc: 1.0000 - val_loss: 3.8604 - val_acc: 0.6971\n",
      "Epoch 891/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5814e-04 - acc: 1.0000 - val_loss: 4.6582 - val_acc: 0.6619\n",
      "Epoch 892/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0181 - acc: 0.9963 - val_loss: 2.9354 - val_acc: 0.7212\n",
      "Epoch 893/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0040 - acc: 0.9985 - val_loss: 3.5375 - val_acc: 0.7051\n",
      "Epoch 894/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0067 - acc: 0.9985 - val_loss: 3.5503 - val_acc: 0.7115\n",
      "Epoch 895/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0070 - acc: 0.9981 - val_loss: 3.7447 - val_acc: 0.6987\n",
      "Epoch 896/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2315e-04 - acc: 1.0000 - val_loss: 3.7687 - val_acc: 0.6971\n",
      "Epoch 897/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.7109e-05 - acc: 1.0000 - val_loss: 3.8422 - val_acc: 0.6971\n",
      "Epoch 898/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3824e-04 - acc: 0.9996 - val_loss: 3.0125 - val_acc: 0.7244\n",
      "Epoch 899/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7183e-04 - acc: 1.0000 - val_loss: 3.5209 - val_acc: 0.7131\n",
      "Epoch 900/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8022e-06 - acc: 1.0000 - val_loss: 3.5587 - val_acc: 0.7131\n",
      "Epoch 901/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0602e-05 - acc: 1.0000 - val_loss: 3.5322 - val_acc: 0.7131\n",
      "Epoch 902/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4098e-04 - acc: 1.0000 - val_loss: 3.4567 - val_acc: 0.7196\n",
      "Epoch 903/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9100e-06 - acc: 1.0000 - val_loss: 3.4598 - val_acc: 0.7196\n",
      "Epoch 904/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0440e-06 - acc: 1.0000 - val_loss: 3.4559 - val_acc: 0.7196\n",
      "Epoch 905/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0905e-05 - acc: 1.0000 - val_loss: 3.3705 - val_acc: 0.7212\n",
      "Epoch 906/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4387e-06 - acc: 1.0000 - val_loss: 3.3800 - val_acc: 0.7212\n",
      "Epoch 907/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4007e-06 - acc: 1.0000 - val_loss: 3.3809 - val_acc: 0.7212\n",
      "Epoch 908/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1962e-06 - acc: 1.0000 - val_loss: 3.4199 - val_acc: 0.7196\n",
      "Epoch 909/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.3102e-06 - acc: 1.0000 - val_loss: 3.3953 - val_acc: 0.7212\n",
      "Epoch 910/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.0658e-06 - acc: 1.0000 - val_loss: 3.4277 - val_acc: 0.7196\n",
      "Epoch 911/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9787e-07 - acc: 1.0000 - val_loss: 3.4280 - val_acc: 0.7196\n",
      "Epoch 912/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5516e-06 - acc: 1.0000 - val_loss: 3.4042 - val_acc: 0.7196\n",
      "Epoch 913/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.4851e-07 - acc: 1.0000 - val_loss: 3.4228 - val_acc: 0.7196\n",
      "Epoch 914/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2856e-06 - acc: 1.0000 - val_loss: 3.4035 - val_acc: 0.7196\n",
      "Epoch 915/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2304e-06 - acc: 1.0000 - val_loss: 3.3981 - val_acc: 0.7196\n",
      "Epoch 916/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.9385e-06 - acc: 1.0000 - val_loss: 3.4113 - val_acc: 0.7196\n",
      "Epoch 917/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3412e-06 - acc: 1.0000 - val_loss: 3.4222 - val_acc: 0.7196\n",
      "Epoch 918/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1750e-06 - acc: 1.0000 - val_loss: 3.4353 - val_acc: 0.7196\n",
      "Epoch 919/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0236e-07 - acc: 1.0000 - val_loss: 3.4247 - val_acc: 0.7196\n",
      "Epoch 920/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6467e-07 - acc: 1.0000 - val_loss: 3.4031 - val_acc: 0.7212\n",
      "Epoch 921/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0161 - acc: 0.9963 - val_loss: 4.3945 - val_acc: 0.6651\n",
      "Epoch 922/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0038e-04 - acc: 1.0000 - val_loss: 4.3418 - val_acc: 0.6715\n",
      "Epoch 923/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.7845e-05 - acc: 1.0000 - val_loss: 4.2392 - val_acc: 0.6763\n",
      "Epoch 924/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5126e-05 - acc: 1.0000 - val_loss: 4.2043 - val_acc: 0.6779\n",
      "Epoch 925/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3409e-05 - acc: 1.0000 - val_loss: 4.2229 - val_acc: 0.6779\n",
      "Epoch 926/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3350e-05 - acc: 1.0000 - val_loss: 4.2222 - val_acc: 0.6779\n",
      "Epoch 927/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.3425e-05 - acc: 1.0000 - val_loss: 4.2177 - val_acc: 0.6779\n",
      "Epoch 928/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6332e-04 - acc: 0.9996 - val_loss: 4.1604 - val_acc: 0.6779\n",
      "Epoch 929/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.0730e-04 - acc: 0.9996 - val_loss: 3.9419 - val_acc: 0.6875\n",
      "Epoch 930/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.6313e-06 - acc: 1.0000 - val_loss: 4.0329 - val_acc: 0.6843\n",
      "Epoch 931/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.5034e-06 - acc: 1.0000 - val_loss: 4.0666 - val_acc: 0.6827\n",
      "Epoch 932/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.3843e-06 - acc: 1.0000 - val_loss: 4.0518 - val_acc: 0.6827\n",
      "Epoch 933/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9575e-06 - acc: 1.0000 - val_loss: 4.0336 - val_acc: 0.6843\n",
      "Epoch 934/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.5599e-06 - acc: 1.0000 - val_loss: 4.0447 - val_acc: 0.6843\n",
      "Epoch 935/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.1546e-06 - acc: 1.0000 - val_loss: 4.0450 - val_acc: 0.6827\n",
      "Epoch 936/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0946e-06 - acc: 1.0000 - val_loss: 4.0482 - val_acc: 0.6843\n",
      "Epoch 937/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0523e-06 - acc: 1.0000 - val_loss: 4.0325 - val_acc: 0.6859\n",
      "Epoch 938/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.0488e-06 - acc: 1.0000 - val_loss: 4.0177 - val_acc: 0.6859\n",
      "Epoch 939/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6720e-04 - acc: 1.0000 - val_loss: 3.2423 - val_acc: 0.7308\n",
      "Epoch 940/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0126 - acc: 0.9978 - val_loss: 2.7936 - val_acc: 0.7308\n",
      "Epoch 941/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0104 - acc: 0.9970 - val_loss: 3.4538 - val_acc: 0.7051\n",
      "Epoch 942/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.0163e-05 - acc: 1.0000 - val_loss: 3.8279 - val_acc: 0.6779\n",
      "Epoch 943/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0390e-05 - acc: 1.0000 - val_loss: 3.8262 - val_acc: 0.6795\n",
      "Epoch 944/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6784e-05 - acc: 1.0000 - val_loss: 3.8469 - val_acc: 0.6779\n",
      "Epoch 945/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.0577e-05 - acc: 1.0000 - val_loss: 3.7241 - val_acc: 0.6875\n",
      "Epoch 946/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.9807e-06 - acc: 1.0000 - val_loss: 3.7614 - val_acc: 0.6859\n",
      "Epoch 947/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 4.1956 - val_acc: 0.6747\n",
      "Epoch 948/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6964e-04 - acc: 1.0000 - val_loss: 4.2434 - val_acc: 0.6651\n",
      "Epoch 949/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 5.1415 - val_acc: 0.6442\n",
      "Epoch 950/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 3.2833 - val_acc: 0.7244\n",
      "Epoch 951/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4480e-04 - acc: 0.9996 - val_loss: 4.2959 - val_acc: 0.6795\n",
      "Epoch 952/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1230e-04 - acc: 1.0000 - val_loss: 4.6475 - val_acc: 0.6635\n",
      "Epoch 953/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.7970e-04 - acc: 1.0000 - val_loss: 4.0465 - val_acc: 0.6923\n",
      "Epoch 954/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 4.9902 - val_acc: 0.6474\n",
      "Epoch 955/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0067 - acc: 0.9985 - val_loss: 4.5806 - val_acc: 0.6651\n",
      "Epoch 956/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.1100e-05 - acc: 1.0000 - val_loss: 4.4894 - val_acc: 0.6667\n",
      "Epoch 957/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.3764e-06 - acc: 1.0000 - val_loss: 4.4548 - val_acc: 0.6683\n",
      "Epoch 958/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.8181e-05 - acc: 1.0000 - val_loss: 4.3862 - val_acc: 0.6731\n",
      "Epoch 959/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9475e-05 - acc: 1.0000 - val_loss: 4.2145 - val_acc: 0.6763\n",
      "Epoch 960/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 8.2830e-06 - acc: 1.0000 - val_loss: 4.2966 - val_acc: 0.6747\n",
      "Epoch 961/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.7288e-05 - acc: 1.0000 - val_loss: 4.3895 - val_acc: 0.6715\n",
      "Epoch 962/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.8245e-06 - acc: 1.0000 - val_loss: 4.4117 - val_acc: 0.6683\n",
      "Epoch 963/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.6846e-05 - acc: 1.0000 - val_loss: 4.6736 - val_acc: 0.6635\n",
      "Epoch 964/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1424e-06 - acc: 1.0000 - val_loss: 4.6243 - val_acc: 0.6667\n",
      "Epoch 965/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4565e-06 - acc: 1.0000 - val_loss: 4.6495 - val_acc: 0.6667\n",
      "Epoch 966/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.8562e-05 - acc: 1.0000 - val_loss: 4.4721 - val_acc: 0.6699\n",
      "Epoch 967/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2208e-06 - acc: 1.0000 - val_loss: 4.5014 - val_acc: 0.6699\n",
      "Epoch 968/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.0921e-06 - acc: 1.0000 - val_loss: 4.5778 - val_acc: 0.6683\n",
      "Epoch 969/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5114e-06 - acc: 1.0000 - val_loss: 4.5956 - val_acc: 0.6667\n",
      "Epoch 970/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.4460e-05 - acc: 1.0000 - val_loss: 4.6926 - val_acc: 0.6635\n",
      "Epoch 971/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.8283e-07 - acc: 1.0000 - val_loss: 4.6868 - val_acc: 0.6635\n",
      "Epoch 972/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.1281e-06 - acc: 1.0000 - val_loss: 4.6708 - val_acc: 0.6635\n",
      "Epoch 973/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.9065e-06 - acc: 1.0000 - val_loss: 4.6468 - val_acc: 0.6635\n",
      "Epoch 974/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 3.9852e-06 - acc: 1.0000 - val_loss: 4.5830 - val_acc: 0.6683\n",
      "Epoch 975/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4592e-07 - acc: 1.0000 - val_loss: 4.5818 - val_acc: 0.6683\n",
      "Epoch 976/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.6764e-07 - acc: 1.0000 - val_loss: 4.5891 - val_acc: 0.6667\n",
      "Epoch 977/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.5458e-07 - acc: 1.0000 - val_loss: 4.5757 - val_acc: 0.6683\n",
      "Epoch 978/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8024e-07 - acc: 1.0000 - val_loss: 4.5917 - val_acc: 0.6683\n",
      "Epoch 979/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.3210e-07 - acc: 1.0000 - val_loss: 4.5697 - val_acc: 0.6683\n",
      "Epoch 980/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 9.6263e-07 - acc: 1.0000 - val_loss: 4.5752 - val_acc: 0.6683\n",
      "Epoch 981/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0058 - acc: 0.9989 - val_loss: 2.1666 - val_acc: 0.7452\n",
      "Epoch 982/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0177 - acc: 0.9959 - val_loss: 2.6549 - val_acc: 0.7420\n",
      "Epoch 983/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 4.0455 - val_acc: 0.6843\n",
      "Epoch 984/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.6740e-04 - acc: 1.0000 - val_loss: 3.8741 - val_acc: 0.6875\n",
      "Epoch 985/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.9930e-05 - acc: 1.0000 - val_loss: 3.8599 - val_acc: 0.6875\n",
      "Epoch 986/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 7.0778e-06 - acc: 1.0000 - val_loss: 3.8955 - val_acc: 0.6875\n",
      "Epoch 987/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.3552e-05 - acc: 1.0000 - val_loss: 3.9286 - val_acc: 0.6875\n",
      "Epoch 988/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.9305e-06 - acc: 1.0000 - val_loss: 3.9693 - val_acc: 0.6859\n",
      "Epoch 989/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.2483e-05 - acc: 1.0000 - val_loss: 4.0237 - val_acc: 0.6843\n",
      "Epoch 990/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 4.4505e-06 - acc: 1.0000 - val_loss: 4.0167 - val_acc: 0.6843\n",
      "Epoch 991/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 6.6005e-06 - acc: 1.0000 - val_loss: 4.0281 - val_acc: 0.6843\n",
      "Epoch 992/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.8453e-06 - acc: 1.0000 - val_loss: 4.0227 - val_acc: 0.6843\n",
      "Epoch 993/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1905e-06 - acc: 1.0000 - val_loss: 4.0197 - val_acc: 0.6843\n",
      "Epoch 994/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 5.0754e-06 - acc: 1.0000 - val_loss: 4.0003 - val_acc: 0.6843\n",
      "Epoch 995/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.4585e-06 - acc: 1.0000 - val_loss: 4.0036 - val_acc: 0.6843\n",
      "Epoch 996/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.2112e-06 - acc: 1.0000 - val_loss: 4.0019 - val_acc: 0.6843\n",
      "Epoch 997/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.5537e-06 - acc: 1.0000 - val_loss: 4.0028 - val_acc: 0.6843\n",
      "Epoch 998/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 2.1202e-06 - acc: 1.0000 - val_loss: 3.9884 - val_acc: 0.6843\n",
      "Epoch 999/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.7849e-06 - acc: 1.0000 - val_loss: 3.9896 - val_acc: 0.6843\n",
      "Epoch 1000/1000\n",
      "2682/2682 [==============================] - 46s 17ms/step - loss: 1.4190e-06 - acc: 1.0000 - val_loss: 4.0127 - val_acc: 0.6843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x20ed87900f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resmodel2.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=16,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.16      0.27       234\n",
      "          1       0.66      1.00      0.80       390\n",
      "\n",
      "avg / total       0.79      0.68      0.60       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e5caa6e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res2_pred_base = resmodel2.predict(X_test)\n",
    "res2_pred = [label_decoder(i) for i in res2_pred_base]\n",
    "res2_cm = confusion_matrix(y_test, res2_pred)\n",
    "sns.heatmap(res2_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, res2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardising images using StandardScaler in makes the model perform worse than un-standardised, surprisingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggmodel = keras.applications.VGG16(weights = None, classes =2, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,268,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggmodel.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/10\n",
      "2682/2682 [==============================] - 433s 162ms/step - loss: 8.0154 - acc: 0.5000 - val_loss: 10.0189 - val_acc: 0.3750\n",
      "Epoch 2/10\n",
      "2682/2682 [==============================] - 324s 121ms/step - loss: 8.0151 - acc: 0.5000 - val_loss: 10.0189 - val_acc: 0.3750\n",
      "Epoch 3/10\n",
      " 239/2682 [=>............................] - ETA: 4:46 - loss: 8.3169 - acc: 0.4812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-056eeb36a018>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           callbacks=[history])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1214\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    243\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[0mfetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m     updated = session.run(\n\u001b[1;32m-> 2824\u001b[1;33m         fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2825\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vggmodel.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=1,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "incmodel = keras.applications.InceptionV3(weights=None, classes=2, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 111, 111, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 111, 111, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 111, 111, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 109, 109, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 109, 109, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 109, 109, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 109, 109, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 54, 54, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 54, 54, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 54, 54, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 52, 52, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 52, 52, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 52, 52, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 25, 25, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 25, 25, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 25, 25, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 25, 25, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 25, 25, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 25, 25, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 12, 12, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 12, 12, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 12, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 12, 12, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 12, 12, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 12, 12, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 12, 12, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 12, 12, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 12, 12, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 5, 5, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 5, 5, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 5, 5, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 5, 5, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 5, 5, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 5, 5, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 5, 5, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 5, 5, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 2)            4098        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,806,882\n",
      "Trainable params: 21,772,450\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "incmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "incmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0682 - acc: 0.9720 - val_loss: 0.7960 - val_acc: 0.7901\n",
      "Epoch 2/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0550 - acc: 0.9799 - val_loss: 1.9990 - val_acc: 0.6859\n",
      "Epoch 3/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0691 - acc: 0.9758 - val_loss: 0.8199 - val_acc: 0.7324\n",
      "Epoch 4/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0433 - acc: 0.9828 - val_loss: 2.2192 - val_acc: 0.6587\n",
      "Epoch 5/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.7864 - val_acc: 0.8253\n",
      "Epoch 6/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0206 - acc: 0.9922 - val_loss: 1.1723 - val_acc: 0.7965\n",
      "Epoch 7/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0286 - acc: 0.9922 - val_loss: 1.5566 - val_acc: 0.7484\n",
      "Epoch 8/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0673 - acc: 0.9754 - val_loss: 0.9624 - val_acc: 0.7981\n",
      "Epoch 9/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0332 - acc: 0.9873 - val_loss: 0.8289 - val_acc: 0.8061\n",
      "Epoch 10/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0423 - acc: 0.9858 - val_loss: 0.9208 - val_acc: 0.8349\n",
      "Epoch 11/1000\n",
      "2682/2682 [==============================] - 37s 14ms/step - loss: 0.0458 - acc: 0.9873 - val_loss: 2.2236 - val_acc: 0.6843\n",
      "Epoch 12/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0298 - acc: 0.9899 - val_loss: 1.0889 - val_acc: 0.7676\n",
      "Epoch 13/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0381 - acc: 0.9851 - val_loss: 0.5472 - val_acc: 0.8654\n",
      "Epoch 14/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0221 - acc: 0.9929 - val_loss: 1.5443 - val_acc: 0.7804\n",
      "Epoch 15/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0226 - acc: 0.9914 - val_loss: 1.1889 - val_acc: 0.7917\n",
      "Epoch 16/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0300 - acc: 0.9911 - val_loss: 0.6109 - val_acc: 0.8686\n",
      "Epoch 17/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0182 - acc: 0.9922 - val_loss: 2.0132 - val_acc: 0.7228\n",
      "Epoch 18/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0135 - acc: 0.9940 - val_loss: 1.9925 - val_acc: 0.7308\n",
      "Epoch 19/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0242 - acc: 0.9929 - val_loss: 2.3072 - val_acc: 0.6763\n",
      "Epoch 20/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0599 - acc: 0.9795 - val_loss: 0.4551 - val_acc: 0.8446\n",
      "Epoch 21/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0453 - acc: 0.9836 - val_loss: 2.3026 - val_acc: 0.7147\n",
      "Epoch 22/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0281 - acc: 0.9896 - val_loss: 2.1049 - val_acc: 0.7099\n",
      "Epoch 23/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.9699 - val_acc: 0.8333\n",
      "Epoch 24/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0199 - acc: 0.9944 - val_loss: 1.0065 - val_acc: 0.7981\n",
      "Epoch 25/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0090 - acc: 0.9963 - val_loss: 1.7076 - val_acc: 0.7612\n",
      "Epoch 26/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0104 - acc: 0.9959 - val_loss: 1.2326 - val_acc: 0.7885\n",
      "Epoch 27/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 1.0167 - val_acc: 0.8141\n",
      "Epoch 28/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0527 - acc: 0.9791 - val_loss: 1.8788 - val_acc: 0.7163\n",
      "Epoch 29/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0153 - acc: 0.9948 - val_loss: 1.6054 - val_acc: 0.7260\n",
      "Epoch 30/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0122 - acc: 0.9959 - val_loss: 1.9898 - val_acc: 0.7179\n",
      "Epoch 31/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0162 - acc: 0.9955 - val_loss: 2.6056 - val_acc: 0.7099\n",
      "Epoch 32/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.8388 - val_acc: 0.8590\n",
      "Epoch 33/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.8568 - val_acc: 0.8045\n",
      "Epoch 34/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0184 - acc: 0.9944 - val_loss: 1.0225 - val_acc: 0.8301\n",
      "Epoch 35/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0140 - acc: 0.9955 - val_loss: 1.2749 - val_acc: 0.7885\n",
      "Epoch 36/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0141 - acc: 0.9955 - val_loss: 2.6811 - val_acc: 0.6619\n",
      "Epoch 37/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0114 - acc: 0.9955 - val_loss: 1.3363 - val_acc: 0.7837\n",
      "Epoch 38/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0121 - acc: 0.9952 - val_loss: 1.3142 - val_acc: 0.6699\n",
      "Epoch 39/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0144 - acc: 0.9952 - val_loss: 1.9394 - val_acc: 0.7676\n",
      "Epoch 40/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0094 - acc: 0.9959 - val_loss: 0.7496 - val_acc: 0.8397\n",
      "Epoch 41/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0052 - acc: 0.9974 - val_loss: 1.5962 - val_acc: 0.7901\n",
      "Epoch 42/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0146 - acc: 0.9955 - val_loss: 1.4475 - val_acc: 0.7885\n",
      "Epoch 43/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0262 - acc: 0.9911 - val_loss: 1.4360 - val_acc: 0.7821\n",
      "Epoch 44/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 3.1063 - val_acc: 0.6651\n",
      "Epoch 45/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0231 - acc: 0.9918 - val_loss: 1.8475 - val_acc: 0.7628\n",
      "Epoch 46/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0177 - acc: 0.9955 - val_loss: 1.7392 - val_acc: 0.7340\n",
      "Epoch 47/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.5469 - val_acc: 0.8494\n",
      "Epoch 48/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0131 - acc: 0.9963 - val_loss: 1.3298 - val_acc: 0.7788\n",
      "Epoch 49/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8452e-04 - acc: 1.0000 - val_loss: 1.5007 - val_acc: 0.7708\n",
      "Epoch 50/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7326 - val_acc: 0.6763\n",
      "Epoch 51/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0322 - acc: 0.9881 - val_loss: 1.7242 - val_acc: 0.7644\n",
      "Epoch 52/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0158 - acc: 0.9933 - val_loss: 1.0935 - val_acc: 0.8269\n",
      "Epoch 53/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.7699e-04 - acc: 1.0000 - val_loss: 1.1628 - val_acc: 0.8221\n",
      "Epoch 54/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1279e-04 - acc: 1.0000 - val_loss: 1.3870 - val_acc: 0.8093\n",
      "Epoch 55/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9858e-04 - acc: 1.0000 - val_loss: 1.2450 - val_acc: 0.8189\n",
      "Epoch 56/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.6167e-05 - acc: 1.0000 - val_loss: 1.2679 - val_acc: 0.8157\n",
      "Epoch 57/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3094e-04 - acc: 1.0000 - val_loss: 1.3702 - val_acc: 0.8189\n",
      "Epoch 58/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.6010e-05 - acc: 1.0000 - val_loss: 1.3465 - val_acc: 0.8221\n",
      "Epoch 59/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.9010e-05 - acc: 1.0000 - val_loss: 1.2845 - val_acc: 0.8221\n",
      "Epoch 60/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.1541e-05 - acc: 1.0000 - val_loss: 1.2914 - val_acc: 0.8237\n",
      "Epoch 61/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3080e-05 - acc: 1.0000 - val_loss: 1.3876 - val_acc: 0.8141\n",
      "Epoch 62/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0731e-05 - acc: 1.0000 - val_loss: 1.4079 - val_acc: 0.8141\n",
      "Epoch 63/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4153e-05 - acc: 1.0000 - val_loss: 1.5116 - val_acc: 0.8157\n",
      "Epoch 64/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7533e-05 - acc: 1.0000 - val_loss: 1.4563 - val_acc: 0.8173\n",
      "Epoch 65/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2789e-05 - acc: 1.0000 - val_loss: 1.4739 - val_acc: 0.8173\n",
      "Epoch 66/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6812e-05 - acc: 1.0000 - val_loss: 1.5171 - val_acc: 0.8141\n",
      "Epoch 67/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3898e-05 - acc: 1.0000 - val_loss: 1.4958 - val_acc: 0.8141\n",
      "Epoch 68/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8875e-05 - acc: 1.0000 - val_loss: 1.4617 - val_acc: 0.8189\n",
      "Epoch 69/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1109e-05 - acc: 1.0000 - val_loss: 1.4830 - val_acc: 0.8189\n",
      "Epoch 70/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4129e-05 - acc: 1.0000 - val_loss: 1.3947 - val_acc: 0.8269\n",
      "Epoch 71/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7662e-05 - acc: 1.0000 - val_loss: 1.5068 - val_acc: 0.8205\n",
      "Epoch 72/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4071e-05 - acc: 1.0000 - val_loss: 1.6356 - val_acc: 0.8093\n",
      "Epoch 73/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8574e-06 - acc: 1.0000 - val_loss: 1.6263 - val_acc: 0.8077\n",
      "Epoch 74/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7682e-06 - acc: 1.0000 - val_loss: 1.5938 - val_acc: 0.8125\n",
      "Epoch 75/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9268e-06 - acc: 1.0000 - val_loss: 1.6081 - val_acc: 0.8125\n",
      "Epoch 76/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0759e-06 - acc: 1.0000 - val_loss: 1.6514 - val_acc: 0.8109\n",
      "Epoch 77/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2182e-06 - acc: 1.0000 - val_loss: 1.5937 - val_acc: 0.8141\n",
      "Epoch 78/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4345e-06 - acc: 1.0000 - val_loss: 1.6078 - val_acc: 0.8141\n",
      "Epoch 79/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2984e-06 - acc: 1.0000 - val_loss: 1.6113 - val_acc: 0.8141\n",
      "Epoch 80/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5883e-06 - acc: 1.0000 - val_loss: 1.6281 - val_acc: 0.8141\n",
      "Epoch 81/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4678e-06 - acc: 1.0000 - val_loss: 1.6481 - val_acc: 0.8125\n",
      "Epoch 82/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.6660e-06 - acc: 1.0000 - val_loss: 1.7165 - val_acc: 0.8045\n",
      "Epoch 83/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0589e-06 - acc: 1.0000 - val_loss: 1.6564 - val_acc: 0.8109\n",
      "Epoch 84/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8828e-06 - acc: 1.0000 - val_loss: 1.6708 - val_acc: 0.8189\n",
      "Epoch 85/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4213e-06 - acc: 1.0000 - val_loss: 1.6756 - val_acc: 0.8125\n",
      "Epoch 86/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3019e-06 - acc: 1.0000 - val_loss: 1.6555 - val_acc: 0.8173\n",
      "Epoch 87/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4731e-06 - acc: 1.0000 - val_loss: 1.6895 - val_acc: 0.8093\n",
      "Epoch 88/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8405e-06 - acc: 1.0000 - val_loss: 1.7226 - val_acc: 0.8061\n",
      "Epoch 89/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1044e-06 - acc: 1.0000 - val_loss: 1.7556 - val_acc: 0.8061\n",
      "Epoch 90/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4791e-06 - acc: 1.0000 - val_loss: 1.7657 - val_acc: 0.8077\n",
      "Epoch 91/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3884e-06 - acc: 1.0000 - val_loss: 1.7515 - val_acc: 0.8141\n",
      "Epoch 92/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9087e-06 - acc: 1.0000 - val_loss: 1.8113 - val_acc: 0.8061\n",
      "Epoch 93/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.1202 - acc: 0.9605 - val_loss: 0.9404 - val_acc: 0.8157\n",
      "Epoch 94/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0537 - acc: 0.9802 - val_loss: 0.8939 - val_acc: 0.8141\n",
      "Epoch 95/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0243 - acc: 0.9922 - val_loss: 1.4284 - val_acc: 0.7340\n",
      "Epoch 96/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0151 - acc: 0.9944 - val_loss: 0.9127 - val_acc: 0.8157\n",
      "Epoch 97/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0131 - acc: 0.9952 - val_loss: 0.9815 - val_acc: 0.7917\n",
      "Epoch 98/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0084 - acc: 0.9966 - val_loss: 0.6534 - val_acc: 0.8429\n",
      "Epoch 99/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0048 - acc: 0.9974 - val_loss: 0.8309 - val_acc: 0.8013\n",
      "Epoch 100/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 1.2072 - val_acc: 0.7804\n",
      "Epoch 101/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0022 - acc: 0.9989 - val_loss: 1.7344 - val_acc: 0.7212\n",
      "Epoch 102/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0138 - acc: 0.9933 - val_loss: 1.3721 - val_acc: 0.7740\n",
      "Epoch 103/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0273 - acc: 0.9884 - val_loss: 0.6656 - val_acc: 0.8686\n",
      "Epoch 104/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 1.2524 - val_acc: 0.7949\n",
      "Epoch 105/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.1801e-04 - acc: 1.0000 - val_loss: 1.3810 - val_acc: 0.7949\n",
      "Epoch 106/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0153e-04 - acc: 1.0000 - val_loss: 1.4176 - val_acc: 0.7981\n",
      "Epoch 107/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5745e-04 - acc: 1.0000 - val_loss: 1.5241 - val_acc: 0.7917\n",
      "Epoch 108/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1605e-04 - acc: 1.0000 - val_loss: 1.4846 - val_acc: 0.8077\n",
      "Epoch 109/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0410e-05 - acc: 1.0000 - val_loss: 1.4833 - val_acc: 0.8141\n",
      "Epoch 110/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0262e-05 - acc: 1.0000 - val_loss: 1.4568 - val_acc: 0.8205\n",
      "Epoch 111/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0542 - acc: 0.9825 - val_loss: 0.6643 - val_acc: 0.7356\n",
      "Epoch 112/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0269 - acc: 0.9918 - val_loss: 1.0655 - val_acc: 0.7436\n",
      "Epoch 113/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0045 - acc: 0.9993 - val_loss: 1.1201 - val_acc: 0.7837\n",
      "Epoch 114/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 1.1201 - val_acc: 0.7997\n",
      "Epoch 115/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 1.9325 - val_acc: 0.7452\n",
      "Epoch 116/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2461e-04 - acc: 1.0000 - val_loss: 1.4892 - val_acc: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.6132 - val_acc: 0.8734\n",
      "Epoch 118/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0184 - acc: 0.9922 - val_loss: 2.2716 - val_acc: 0.7260\n",
      "Epoch 119/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 1.4222 - val_acc: 0.8029\n",
      "Epoch 120/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 1.6177 - val_acc: 0.7804\n",
      "Epoch 121/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6668e-04 - acc: 1.0000 - val_loss: 2.0544 - val_acc: 0.7404\n",
      "Epoch 122/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2327e-04 - acc: 1.0000 - val_loss: 1.5917 - val_acc: 0.7917\n",
      "Epoch 123/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8529e-05 - acc: 1.0000 - val_loss: 1.5158 - val_acc: 0.7997\n",
      "Epoch 124/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.4859e-04 - acc: 0.9993 - val_loss: 1.8313 - val_acc: 0.7997\n",
      "Epoch 125/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 2.4588 - val_acc: 0.7340\n",
      "Epoch 126/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0358 - acc: 0.9903 - val_loss: 1.4792 - val_acc: 0.7660\n",
      "Epoch 127/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.3032 - val_acc: 0.8173\n",
      "Epoch 128/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.2154e-04 - acc: 1.0000 - val_loss: 1.0921 - val_acc: 0.8285\n",
      "Epoch 129/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1757e-04 - acc: 1.0000 - val_loss: 1.2886 - val_acc: 0.8205\n",
      "Epoch 130/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6689e-04 - acc: 1.0000 - val_loss: 1.3355 - val_acc: 0.8205\n",
      "Epoch 131/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0086 - acc: 0.9966 - val_loss: 0.9011 - val_acc: 0.8526\n",
      "Epoch 132/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0196 - acc: 0.9940 - val_loss: 2.2389 - val_acc: 0.7292\n",
      "Epoch 133/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 1.0793 - val_acc: 0.8237\n",
      "Epoch 134/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2943e-04 - acc: 1.0000 - val_loss: 1.0156 - val_acc: 0.8333\n",
      "Epoch 135/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6454e-04 - acc: 1.0000 - val_loss: 1.2401 - val_acc: 0.8189\n",
      "Epoch 136/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8522e-04 - acc: 1.0000 - val_loss: 1.0968 - val_acc: 0.8381\n",
      "Epoch 137/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.4426e-05 - acc: 1.0000 - val_loss: 1.2419 - val_acc: 0.8221\n",
      "Epoch 138/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0299e-04 - acc: 1.0000 - val_loss: 1.3782 - val_acc: 0.8141\n",
      "Epoch 139/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1497e-05 - acc: 1.0000 - val_loss: 1.3717 - val_acc: 0.8221\n",
      "Epoch 140/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5121e-05 - acc: 1.0000 - val_loss: 1.4292 - val_acc: 0.8205\n",
      "Epoch 141/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6429e-05 - acc: 1.0000 - val_loss: 1.4750 - val_acc: 0.8221\n",
      "Epoch 142/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8819e-05 - acc: 1.0000 - val_loss: 1.4762 - val_acc: 0.8205\n",
      "Epoch 143/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3531e-04 - acc: 1.0000 - val_loss: 1.4056 - val_acc: 0.8253\n",
      "Epoch 144/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8791e-04 - acc: 1.0000 - val_loss: 1.4400 - val_acc: 0.8397\n",
      "Epoch 145/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9944e-05 - acc: 1.0000 - val_loss: 1.3829 - val_acc: 0.8365\n",
      "Epoch 146/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7612e-06 - acc: 1.0000 - val_loss: 1.3943 - val_acc: 0.8397\n",
      "Epoch 147/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0330 - acc: 0.9911 - val_loss: 1.3780 - val_acc: 0.6651\n",
      "Epoch 148/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0456 - acc: 0.9862 - val_loss: 2.1804 - val_acc: 0.6923\n",
      "Epoch 149/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0067 - acc: 0.9974 - val_loss: 2.1357 - val_acc: 0.6843\n",
      "Epoch 150/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0053 - acc: 0.9989 - val_loss: 1.0149 - val_acc: 0.7917\n",
      "Epoch 151/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 1.3023 - val_acc: 0.7965\n",
      "Epoch 152/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.6080e-04 - acc: 0.9996 - val_loss: 1.1766 - val_acc: 0.8093\n",
      "Epoch 153/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4139e-04 - acc: 1.0000 - val_loss: 1.2962 - val_acc: 0.8077\n",
      "Epoch 154/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0766e-04 - acc: 1.0000 - val_loss: 1.3254 - val_acc: 0.8045\n",
      "Epoch 155/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4598e-04 - acc: 1.0000 - val_loss: 1.3785 - val_acc: 0.8045\n",
      "Epoch 156/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3759e-05 - acc: 1.0000 - val_loss: 1.3648 - val_acc: 0.8093\n",
      "Epoch 157/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0271e-05 - acc: 1.0000 - val_loss: 1.3927 - val_acc: 0.8109\n",
      "Epoch 158/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3722e-05 - acc: 1.0000 - val_loss: 1.4387 - val_acc: 0.8045\n",
      "Epoch 159/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8159e-05 - acc: 1.0000 - val_loss: 1.4801 - val_acc: 0.8045\n",
      "Epoch 160/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.9057e-05 - acc: 1.0000 - val_loss: 1.4496 - val_acc: 0.8077\n",
      "Epoch 161/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6678e-05 - acc: 1.0000 - val_loss: 1.5007 - val_acc: 0.8029\n",
      "Epoch 162/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5910e-05 - acc: 1.0000 - val_loss: 1.4979 - val_acc: 0.8061\n",
      "Epoch 163/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0855e-04 - acc: 1.0000 - val_loss: 1.3832 - val_acc: 0.7997\n",
      "Epoch 164/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8259e-04 - acc: 1.0000 - val_loss: 1.9056 - val_acc: 0.7708\n",
      "Epoch 165/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6745e-05 - acc: 1.0000 - val_loss: 1.5974 - val_acc: 0.8045\n",
      "Epoch 166/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1127e-05 - acc: 1.0000 - val_loss: 1.6094 - val_acc: 0.8061\n",
      "Epoch 167/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.1154e-05 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.8173\n",
      "Epoch 168/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8565e-05 - acc: 1.0000 - val_loss: 1.5456 - val_acc: 0.8189\n",
      "Epoch 169/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4018e-05 - acc: 1.0000 - val_loss: 1.5079 - val_acc: 0.8237\n",
      "Epoch 170/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4042e-05 - acc: 1.0000 - val_loss: 1.5995 - val_acc: 0.8221\n",
      "Epoch 171/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0567 - acc: 0.9828 - val_loss: 1.0292 - val_acc: 0.8173\n",
      "Epoch 172/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0122 - acc: 0.9963 - val_loss: 1.3595 - val_acc: 0.8013\n",
      "Epoch 173/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 1.2971 - val_acc: 0.7917\n",
      "Epoch 174/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2417e-04 - acc: 1.0000 - val_loss: 1.2044 - val_acc: 0.8061\n",
      "Epoch 175/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1032e-04 - acc: 1.0000 - val_loss: 1.0073 - val_acc: 0.8253\n",
      "Epoch 176/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5674e-04 - acc: 1.0000 - val_loss: 1.3478 - val_acc: 0.8189\n",
      "Epoch 177/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.5425 - val_acc: 0.7772\n",
      "Epoch 178/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9754e-04 - acc: 1.0000 - val_loss: 1.5371 - val_acc: 0.7981\n",
      "Epoch 179/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0461e-04 - acc: 1.0000 - val_loss: 1.4612 - val_acc: 0.8061\n",
      "Epoch 180/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0285 - acc: 0.9907 - val_loss: 1.3195 - val_acc: 0.7901\n",
      "Epoch 181/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0085 - acc: 0.9970 - val_loss: 1.2139 - val_acc: 0.7981\n",
      "Epoch 182/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.9956 - val_acc: 0.8125\n",
      "Epoch 183/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2927e-04 - acc: 1.0000 - val_loss: 1.1127 - val_acc: 0.8157\n",
      "Epoch 184/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7677e-04 - acc: 1.0000 - val_loss: 1.8273 - val_acc: 0.7532\n",
      "Epoch 185/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5287e-04 - acc: 1.0000 - val_loss: 1.3988 - val_acc: 0.8093\n",
      "Epoch 186/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.4441e-05 - acc: 1.0000 - val_loss: 1.4213 - val_acc: 0.8109\n",
      "Epoch 187/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.8506e-05 - acc: 1.0000 - val_loss: 1.4167 - val_acc: 0.8125\n",
      "Epoch 188/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3584e-05 - acc: 1.0000 - val_loss: 1.4531 - val_acc: 0.8141\n",
      "Epoch 189/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9632e-05 - acc: 1.0000 - val_loss: 1.4825 - val_acc: 0.8141\n",
      "Epoch 190/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.8301e-05 - acc: 1.0000 - val_loss: 1.5403 - val_acc: 0.8077\n",
      "Epoch 191/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9641e-05 - acc: 1.0000 - val_loss: 1.5184 - val_acc: 0.8093\n",
      "Epoch 192/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4122e-05 - acc: 1.0000 - val_loss: 1.5203 - val_acc: 0.8109\n",
      "Epoch 193/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.9541e-05 - acc: 1.0000 - val_loss: 1.7185 - val_acc: 0.7949\n",
      "Epoch 194/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0216 - acc: 0.9925 - val_loss: 0.7832 - val_acc: 0.8429\n",
      "Epoch 195/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0107 - acc: 0.9974 - val_loss: 1.9918 - val_acc: 0.7324\n",
      "Epoch 196/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 1.5613 - val_acc: 0.7692\n",
      "Epoch 197/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2974e-04 - acc: 1.0000 - val_loss: 1.1868 - val_acc: 0.8205\n",
      "Epoch 198/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5710e-04 - acc: 1.0000 - val_loss: 1.3214 - val_acc: 0.7981\n",
      "Epoch 199/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0626e-04 - acc: 1.0000 - val_loss: 1.4797 - val_acc: 0.7885\n",
      "Epoch 200/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.6974e-05 - acc: 1.0000 - val_loss: 1.4581 - val_acc: 0.7965\n",
      "Epoch 201/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4531e-05 - acc: 1.0000 - val_loss: 1.4342 - val_acc: 0.8061\n",
      "Epoch 202/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7987e-05 - acc: 1.0000 - val_loss: 1.4339 - val_acc: 0.8093\n",
      "Epoch 203/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5515e-04 - acc: 1.0000 - val_loss: 1.4697 - val_acc: 0.8109\n",
      "Epoch 204/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0174 - acc: 0.9952 - val_loss: 2.1051 - val_acc: 0.7500\n",
      "Epoch 205/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0115 - acc: 0.9955 - val_loss: 0.5903 - val_acc: 0.8750\n",
      "Epoch 206/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0015 - acc: 0.9993 - val_loss: 0.8955 - val_acc: 0.8494\n",
      "Epoch 207/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0806e-04 - acc: 1.0000 - val_loss: 1.3635 - val_acc: 0.8141\n",
      "Epoch 208/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.9125e-05 - acc: 1.0000 - val_loss: 1.4071 - val_acc: 0.8157\n",
      "Epoch 209/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0797e-05 - acc: 1.0000 - val_loss: 1.4341 - val_acc: 0.8189\n",
      "Epoch 210/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0185e-04 - acc: 1.0000 - val_loss: 1.3915 - val_acc: 0.8301\n",
      "Epoch 211/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8878e-05 - acc: 1.0000 - val_loss: 1.4706 - val_acc: 0.8173\n",
      "Epoch 212/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.0263e-05 - acc: 1.0000 - val_loss: 1.4964 - val_acc: 0.8205\n",
      "Epoch 213/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5594e-05 - acc: 1.0000 - val_loss: 1.5077 - val_acc: 0.8189\n",
      "Epoch 214/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4238e-05 - acc: 1.0000 - val_loss: 1.4952 - val_acc: 0.8253\n",
      "Epoch 215/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4990e-05 - acc: 1.0000 - val_loss: 1.5263 - val_acc: 0.8221\n",
      "Epoch 216/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7490e-05 - acc: 1.0000 - val_loss: 1.5441 - val_acc: 0.8221\n",
      "Epoch 217/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7106e-05 - acc: 1.0000 - val_loss: 1.4884 - val_acc: 0.8253\n",
      "Epoch 218/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7216e-05 - acc: 1.0000 - val_loss: 1.5478 - val_acc: 0.8173\n",
      "Epoch 219/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0249 - acc: 0.9940 - val_loss: 0.8173 - val_acc: 0.8013\n",
      "Epoch 220/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0178 - acc: 0.9940 - val_loss: 1.4195 - val_acc: 0.7949\n",
      "Epoch 221/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2140e-04 - acc: 1.0000 - val_loss: 1.4104 - val_acc: 0.7885\n",
      "Epoch 222/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.5757e-04 - acc: 1.0000 - val_loss: 1.3798 - val_acc: 0.8061\n",
      "Epoch 223/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6474e-04 - acc: 1.0000 - val_loss: 1.5429 - val_acc: 0.7949\n",
      "Epoch 224/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0654e-04 - acc: 1.0000 - val_loss: 1.4922 - val_acc: 0.8061\n",
      "Epoch 225/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.5588e-05 - acc: 1.0000 - val_loss: 1.4731 - val_acc: 0.8157\n",
      "Epoch 226/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.2141e-05 - acc: 1.0000 - val_loss: 1.4819 - val_acc: 0.8141\n",
      "Epoch 227/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0418e-05 - acc: 1.0000 - val_loss: 1.4821 - val_acc: 0.8189\n",
      "Epoch 228/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3503e-05 - acc: 1.0000 - val_loss: 1.5203 - val_acc: 0.8157\n",
      "Epoch 229/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6079e-05 - acc: 1.0000 - val_loss: 1.5428 - val_acc: 0.8157\n",
      "Epoch 230/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7403e-04 - acc: 1.0000 - val_loss: 1.9912 - val_acc: 0.7580\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.0391e-05 - acc: 1.0000 - val_loss: 1.6365 - val_acc: 0.8045\n",
      "Epoch 232/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8496e-04 - acc: 1.0000 - val_loss: 1.7409 - val_acc: 0.8077\n",
      "Epoch 233/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0138 - acc: 0.9952 - val_loss: 1.1822 - val_acc: 0.8077\n",
      "Epoch 234/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 1.9015 - val_acc: 0.7821\n",
      "Epoch 235/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0063 - acc: 0.9978 - val_loss: 1.6958 - val_acc: 0.7933\n",
      "Epoch 236/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.9005e-04 - acc: 0.9996 - val_loss: 1.3390 - val_acc: 0.8221\n",
      "Epoch 237/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0940e-04 - acc: 1.0000 - val_loss: 1.5129 - val_acc: 0.8109\n",
      "Epoch 238/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4866e-04 - acc: 1.0000 - val_loss: 1.4767 - val_acc: 0.8141\n",
      "Epoch 239/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.9092e-05 - acc: 1.0000 - val_loss: 1.5622 - val_acc: 0.8077\n",
      "Epoch 240/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3341e-05 - acc: 1.0000 - val_loss: 1.6448 - val_acc: 0.8029\n",
      "Epoch 241/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.7555e-05 - acc: 1.0000 - val_loss: 1.6661 - val_acc: 0.7997\n",
      "Epoch 242/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6871e-05 - acc: 1.0000 - val_loss: 1.6769 - val_acc: 0.8013\n",
      "Epoch 243/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2059e-05 - acc: 1.0000 - val_loss: 1.6918 - val_acc: 0.7997\n",
      "Epoch 244/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4367e-05 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 0.7981\n",
      "Epoch 245/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8377e-05 - acc: 1.0000 - val_loss: 1.6927 - val_acc: 0.8045\n",
      "Epoch 246/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6725e-05 - acc: 1.0000 - val_loss: 1.7196 - val_acc: 0.8013\n",
      "Epoch 247/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9298e-05 - acc: 1.0000 - val_loss: 1.7388 - val_acc: 0.7997\n",
      "Epoch 248/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7742e-05 - acc: 1.0000 - val_loss: 1.7550 - val_acc: 0.8013\n",
      "Epoch 249/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4065e-05 - acc: 1.0000 - val_loss: 1.7589 - val_acc: 0.8045\n",
      "Epoch 250/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4903e-05 - acc: 1.0000 - val_loss: 1.7592 - val_acc: 0.8045\n",
      "Epoch 251/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0767e-05 - acc: 1.0000 - val_loss: 1.7495 - val_acc: 0.8029\n",
      "Epoch 252/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2199e-06 - acc: 1.0000 - val_loss: 1.7664 - val_acc: 0.8013\n",
      "Epoch 253/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0255 - acc: 0.9933 - val_loss: 0.8990 - val_acc: 0.7853\n",
      "Epoch 254/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0164 - acc: 0.9959 - val_loss: 1.6012 - val_acc: 0.7933\n",
      "Epoch 255/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7863e-04 - acc: 1.0000 - val_loss: 1.2232 - val_acc: 0.8205\n",
      "Epoch 256/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6619e-04 - acc: 1.0000 - val_loss: 1.1943 - val_acc: 0.8301\n",
      "Epoch 257/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9124e-04 - acc: 1.0000 - val_loss: 1.3571 - val_acc: 0.8253\n",
      "Epoch 258/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0444e-04 - acc: 1.0000 - val_loss: 1.4101 - val_acc: 0.8237\n",
      "Epoch 259/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8483e-05 - acc: 1.0000 - val_loss: 1.4194 - val_acc: 0.8253\n",
      "Epoch 260/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3752e-05 - acc: 1.0000 - val_loss: 1.4480 - val_acc: 0.8253\n",
      "Epoch 261/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0025e-05 - acc: 1.0000 - val_loss: 1.5049 - val_acc: 0.8237\n",
      "Epoch 262/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0513e-05 - acc: 1.0000 - val_loss: 1.4860 - val_acc: 0.8221\n",
      "Epoch 263/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 2.1587 - val_acc: 0.7564\n",
      "Epoch 264/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0125 - acc: 0.9955 - val_loss: 1.4456 - val_acc: 0.7564\n",
      "Epoch 265/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 1.5105 - val_acc: 0.8157\n",
      "Epoch 266/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.8309e-04 - acc: 1.0000 - val_loss: 1.3655 - val_acc: 0.8237\n",
      "Epoch 267/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3821e-04 - acc: 1.0000 - val_loss: 1.2719 - val_acc: 0.8349\n",
      "Epoch 268/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5733e-04 - acc: 1.0000 - val_loss: 1.4434 - val_acc: 0.8301\n",
      "Epoch 269/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7266e-05 - acc: 1.0000 - val_loss: 1.4002 - val_acc: 0.8301\n",
      "Epoch 270/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4654e-05 - acc: 1.0000 - val_loss: 1.4080 - val_acc: 0.8333\n",
      "Epoch 271/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3375e-05 - acc: 1.0000 - val_loss: 1.4265 - val_acc: 0.8333\n",
      "Epoch 272/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5101e-05 - acc: 1.0000 - val_loss: 1.4378 - val_acc: 0.8285\n",
      "Epoch 273/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6308e-05 - acc: 1.0000 - val_loss: 1.4509 - val_acc: 0.8285\n",
      "Epoch 274/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6134e-05 - acc: 1.0000 - val_loss: 1.4774 - val_acc: 0.8269\n",
      "Epoch 275/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3372e-05 - acc: 1.0000 - val_loss: 1.4677 - val_acc: 0.8317\n",
      "Epoch 276/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2430e-05 - acc: 1.0000 - val_loss: 1.4688 - val_acc: 0.8365\n",
      "Epoch 277/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6441e-05 - acc: 1.0000 - val_loss: 1.4926 - val_acc: 0.8285\n",
      "Epoch 278/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1258e-04 - acc: 1.0000 - val_loss: 1.7433 - val_acc: 0.8205\n",
      "Epoch 279/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0196 - acc: 0.9937 - val_loss: 1.4018 - val_acc: 0.8061\n",
      "Epoch 280/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0056 - acc: 0.9966 - val_loss: 0.8825 - val_acc: 0.8526\n",
      "Epoch 281/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.4488 - val_acc: 0.7933\n",
      "Epoch 282/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7209e-04 - acc: 1.0000 - val_loss: 1.5114 - val_acc: 0.7949\n",
      "Epoch 283/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.7283e-05 - acc: 1.0000 - val_loss: 1.5862 - val_acc: 0.7949\n",
      "Epoch 284/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 1.6389 - val_acc: 0.7821\n",
      "Epoch 285/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 1.1487 - val_acc: 0.8510\n",
      "Epoch 286/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 1.3593 - val_acc: 0.8269\n",
      "Epoch 287/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0015 - acc: 0.9993 - val_loss: 1.5533 - val_acc: 0.8109\n",
      "Epoch 288/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.5392e-05 - acc: 1.0000 - val_loss: 1.4383 - val_acc: 0.8301\n",
      "Epoch 289/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8578e-05 - acc: 1.0000 - val_loss: 1.4999 - val_acc: 0.8253\n",
      "Epoch 290/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.7043e-05 - acc: 1.0000 - val_loss: 1.4987 - val_acc: 0.8269\n",
      "Epoch 291/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5963e-05 - acc: 1.0000 - val_loss: 1.5391 - val_acc: 0.8221\n",
      "Epoch 292/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 2.0888 - val_acc: 0.7708\n",
      "Epoch 293/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0185 - acc: 0.9955 - val_loss: 1.1265 - val_acc: 0.8365\n",
      "Epoch 294/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.9951e-04 - acc: 1.0000 - val_loss: 1.5237 - val_acc: 0.8125\n",
      "Epoch 295/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4670e-04 - acc: 1.0000 - val_loss: 1.3839 - val_acc: 0.8189\n",
      "Epoch 296/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.4209e-05 - acc: 1.0000 - val_loss: 1.4977 - val_acc: 0.8173\n",
      "Epoch 297/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0325e-05 - acc: 1.0000 - val_loss: 1.5573 - val_acc: 0.8109\n",
      "Epoch 298/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8956e-05 - acc: 1.0000 - val_loss: 1.4485 - val_acc: 0.8189\n",
      "Epoch 299/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0888e-05 - acc: 1.0000 - val_loss: 1.4909 - val_acc: 0.8189\n",
      "Epoch 300/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6492e-05 - acc: 1.0000 - val_loss: 1.4985 - val_acc: 0.8189\n",
      "Epoch 301/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3339e-05 - acc: 1.0000 - val_loss: 1.5958 - val_acc: 0.8141\n",
      "Epoch 302/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0089e-05 - acc: 1.0000 - val_loss: 1.5931 - val_acc: 0.8157\n",
      "Epoch 303/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0518e-05 - acc: 1.0000 - val_loss: 1.6393 - val_acc: 0.8141\n",
      "Epoch 304/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9652e-05 - acc: 1.0000 - val_loss: 1.6646 - val_acc: 0.8141\n",
      "Epoch 305/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4282e-05 - acc: 1.0000 - val_loss: 1.6534 - val_acc: 0.8157\n",
      "Epoch 306/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.8778e-06 - acc: 1.0000 - val_loss: 1.6699 - val_acc: 0.8157\n",
      "Epoch 307/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0205e-05 - acc: 1.0000 - val_loss: 1.6707 - val_acc: 0.8173\n",
      "Epoch 308/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2679e-06 - acc: 1.0000 - val_loss: 1.6837 - val_acc: 0.8173\n",
      "Epoch 309/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2287e-05 - acc: 1.0000 - val_loss: 1.7050 - val_acc: 0.8173\n",
      "Epoch 310/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.3092e-06 - acc: 1.0000 - val_loss: 1.7296 - val_acc: 0.8173\n",
      "Epoch 311/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.7298e-06 - acc: 1.0000 - val_loss: 1.7529 - val_acc: 0.8141\n",
      "Epoch 312/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3760e-06 - acc: 1.0000 - val_loss: 1.7613 - val_acc: 0.8141\n",
      "Epoch 313/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.6081e-06 - acc: 1.0000 - val_loss: 1.7620 - val_acc: 0.8173\n",
      "Epoch 314/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.3925e-06 - acc: 1.0000 - val_loss: 1.7483 - val_acc: 0.8189\n",
      "Epoch 315/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0777e-04 - acc: 1.0000 - val_loss: 1.5518 - val_acc: 0.8365\n",
      "Epoch 316/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4874e-05 - acc: 1.0000 - val_loss: 1.8518 - val_acc: 0.8125\n",
      "Epoch 317/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8201e-05 - acc: 1.0000 - val_loss: 1.7218 - val_acc: 0.8301\n",
      "Epoch 318/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8877e-06 - acc: 1.0000 - val_loss: 1.7235 - val_acc: 0.8285\n",
      "Epoch 319/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5594e-06 - acc: 1.0000 - val_loss: 1.7809 - val_acc: 0.8221\n",
      "Epoch 320/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6986e-06 - acc: 1.0000 - val_loss: 1.8088 - val_acc: 0.8221\n",
      "Epoch 321/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2493e-06 - acc: 1.0000 - val_loss: 1.8510 - val_acc: 0.8157\n",
      "Epoch 322/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9772e-06 - acc: 1.0000 - val_loss: 1.8773 - val_acc: 0.8141\n",
      "Epoch 323/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8329e-06 - acc: 1.0000 - val_loss: 1.8786 - val_acc: 0.8157\n",
      "Epoch 324/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7844e-06 - acc: 1.0000 - val_loss: 1.8837 - val_acc: 0.8157\n",
      "Epoch 325/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9797e-06 - acc: 1.0000 - val_loss: 1.9138 - val_acc: 0.8109\n",
      "Epoch 326/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6343e-06 - acc: 1.0000 - val_loss: 1.8950 - val_acc: 0.8157\n",
      "Epoch 327/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4224e-06 - acc: 1.0000 - val_loss: 1.9022 - val_acc: 0.8141\n",
      "Epoch 328/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3724e-06 - acc: 1.0000 - val_loss: 1.8993 - val_acc: 0.8157\n",
      "Epoch 329/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7859e-06 - acc: 1.0000 - val_loss: 1.9158 - val_acc: 0.8189\n",
      "Epoch 330/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5069e-06 - acc: 1.0000 - val_loss: 2.0221 - val_acc: 0.8109\n",
      "Epoch 331/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2662e-06 - acc: 1.0000 - val_loss: 1.9359 - val_acc: 0.8157\n",
      "Epoch 332/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2158e-06 - acc: 1.0000 - val_loss: 1.9548 - val_acc: 0.8157\n",
      "Epoch 333/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3744e-07 - acc: 1.0000 - val_loss: 1.9427 - val_acc: 0.8157\n",
      "Epoch 334/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.9563e-07 - acc: 1.0000 - val_loss: 1.9517 - val_acc: 0.8157\n",
      "Epoch 335/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.8033e-07 - acc: 1.0000 - val_loss: 1.9831 - val_acc: 0.8157\n",
      "Epoch 336/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2077e-06 - acc: 1.0000 - val_loss: 1.9969 - val_acc: 0.8141\n",
      "Epoch 337/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2690e-07 - acc: 1.0000 - val_loss: 2.0073 - val_acc: 0.8125\n",
      "Epoch 338/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.0628e-07 - acc: 1.0000 - val_loss: 1.9963 - val_acc: 0.8141\n",
      "Epoch 339/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4275e-07 - acc: 1.0000 - val_loss: 2.0116 - val_acc: 0.8141\n",
      "Epoch 340/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0191 - acc: 0.9959 - val_loss: 1.3444 - val_acc: 0.8349\n",
      "Epoch 341/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 1.6022 - val_acc: 0.8093\n",
      "Epoch 342/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4714e-04 - acc: 1.0000 - val_loss: 1.4960 - val_acc: 0.8173\n",
      "Epoch 343/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8790e-04 - acc: 1.0000 - val_loss: 1.4930 - val_acc: 0.8237\n",
      "Epoch 344/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4572e-05 - acc: 1.0000 - val_loss: 1.5270 - val_acc: 0.8189\n",
      "Epoch 345/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6145e-04 - acc: 1.0000 - val_loss: 1.6377 - val_acc: 0.8061\n",
      "Epoch 346/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9382e-05 - acc: 1.0000 - val_loss: 1.5993 - val_acc: 0.8141\n",
      "Epoch 347/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2315e-05 - acc: 1.0000 - val_loss: 1.6154 - val_acc: 0.8141\n",
      "Epoch 348/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7449e-05 - acc: 1.0000 - val_loss: 1.6035 - val_acc: 0.8157\n",
      "Epoch 349/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5192e-05 - acc: 1.0000 - val_loss: 1.5597 - val_acc: 0.8189\n",
      "Epoch 350/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8562e-05 - acc: 1.0000 - val_loss: 1.5821 - val_acc: 0.8189\n",
      "Epoch 351/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3920e-05 - acc: 1.0000 - val_loss: 1.7008 - val_acc: 0.8093\n",
      "Epoch 352/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2253e-05 - acc: 1.0000 - val_loss: 1.7058 - val_acc: 0.8093\n",
      "Epoch 353/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.4927e-06 - acc: 1.0000 - val_loss: 1.7078 - val_acc: 0.8125\n",
      "Epoch 354/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3094e-05 - acc: 1.0000 - val_loss: 1.6864 - val_acc: 0.8141\n",
      "Epoch 355/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8581e-06 - acc: 1.0000 - val_loss: 1.6798 - val_acc: 0.8141\n",
      "Epoch 356/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.9960e-06 - acc: 1.0000 - val_loss: 1.6719 - val_acc: 0.8157\n",
      "Epoch 357/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7199e-06 - acc: 1.0000 - val_loss: 1.7660 - val_acc: 0.8109\n",
      "Epoch 358/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.4724e-06 - acc: 1.0000 - val_loss: 1.7232 - val_acc: 0.8141\n",
      "Epoch 359/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.7497e-06 - acc: 1.0000 - val_loss: 1.7129 - val_acc: 0.8141\n",
      "Epoch 360/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5210e-06 - acc: 1.0000 - val_loss: 1.7247 - val_acc: 0.8141\n",
      "Epoch 361/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0145e-06 - acc: 1.0000 - val_loss: 1.7295 - val_acc: 0.8157\n",
      "Epoch 362/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.2574e-06 - acc: 1.0000 - val_loss: 1.6918 - val_acc: 0.8173\n",
      "Epoch 363/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3921e-06 - acc: 1.0000 - val_loss: 1.7518 - val_acc: 0.8157\n",
      "Epoch 364/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9980e-06 - acc: 1.0000 - val_loss: 1.7224 - val_acc: 0.8157\n",
      "Epoch 365/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3243e-06 - acc: 1.0000 - val_loss: 1.7531 - val_acc: 0.8141\n",
      "Epoch 366/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4828e-06 - acc: 1.0000 - val_loss: 1.7552 - val_acc: 0.8141\n",
      "Epoch 367/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4838e-05 - acc: 1.0000 - val_loss: 1.9852 - val_acc: 0.7981\n",
      "Epoch 368/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3360e-06 - acc: 1.0000 - val_loss: 1.8279 - val_acc: 0.8109\n",
      "Epoch 369/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4305e-06 - acc: 1.0000 - val_loss: 1.7984 - val_acc: 0.8157\n",
      "Epoch 370/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0211e-06 - acc: 1.0000 - val_loss: 1.8323 - val_acc: 0.8157\n",
      "Epoch 371/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6776e-06 - acc: 1.0000 - val_loss: 1.8808 - val_acc: 0.8125\n",
      "Epoch 372/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3066e-06 - acc: 1.0000 - val_loss: 1.8865 - val_acc: 0.8125\n",
      "Epoch 373/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4367e-06 - acc: 1.0000 - val_loss: 1.8903 - val_acc: 0.8125\n",
      "Epoch 374/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2120e-06 - acc: 1.0000 - val_loss: 1.9119 - val_acc: 0.8125\n",
      "Epoch 375/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0808e-06 - acc: 1.0000 - val_loss: 1.8326 - val_acc: 0.8173\n",
      "Epoch 376/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1556e-06 - acc: 1.0000 - val_loss: 1.8964 - val_acc: 0.8157\n",
      "Epoch 377/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3871e-06 - acc: 1.0000 - val_loss: 1.9491 - val_acc: 0.8125\n",
      "Epoch 378/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5349e-06 - acc: 1.0000 - val_loss: 1.9556 - val_acc: 0.8109\n",
      "Epoch 379/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1345e-06 - acc: 1.0000 - val_loss: 1.9487 - val_acc: 0.8125\n",
      "Epoch 380/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.4074e-07 - acc: 1.0000 - val_loss: 1.9490 - val_acc: 0.8125\n",
      "Epoch 381/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8985e-07 - acc: 1.0000 - val_loss: 1.9477 - val_acc: 0.8125\n",
      "Epoch 382/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.5023e-07 - acc: 1.0000 - val_loss: 1.9437 - val_acc: 0.8125\n",
      "Epoch 383/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0767e-06 - acc: 1.0000 - val_loss: 1.9260 - val_acc: 0.8141\n",
      "Epoch 384/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0823e-07 - acc: 1.0000 - val_loss: 1.9354 - val_acc: 0.8157\n",
      "Epoch 385/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4198e-07 - acc: 1.0000 - val_loss: 1.9539 - val_acc: 0.8141\n",
      "Epoch 386/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.7267e-07 - acc: 1.0000 - val_loss: 1.9390 - val_acc: 0.8157\n",
      "Epoch 387/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3789e-06 - acc: 1.0000 - val_loss: 1.9855 - val_acc: 0.8141\n",
      "Epoch 388/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6535e-07 - acc: 1.0000 - val_loss: 1.9862 - val_acc: 0.8141\n",
      "Epoch 389/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.0256e-06 - acc: 1.0000 - val_loss: 2.1558 - val_acc: 0.8029\n",
      "Epoch 390/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1708e-06 - acc: 1.0000 - val_loss: 2.2067 - val_acc: 0.7997\n",
      "Epoch 391/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.7194e-07 - acc: 1.0000 - val_loss: 2.0624 - val_acc: 0.8125\n",
      "Epoch 392/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7479e-07 - acc: 1.0000 - val_loss: 2.0868 - val_acc: 0.8109\n",
      "Epoch 393/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1583e-07 - acc: 1.0000 - val_loss: 2.0958 - val_acc: 0.8093\n",
      "Epoch 394/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3650e-07 - acc: 1.0000 - val_loss: 2.0605 - val_acc: 0.8109\n",
      "Epoch 395/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3987e-07 - acc: 1.0000 - val_loss: 2.0635 - val_acc: 0.8141\n",
      "Epoch 396/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3437e-07 - acc: 1.0000 - val_loss: 2.0064 - val_acc: 0.8157\n",
      "Epoch 397/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3266e-07 - acc: 1.0000 - val_loss: 2.1270 - val_acc: 0.8109\n",
      "Epoch 398/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9727e-07 - acc: 1.0000 - val_loss: 2.1096 - val_acc: 0.8125\n",
      "Epoch 399/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1632e-07 - acc: 1.0000 - val_loss: 2.1131 - val_acc: 0.8109\n",
      "Epoch 400/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3380e-07 - acc: 1.0000 - val_loss: 2.1132 - val_acc: 0.8141\n",
      "Epoch 401/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3183e-06 - acc: 1.0000 - val_loss: 2.3895 - val_acc: 0.7965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0580e-06 - acc: 1.0000 - val_loss: 2.0223 - val_acc: 0.8173\n",
      "Epoch 403/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3555e-07 - acc: 1.0000 - val_loss: 2.0819 - val_acc: 0.8157\n",
      "Epoch 404/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7482e-07 - acc: 1.0000 - val_loss: 2.0724 - val_acc: 0.8157\n",
      "Epoch 405/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3415e-07 - acc: 1.0000 - val_loss: 2.1132 - val_acc: 0.8173\n",
      "Epoch 406/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4013e-07 - acc: 1.0000 - val_loss: 2.1233 - val_acc: 0.8173\n",
      "Epoch 407/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0051 - acc: 0.9989 - val_loss: 1.7099 - val_acc: 0.7901\n",
      "Epoch 408/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0200 - acc: 0.9948 - val_loss: 1.1858 - val_acc: 0.8558\n",
      "Epoch 409/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.9351e-04 - acc: 0.9996 - val_loss: 2.6767 - val_acc: 0.7404\n",
      "Epoch 410/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8415e-04 - acc: 1.0000 - val_loss: 1.9544 - val_acc: 0.7933\n",
      "Epoch 411/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0303e-04 - acc: 1.0000 - val_loss: 1.8246 - val_acc: 0.8077\n",
      "Epoch 412/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.1349e-05 - acc: 1.0000 - val_loss: 1.6814 - val_acc: 0.8109\n",
      "Epoch 413/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7902e-05 - acc: 1.0000 - val_loss: 1.7372 - val_acc: 0.8109\n",
      "Epoch 414/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0005e-05 - acc: 1.0000 - val_loss: 1.8032 - val_acc: 0.8093\n",
      "Epoch 415/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4181e-05 - acc: 1.0000 - val_loss: 1.8154 - val_acc: 0.8093\n",
      "Epoch 416/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3234e-05 - acc: 1.0000 - val_loss: 1.8366 - val_acc: 0.8077\n",
      "Epoch 417/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1762e-05 - acc: 1.0000 - val_loss: 1.9055 - val_acc: 0.8077\n",
      "Epoch 418/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9357e-05 - acc: 1.0000 - val_loss: 1.8450 - val_acc: 0.8093\n",
      "Epoch 419/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0920e-05 - acc: 1.0000 - val_loss: 1.7921 - val_acc: 0.8109\n",
      "Epoch 420/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4495e-05 - acc: 1.0000 - val_loss: 1.9877 - val_acc: 0.8061\n",
      "Epoch 421/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.1163e-06 - acc: 1.0000 - val_loss: 1.9743 - val_acc: 0.8061\n",
      "Epoch 422/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.4106e-06 - acc: 1.0000 - val_loss: 1.9734 - val_acc: 0.8061\n",
      "Epoch 423/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4210e-06 - acc: 1.0000 - val_loss: 1.9863 - val_acc: 0.8061\n",
      "Epoch 424/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.2275e-06 - acc: 1.0000 - val_loss: 1.9967 - val_acc: 0.8061\n",
      "Epoch 425/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0759e-06 - acc: 1.0000 - val_loss: 1.9873 - val_acc: 0.8061\n",
      "Epoch 426/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6196e-06 - acc: 1.0000 - val_loss: 1.9671 - val_acc: 0.8045\n",
      "Epoch 427/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2390e-05 - acc: 1.0000 - val_loss: 1.9034 - val_acc: 0.8109\n",
      "Epoch 428/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.2679e-06 - acc: 1.0000 - val_loss: 2.0176 - val_acc: 0.8093\n",
      "Epoch 429/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5891e-06 - acc: 1.0000 - val_loss: 2.0003 - val_acc: 0.8077\n",
      "Epoch 430/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5115e-06 - acc: 1.0000 - val_loss: 1.9874 - val_acc: 0.8061\n",
      "Epoch 431/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2196e-06 - acc: 1.0000 - val_loss: 1.9977 - val_acc: 0.8061\n",
      "Epoch 432/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4404e-06 - acc: 1.0000 - val_loss: 1.9862 - val_acc: 0.8061\n",
      "Epoch 433/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2124e-06 - acc: 1.0000 - val_loss: 1.9715 - val_acc: 0.8061\n",
      "Epoch 434/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4660e-06 - acc: 1.0000 - val_loss: 1.9803 - val_acc: 0.8061\n",
      "Epoch 435/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8853e-06 - acc: 1.0000 - val_loss: 2.0150 - val_acc: 0.8077\n",
      "Epoch 436/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2269e-06 - acc: 1.0000 - val_loss: 2.0099 - val_acc: 0.8077\n",
      "Epoch 437/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5444e-06 - acc: 1.0000 - val_loss: 1.9955 - val_acc: 0.8077\n",
      "Epoch 438/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6625e-06 - acc: 1.0000 - val_loss: 1.9965 - val_acc: 0.8077\n",
      "Epoch 439/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.1664e-06 - acc: 1.0000 - val_loss: 1.9689 - val_acc: 0.8157\n",
      "Epoch 440/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3360e-06 - acc: 1.0000 - val_loss: 2.0494 - val_acc: 0.8077\n",
      "Epoch 441/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1828e-06 - acc: 1.0000 - val_loss: 2.0310 - val_acc: 0.8077\n",
      "Epoch 442/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0095e-06 - acc: 1.0000 - val_loss: 2.0191 - val_acc: 0.8077\n",
      "Epoch 443/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.9253e-07 - acc: 1.0000 - val_loss: 2.0253 - val_acc: 0.8077\n",
      "Epoch 444/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7201e-07 - acc: 1.0000 - val_loss: 2.0200 - val_acc: 0.8077\n",
      "Epoch 445/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1185e-06 - acc: 1.0000 - val_loss: 2.0162 - val_acc: 0.8109\n",
      "Epoch 446/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1782e-06 - acc: 1.0000 - val_loss: 1.9865 - val_acc: 0.8141\n",
      "Epoch 447/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0642e-07 - acc: 1.0000 - val_loss: 2.0015 - val_acc: 0.8125\n",
      "Epoch 448/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3555e-06 - acc: 1.0000 - val_loss: 1.9854 - val_acc: 0.8125\n",
      "Epoch 449/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3678e-07 - acc: 1.0000 - val_loss: 2.0106 - val_acc: 0.8109\n",
      "Epoch 450/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.5887e-07 - acc: 1.0000 - val_loss: 2.0068 - val_acc: 0.8125\n",
      "Epoch 451/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.3295e-07 - acc: 1.0000 - val_loss: 2.0224 - val_acc: 0.8109\n",
      "Epoch 452/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2945e-07 - acc: 1.0000 - val_loss: 2.0786 - val_acc: 0.8093\n",
      "Epoch 453/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.8637e-07 - acc: 1.0000 - val_loss: 2.0454 - val_acc: 0.8109\n",
      "Epoch 454/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5486e-07 - acc: 1.0000 - val_loss: 2.0521 - val_acc: 0.8141\n",
      "Epoch 455/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.9830e-07 - acc: 1.0000 - val_loss: 2.0081 - val_acc: 0.8125\n",
      "Epoch 456/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7745e-07 - acc: 1.0000 - val_loss: 2.0056 - val_acc: 0.8125\n",
      "Epoch 457/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3727e-07 - acc: 1.0000 - val_loss: 2.0301 - val_acc: 0.8125\n",
      "Epoch 458/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.0535e-07 - acc: 1.0000 - val_loss: 2.0451 - val_acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3945e-07 - acc: 1.0000 - val_loss: 2.0441 - val_acc: 0.8125\n",
      "Epoch 460/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1944e-07 - acc: 1.0000 - val_loss: 2.0417 - val_acc: 0.8125\n",
      "Epoch 461/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1904e-07 - acc: 1.0000 - val_loss: 2.0538 - val_acc: 0.8125\n",
      "Epoch 462/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5962e-07 - acc: 1.0000 - val_loss: 2.0451 - val_acc: 0.8125\n",
      "Epoch 463/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6674e-07 - acc: 1.0000 - val_loss: 2.0734 - val_acc: 0.8125\n",
      "Epoch 464/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.5411e-07 - acc: 1.0000 - val_loss: 2.1046 - val_acc: 0.8077\n",
      "Epoch 465/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2318e-07 - acc: 1.0000 - val_loss: 2.0943 - val_acc: 0.8093\n",
      "Epoch 466/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6417e-07 - acc: 1.0000 - val_loss: 2.1269 - val_acc: 0.8093\n",
      "Epoch 467/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9138e-07 - acc: 1.0000 - val_loss: 2.0966 - val_acc: 0.8093\n",
      "Epoch 468/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0506e-07 - acc: 1.0000 - val_loss: 2.0733 - val_acc: 0.8093\n",
      "Epoch 469/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5322e-07 - acc: 1.0000 - val_loss: 2.0741 - val_acc: 0.8109\n",
      "Epoch 470/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9803e-07 - acc: 1.0000 - val_loss: 2.1029 - val_acc: 0.8093\n",
      "Epoch 471/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1740e-07 - acc: 1.0000 - val_loss: 2.0911 - val_acc: 0.8109\n",
      "Epoch 472/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0068 - acc: 0.9993 - val_loss: 1.1090 - val_acc: 0.8205\n",
      "Epoch 473/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0648 - acc: 0.9888 - val_loss: 0.8162 - val_acc: 0.8734\n",
      "Epoch 474/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 1.6544 - val_acc: 0.8125\n",
      "Epoch 475/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.7818e-05 - acc: 1.0000 - val_loss: 1.6775 - val_acc: 0.8093\n",
      "Epoch 476/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4625e-05 - acc: 1.0000 - val_loss: 1.7728 - val_acc: 0.8077\n",
      "Epoch 477/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1905e-05 - acc: 1.0000 - val_loss: 1.7813 - val_acc: 0.8077\n",
      "Epoch 478/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.8479e-05 - acc: 1.0000 - val_loss: 1.6911 - val_acc: 0.8109\n",
      "Epoch 479/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0738e-05 - acc: 1.0000 - val_loss: 1.7311 - val_acc: 0.8109\n",
      "Epoch 480/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5162e-05 - acc: 1.0000 - val_loss: 1.7429 - val_acc: 0.8093\n",
      "Epoch 481/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3793e-05 - acc: 1.0000 - val_loss: 1.7744 - val_acc: 0.8093\n",
      "Epoch 482/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6882e-05 - acc: 1.0000 - val_loss: 1.7631 - val_acc: 0.8077\n",
      "Epoch 483/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7508e-05 - acc: 1.0000 - val_loss: 1.7948 - val_acc: 0.8093\n",
      "Epoch 484/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5053e-05 - acc: 1.0000 - val_loss: 1.8074 - val_acc: 0.8077\n",
      "Epoch 485/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7991e-05 - acc: 1.0000 - val_loss: 1.8556 - val_acc: 0.8045\n",
      "Epoch 486/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5990e-05 - acc: 1.0000 - val_loss: 1.8275 - val_acc: 0.8077\n",
      "Epoch 487/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4187e-05 - acc: 1.0000 - val_loss: 1.8058 - val_acc: 0.8109\n",
      "Epoch 488/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9058e-05 - acc: 1.0000 - val_loss: 1.7411 - val_acc: 0.8125\n",
      "Epoch 489/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7461e-06 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 0.8109\n",
      "Epoch 490/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1057e-05 - acc: 1.0000 - val_loss: 1.7927 - val_acc: 0.8109\n",
      "Epoch 491/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4357e-05 - acc: 1.0000 - val_loss: 1.8451 - val_acc: 0.8109\n",
      "Epoch 492/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.9675e-06 - acc: 1.0000 - val_loss: 1.8581 - val_acc: 0.8093\n",
      "Epoch 493/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.5181e-06 - acc: 1.0000 - val_loss: 1.8315 - val_acc: 0.8125\n",
      "Epoch 494/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7904e-06 - acc: 1.0000 - val_loss: 1.8195 - val_acc: 0.8109\n",
      "Epoch 495/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.6692e-06 - acc: 1.0000 - val_loss: 1.8338 - val_acc: 0.8125\n",
      "Epoch 496/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 2.2308 - val_acc: 0.7468\n",
      "Epoch 497/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0277 - acc: 0.9922 - val_loss: 1.5289 - val_acc: 0.8173\n",
      "Epoch 498/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.7606 - val_acc: 0.7804\n",
      "Epoch 499/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6414e-04 - acc: 1.0000 - val_loss: 1.4847 - val_acc: 0.8045\n",
      "Epoch 500/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.8752e-05 - acc: 1.0000 - val_loss: 1.4757 - val_acc: 0.8061\n",
      "Epoch 501/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3242e-05 - acc: 1.0000 - val_loss: 1.5313 - val_acc: 0.8061\n",
      "Epoch 502/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0410e-05 - acc: 1.0000 - val_loss: 1.6163 - val_acc: 0.7997\n",
      "Epoch 503/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0234e-05 - acc: 1.0000 - val_loss: 1.5745 - val_acc: 0.8045\n",
      "Epoch 504/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7095e-05 - acc: 1.0000 - val_loss: 1.6447 - val_acc: 0.8013\n",
      "Epoch 505/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.6687e-05 - acc: 1.0000 - val_loss: 1.5820 - val_acc: 0.8109\n",
      "Epoch 506/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2934e-05 - acc: 1.0000 - val_loss: 1.6008 - val_acc: 0.8125\n",
      "Epoch 507/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0768e-05 - acc: 1.0000 - val_loss: 1.6292 - val_acc: 0.8077\n",
      "Epoch 508/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 1.7391 - val_acc: 0.7837\n",
      "Epoch 509/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 1.6704 - val_acc: 0.7981\n",
      "Epoch 510/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 1.6758 - val_acc: 0.7981\n",
      "Epoch 511/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0569e-04 - acc: 1.0000 - val_loss: 1.9373 - val_acc: 0.7853\n",
      "Epoch 512/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3513e-05 - acc: 1.0000 - val_loss: 1.8170 - val_acc: 0.7997\n",
      "Epoch 513/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9022e-05 - acc: 1.0000 - val_loss: 1.7975 - val_acc: 0.8045\n",
      "Epoch 514/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2316e-05 - acc: 1.0000 - val_loss: 1.8179 - val_acc: 0.8093\n",
      "Epoch 515/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7035e-05 - acc: 1.0000 - val_loss: 1.7746 - val_acc: 0.8109\n",
      "Epoch 516/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5184e-05 - acc: 1.0000 - val_loss: 1.7881 - val_acc: 0.8109\n",
      "Epoch 517/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3504e-05 - acc: 1.0000 - val_loss: 1.8153 - val_acc: 0.8093\n",
      "Epoch 518/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6157e-05 - acc: 1.0000 - val_loss: 1.8331 - val_acc: 0.8093\n",
      "Epoch 519/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0045e-05 - acc: 1.0000 - val_loss: 1.8455 - val_acc: 0.8093\n",
      "Epoch 520/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.5273e-06 - acc: 1.0000 - val_loss: 1.8388 - val_acc: 0.8093\n",
      "Epoch 521/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.5847e-06 - acc: 1.0000 - val_loss: 1.8252 - val_acc: 0.8125\n",
      "Epoch 522/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.6643e-06 - acc: 1.0000 - val_loss: 1.8442 - val_acc: 0.8109\n",
      "Epoch 523/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.6234e-06 - acc: 1.0000 - val_loss: 1.8366 - val_acc: 0.8141\n",
      "Epoch 524/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8544e-06 - acc: 1.0000 - val_loss: 1.8437 - val_acc: 0.8141\n",
      "Epoch 525/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.2171e-06 - acc: 1.0000 - val_loss: 1.8967 - val_acc: 0.8093\n",
      "Epoch 526/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.0079e-06 - acc: 1.0000 - val_loss: 1.8529 - val_acc: 0.8157\n",
      "Epoch 527/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3990e-06 - acc: 1.0000 - val_loss: 1.8974 - val_acc: 0.8125\n",
      "Epoch 528/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.0714e-06 - acc: 1.0000 - val_loss: 1.8825 - val_acc: 0.8157\n",
      "Epoch 529/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7558e-06 - acc: 1.0000 - val_loss: 1.9171 - val_acc: 0.8125\n",
      "Epoch 530/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9662e-06 - acc: 1.0000 - val_loss: 1.8978 - val_acc: 0.8157\n",
      "Epoch 531/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4486e-06 - acc: 1.0000 - val_loss: 1.8952 - val_acc: 0.8125\n",
      "Epoch 532/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9919e-06 - acc: 1.0000 - val_loss: 1.9009 - val_acc: 0.8125\n",
      "Epoch 533/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1431e-06 - acc: 1.0000 - val_loss: 1.9003 - val_acc: 0.8141\n",
      "Epoch 534/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6970e-06 - acc: 1.0000 - val_loss: 1.9151 - val_acc: 0.8109\n",
      "Epoch 535/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3616e-06 - acc: 1.0000 - val_loss: 1.9019 - val_acc: 0.8173\n",
      "Epoch 536/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0068e-05 - acc: 1.0000 - val_loss: 2.0027 - val_acc: 0.8061\n",
      "Epoch 537/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3698e-06 - acc: 1.0000 - val_loss: 2.0199 - val_acc: 0.8109\n",
      "Epoch 538/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4407e-06 - acc: 1.0000 - val_loss: 2.0020 - val_acc: 0.8093\n",
      "Epoch 539/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7104e-06 - acc: 1.0000 - val_loss: 2.0027 - val_acc: 0.8093\n",
      "Epoch 540/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4039e-06 - acc: 1.0000 - val_loss: 1.9822 - val_acc: 0.8125\n",
      "Epoch 541/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0779e-06 - acc: 1.0000 - val_loss: 1.9757 - val_acc: 0.8125\n",
      "Epoch 542/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1832e-06 - acc: 1.0000 - val_loss: 1.9960 - val_acc: 0.8125\n",
      "Epoch 543/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7780e-07 - acc: 1.0000 - val_loss: 2.0100 - val_acc: 0.8125\n",
      "Epoch 544/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9746e-06 - acc: 1.0000 - val_loss: 2.0412 - val_acc: 0.8093\n",
      "Epoch 545/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2138e-06 - acc: 1.0000 - val_loss: 2.0447 - val_acc: 0.8093\n",
      "Epoch 546/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5138e-06 - acc: 1.0000 - val_loss: 1.9981 - val_acc: 0.8125\n",
      "Epoch 547/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4029e-07 - acc: 1.0000 - val_loss: 1.9642 - val_acc: 0.8125\n",
      "Epoch 548/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6492e-07 - acc: 1.0000 - val_loss: 1.9712 - val_acc: 0.8125\n",
      "Epoch 549/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2153e-07 - acc: 1.0000 - val_loss: 2.0092 - val_acc: 0.8125\n",
      "Epoch 550/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6074e-07 - acc: 1.0000 - val_loss: 2.0120 - val_acc: 0.8125\n",
      "Epoch 551/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3855e-07 - acc: 1.0000 - val_loss: 2.0230 - val_acc: 0.8125\n",
      "Epoch 552/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1227e-07 - acc: 1.0000 - val_loss: 2.0525 - val_acc: 0.8109\n",
      "Epoch 553/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1076e-07 - acc: 1.0000 - val_loss: 2.1033 - val_acc: 0.8093\n",
      "Epoch 554/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3345e-07 - acc: 1.0000 - val_loss: 2.0886 - val_acc: 0.8109\n",
      "Epoch 555/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.7381e-07 - acc: 1.0000 - val_loss: 2.2127 - val_acc: 0.7981\n",
      "Epoch 556/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3006e-07 - acc: 1.0000 - val_loss: 2.1821 - val_acc: 0.7997\n",
      "Epoch 557/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.6464e-07 - acc: 1.0000 - val_loss: 2.1522 - val_acc: 0.8029\n",
      "Epoch 558/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4711e-07 - acc: 1.0000 - val_loss: 2.1584 - val_acc: 0.8045\n",
      "Epoch 559/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.6081e-07 - acc: 1.0000 - val_loss: 2.1823 - val_acc: 0.8029\n",
      "Epoch 560/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6938e-07 - acc: 1.0000 - val_loss: 2.1820 - val_acc: 0.8029\n",
      "Epoch 561/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3859e-07 - acc: 1.0000 - val_loss: 2.1661 - val_acc: 0.8029\n",
      "Epoch 562/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8316e-07 - acc: 1.0000 - val_loss: 2.1395 - val_acc: 0.8077\n",
      "Epoch 563/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2361e-07 - acc: 1.0000 - val_loss: 2.1427 - val_acc: 0.8061\n",
      "Epoch 564/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5815e-07 - acc: 1.0000 - val_loss: 2.1322 - val_acc: 0.8077\n",
      "Epoch 565/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5504e-07 - acc: 1.0000 - val_loss: 2.1513 - val_acc: 0.8077\n",
      "Epoch 566/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3450e-06 - acc: 1.0000 - val_loss: 2.3227 - val_acc: 0.7917\n",
      "Epoch 567/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6507e-07 - acc: 1.0000 - val_loss: 2.2986 - val_acc: 0.7949\n",
      "Epoch 568/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5936e-07 - acc: 1.0000 - val_loss: 2.2762 - val_acc: 0.7997\n",
      "Epoch 569/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2494e-07 - acc: 1.0000 - val_loss: 2.3075 - val_acc: 0.7981\n",
      "Epoch 570/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3720e-07 - acc: 1.0000 - val_loss: 2.1578 - val_acc: 0.8125\n",
      "Epoch 571/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1760e-07 - acc: 1.0000 - val_loss: 2.1909 - val_acc: 0.8109\n",
      "Epoch 572/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1995e-07 - acc: 1.0000 - val_loss: 2.2655 - val_acc: 0.8061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7586e-07 - acc: 1.0000 - val_loss: 2.2398 - val_acc: 0.8093\n",
      "Epoch 574/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9632e-07 - acc: 1.0000 - val_loss: 2.2563 - val_acc: 0.8061\n",
      "Epoch 575/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4460e-07 - acc: 1.0000 - val_loss: 2.2301 - val_acc: 0.8093\n",
      "Epoch 576/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7870e-07 - acc: 1.0000 - val_loss: 2.2241 - val_acc: 0.8093\n",
      "Epoch 577/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3007e-07 - acc: 1.0000 - val_loss: 2.2843 - val_acc: 0.8045\n",
      "Epoch 578/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8277e-07 - acc: 1.0000 - val_loss: 2.2627 - val_acc: 0.8061\n",
      "Epoch 579/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6218e-07 - acc: 1.0000 - val_loss: 2.2059 - val_acc: 0.8109\n",
      "Epoch 580/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6590e-07 - acc: 1.0000 - val_loss: 2.2025 - val_acc: 0.8109\n",
      "Epoch 581/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2504e-07 - acc: 1.0000 - val_loss: 2.2154 - val_acc: 0.8125\n",
      "Epoch 582/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8026e-07 - acc: 1.0000 - val_loss: 2.2279 - val_acc: 0.8109\n",
      "Epoch 583/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5132e-07 - acc: 1.0000 - val_loss: 2.2088 - val_acc: 0.8125\n",
      "Epoch 584/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5492e-07 - acc: 1.0000 - val_loss: 2.2276 - val_acc: 0.8109\n",
      "Epoch 585/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4506e-07 - acc: 1.0000 - val_loss: 2.2191 - val_acc: 0.8109\n",
      "Epoch 586/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4094e-07 - acc: 1.0000 - val_loss: 2.2010 - val_acc: 0.8157\n",
      "Epoch 587/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5126e-07 - acc: 1.0000 - val_loss: 2.2360 - val_acc: 0.8109\n",
      "Epoch 588/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5970e-07 - acc: 1.0000 - val_loss: 2.1895 - val_acc: 0.8157\n",
      "Epoch 589/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5052e-07 - acc: 1.0000 - val_loss: 2.2296 - val_acc: 0.8125\n",
      "Epoch 590/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4563e-07 - acc: 1.0000 - val_loss: 2.2250 - val_acc: 0.8157\n",
      "Epoch 591/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3597e-07 - acc: 1.0000 - val_loss: 2.2118 - val_acc: 0.8157\n",
      "Epoch 592/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3832e-07 - acc: 1.0000 - val_loss: 2.2139 - val_acc: 0.8157\n",
      "Epoch 593/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3319e-07 - acc: 1.0000 - val_loss: 2.2248 - val_acc: 0.8141\n",
      "Epoch 594/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2071e-07 - acc: 1.0000 - val_loss: 2.1650 - val_acc: 0.8141\n",
      "Epoch 595/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5735e-07 - acc: 1.0000 - val_loss: 2.1831 - val_acc: 0.8157\n",
      "Epoch 596/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3303e-07 - acc: 1.0000 - val_loss: 2.2004 - val_acc: 0.8157\n",
      "Epoch 597/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3048e-07 - acc: 1.0000 - val_loss: 2.2096 - val_acc: 0.8157\n",
      "Epoch 598/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3761e-07 - acc: 1.0000 - val_loss: 2.2161 - val_acc: 0.8157\n",
      "Epoch 599/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4328e-07 - acc: 1.0000 - val_loss: 2.2229 - val_acc: 0.8141\n",
      "Epoch 600/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7835e-07 - acc: 1.0000 - val_loss: 2.4065 - val_acc: 0.7901\n",
      "Epoch 601/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3234e-07 - acc: 1.0000 - val_loss: 2.3107 - val_acc: 0.8061\n",
      "Epoch 602/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2474e-07 - acc: 1.0000 - val_loss: 2.2945 - val_acc: 0.8061\n",
      "Epoch 603/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2559e-07 - acc: 1.0000 - val_loss: 2.2733 - val_acc: 0.8093\n",
      "Epoch 604/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2659e-07 - acc: 1.0000 - val_loss: 2.2654 - val_acc: 0.8125\n",
      "Epoch 605/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2361e-07 - acc: 1.0000 - val_loss: 2.2844 - val_acc: 0.8093\n",
      "Epoch 606/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2908e-07 - acc: 1.0000 - val_loss: 2.2866 - val_acc: 0.8077\n",
      "Epoch 607/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3019e-07 - acc: 1.0000 - val_loss: 2.2841 - val_acc: 0.8109\n",
      "Epoch 608/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3686e-07 - acc: 1.0000 - val_loss: 2.3055 - val_acc: 0.8077\n",
      "Epoch 609/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4661e-07 - acc: 1.0000 - val_loss: 2.2973 - val_acc: 0.8077\n",
      "Epoch 610/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2477e-07 - acc: 1.0000 - val_loss: 2.2953 - val_acc: 0.8077\n",
      "Epoch 611/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4506e-07 - acc: 1.0000 - val_loss: 2.0047 - val_acc: 0.8301\n",
      "Epoch 612/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2534e-07 - acc: 1.0000 - val_loss: 2.1793 - val_acc: 0.8173\n",
      "Epoch 613/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2421e-07 - acc: 1.0000 - val_loss: 2.2458 - val_acc: 0.8157\n",
      "Epoch 614/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2912e-07 - acc: 1.0000 - val_loss: 2.2585 - val_acc: 0.8157\n",
      "Epoch 615/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6050e-07 - acc: 1.0000 - val_loss: 2.5372 - val_acc: 0.7853\n",
      "Epoch 616/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2459e-07 - acc: 1.0000 - val_loss: 2.4442 - val_acc: 0.7949\n",
      "Epoch 617/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4681e-07 - acc: 1.0000 - val_loss: 2.4451 - val_acc: 0.7949\n",
      "Epoch 618/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2445e-07 - acc: 1.0000 - val_loss: 2.2715 - val_acc: 0.8109\n",
      "Epoch 619/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2434e-07 - acc: 1.0000 - val_loss: 2.2768 - val_acc: 0.8125\n",
      "Epoch 620/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2434e-07 - acc: 1.0000 - val_loss: 2.2378 - val_acc: 0.8141\n",
      "Epoch 621/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6026e-07 - acc: 1.0000 - val_loss: 2.1200 - val_acc: 0.8205\n",
      "Epoch 622/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2557e-07 - acc: 1.0000 - val_loss: 2.1736 - val_acc: 0.8173\n",
      "Epoch 623/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2514e-07 - acc: 1.0000 - val_loss: 2.2562 - val_acc: 0.8157\n",
      "Epoch 624/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2812e-07 - acc: 1.0000 - val_loss: 2.2170 - val_acc: 0.8157\n",
      "Epoch 625/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2008e-07 - acc: 1.0000 - val_loss: 2.2480 - val_acc: 0.8157\n",
      "Epoch 626/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2183e-07 - acc: 1.0000 - val_loss: 2.2617 - val_acc: 0.8157\n",
      "Epoch 627/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2239e-07 - acc: 1.0000 - val_loss: 2.2489 - val_acc: 0.8157\n",
      "Epoch 628/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2165e-07 - acc: 1.0000 - val_loss: 2.2762 - val_acc: 0.8157\n",
      "Epoch 629/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2054e-07 - acc: 1.0000 - val_loss: 2.2852 - val_acc: 0.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 630/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1950e-07 - acc: 1.0000 - val_loss: 2.3017 - val_acc: 0.8157\n",
      "Epoch 631/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2170e-07 - acc: 1.0000 - val_loss: 2.3025 - val_acc: 0.8157\n",
      "Epoch 632/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1968e-07 - acc: 1.0000 - val_loss: 2.3074 - val_acc: 0.8157\n",
      "Epoch 633/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1979e-07 - acc: 1.0000 - val_loss: 2.3051 - val_acc: 0.8157\n",
      "Epoch 634/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1950e-07 - acc: 1.0000 - val_loss: 2.3146 - val_acc: 0.8157\n",
      "Epoch 635/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4511e-07 - acc: 1.0000 - val_loss: 2.0637 - val_acc: 0.8333\n",
      "Epoch 636/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2619e-07 - acc: 1.0000 - val_loss: 2.2449 - val_acc: 0.8205\n",
      "Epoch 637/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2297e-07 - acc: 1.0000 - val_loss: 2.2886 - val_acc: 0.8173\n",
      "Epoch 638/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2150e-07 - acc: 1.0000 - val_loss: 2.2913 - val_acc: 0.8157\n",
      "Epoch 639/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2179e-07 - acc: 1.0000 - val_loss: 2.3215 - val_acc: 0.8109\n",
      "Epoch 640/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2070e-07 - acc: 1.0000 - val_loss: 2.3321 - val_acc: 0.8093\n",
      "Epoch 641/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1950e-07 - acc: 1.0000 - val_loss: 2.3392 - val_acc: 0.8093\n",
      "Epoch 642/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2134e-07 - acc: 1.0000 - val_loss: 2.3583 - val_acc: 0.8077\n",
      "Epoch 643/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2105e-07 - acc: 1.0000 - val_loss: 2.3534 - val_acc: 0.8045\n",
      "Epoch 644/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3194e-07 - acc: 1.0000 - val_loss: 2.5601 - val_acc: 0.7869\n",
      "Epoch 645/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2519e-07 - acc: 1.0000 - val_loss: 2.3721 - val_acc: 0.8141\n",
      "Epoch 646/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2281e-07 - acc: 1.0000 - val_loss: 2.3415 - val_acc: 0.8173\n",
      "Epoch 647/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2048e-07 - acc: 1.0000 - val_loss: 2.3378 - val_acc: 0.8157\n",
      "Epoch 648/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2619e-07 - acc: 1.0000 - val_loss: 2.3347 - val_acc: 0.8093\n",
      "Epoch 649/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2421e-07 - acc: 1.0000 - val_loss: 2.3457 - val_acc: 0.8125\n",
      "Epoch 650/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2074e-07 - acc: 1.0000 - val_loss: 2.3575 - val_acc: 0.8109\n",
      "Epoch 651/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2019e-07 - acc: 1.0000 - val_loss: 2.3346 - val_acc: 0.8141\n",
      "Epoch 652/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2025e-07 - acc: 1.0000 - val_loss: 2.3075 - val_acc: 0.8189\n",
      "Epoch 653/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1970e-07 - acc: 1.0000 - val_loss: 2.3148 - val_acc: 0.8173\n",
      "Epoch 654/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2025e-07 - acc: 1.0000 - val_loss: 2.3411 - val_acc: 0.8141\n",
      "Epoch 655/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1983e-07 - acc: 1.0000 - val_loss: 2.3426 - val_acc: 0.8141\n",
      "Epoch 656/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2028e-07 - acc: 1.0000 - val_loss: 2.3561 - val_acc: 0.8109\n",
      "Epoch 657/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1930e-07 - acc: 1.0000 - val_loss: 2.3536 - val_acc: 0.8141\n",
      "Epoch 658/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9735e-07 - acc: 1.0000 - val_loss: 2.5407 - val_acc: 0.7933\n",
      "Epoch 659/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2005e-07 - acc: 1.0000 - val_loss: 2.3712 - val_acc: 0.8061\n",
      "Epoch 660/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2492e-07 - acc: 1.0000 - val_loss: 2.1516 - val_acc: 0.8237\n",
      "Epoch 661/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1976e-07 - acc: 1.0000 - val_loss: 2.2943 - val_acc: 0.8125\n",
      "Epoch 662/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2194e-07 - acc: 1.0000 - val_loss: 2.2923 - val_acc: 0.8109\n",
      "Epoch 663/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2172e-07 - acc: 1.0000 - val_loss: 2.2962 - val_acc: 0.8093\n",
      "Epoch 664/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2123e-07 - acc: 1.0000 - val_loss: 2.2627 - val_acc: 0.8157\n",
      "Epoch 665/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1961e-07 - acc: 1.0000 - val_loss: 2.2624 - val_acc: 0.8141\n",
      "Epoch 666/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1941e-07 - acc: 1.0000 - val_loss: 2.2714 - val_acc: 0.8141\n",
      "Epoch 667/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.2706 - val_acc: 0.8125\n",
      "Epoch 668/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1963e-07 - acc: 1.0000 - val_loss: 2.2884 - val_acc: 0.8125\n",
      "Epoch 669/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1941e-07 - acc: 1.0000 - val_loss: 2.2800 - val_acc: 0.8125\n",
      "Epoch 670/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1992e-07 - acc: 1.0000 - val_loss: 2.2392 - val_acc: 0.8157\n",
      "Epoch 671/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1925e-07 - acc: 1.0000 - val_loss: 2.2935 - val_acc: 0.8125\n",
      "Epoch 672/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1945e-07 - acc: 1.0000 - val_loss: 2.3004 - val_acc: 0.8125\n",
      "Epoch 673/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2105e-07 - acc: 1.0000 - val_loss: 2.2706 - val_acc: 0.8141\n",
      "Epoch 674/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1972e-07 - acc: 1.0000 - val_loss: 2.2889 - val_acc: 0.8125\n",
      "Epoch 675/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1988e-07 - acc: 1.0000 - val_loss: 2.3060 - val_acc: 0.8125\n",
      "Epoch 676/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2003e-07 - acc: 1.0000 - val_loss: 2.3100 - val_acc: 0.8109\n",
      "Epoch 677/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0776 - acc: 0.9914 - val_loss: 4.1686 - val_acc: 0.7099\n",
      "Epoch 678/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0087 - acc: 0.9981 - val_loss: 2.3657 - val_acc: 0.7965\n",
      "Epoch 679/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 2.3008 - val_acc: 0.7981\n",
      "Epoch 680/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5351e-04 - acc: 1.0000 - val_loss: 2.3510 - val_acc: 0.7965\n",
      "Epoch 681/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9587e-05 - acc: 1.0000 - val_loss: 2.3811 - val_acc: 0.7965\n",
      "Epoch 682/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9245e-05 - acc: 1.0000 - val_loss: 2.4611 - val_acc: 0.7949\n",
      "Epoch 683/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2599e-05 - acc: 1.0000 - val_loss: 2.3823 - val_acc: 0.7965\n",
      "Epoch 684/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.1867e-06 - acc: 1.0000 - val_loss: 2.3851 - val_acc: 0.7965\n",
      "Epoch 685/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.3057e-06 - acc: 1.0000 - val_loss: 2.3594 - val_acc: 0.7981\n",
      "Epoch 686/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2488e-05 - acc: 1.0000 - val_loss: 2.3774 - val_acc: 0.7965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 687/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6888e-05 - acc: 1.0000 - val_loss: 2.3573 - val_acc: 0.7981\n",
      "Epoch 688/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0045 - acc: 0.9993 - val_loss: 2.4863 - val_acc: 0.7756\n",
      "Epoch 689/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 2.4217 - val_acc: 0.7821\n",
      "Epoch 690/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8623e-05 - acc: 1.0000 - val_loss: 2.3602 - val_acc: 0.7917\n",
      "Epoch 691/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9100e-05 - acc: 1.0000 - val_loss: 2.3172 - val_acc: 0.7933\n",
      "Epoch 692/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3385e-04 - acc: 1.0000 - val_loss: 2.2908 - val_acc: 0.7965\n",
      "Epoch 693/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1154e-05 - acc: 1.0000 - val_loss: 2.2678 - val_acc: 0.8029\n",
      "Epoch 694/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8932e-05 - acc: 1.0000 - val_loss: 2.2615 - val_acc: 0.8061\n",
      "Epoch 695/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9965e-05 - acc: 1.0000 - val_loss: 2.3361 - val_acc: 0.7917\n",
      "Epoch 696/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6163e-06 - acc: 1.0000 - val_loss: 2.2556 - val_acc: 0.8029\n",
      "Epoch 697/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2758e-05 - acc: 1.0000 - val_loss: 2.2127 - val_acc: 0.8093\n",
      "Epoch 698/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8181e-06 - acc: 1.0000 - val_loss: 2.2486 - val_acc: 0.8045\n",
      "Epoch 699/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0812e-06 - acc: 1.0000 - val_loss: 2.2495 - val_acc: 0.8029\n",
      "Epoch 700/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.8090e-06 - acc: 1.0000 - val_loss: 2.2694 - val_acc: 0.8013\n",
      "Epoch 701/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3433e-06 - acc: 1.0000 - val_loss: 2.2747 - val_acc: 0.8013\n",
      "Epoch 702/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9454e-06 - acc: 1.0000 - val_loss: 2.2866 - val_acc: 0.7997\n",
      "Epoch 703/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6030e-06 - acc: 1.0000 - val_loss: 2.2900 - val_acc: 0.7997\n",
      "Epoch 704/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4788e-06 - acc: 1.0000 - val_loss: 2.2796 - val_acc: 0.8013\n",
      "Epoch 705/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.9105e-07 - acc: 1.0000 - val_loss: 2.2625 - val_acc: 0.8013\n",
      "Epoch 706/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0345 - acc: 0.9937 - val_loss: 1.1360 - val_acc: 0.8077\n",
      "Epoch 707/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0064 - acc: 0.9989 - val_loss: 2.0947 - val_acc: 0.7837\n",
      "Epoch 708/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 1.8947 - val_acc: 0.8141\n",
      "Epoch 709/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2867e-05 - acc: 1.0000 - val_loss: 1.9224 - val_acc: 0.8093\n",
      "Epoch 710/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3569e-05 - acc: 1.0000 - val_loss: 1.9128 - val_acc: 0.8109\n",
      "Epoch 711/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.3243e-06 - acc: 1.0000 - val_loss: 1.9238 - val_acc: 0.8109\n",
      "Epoch 712/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9172e-05 - acc: 1.0000 - val_loss: 1.9326 - val_acc: 0.8109\n",
      "Epoch 713/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8785e-06 - acc: 1.0000 - val_loss: 1.9514 - val_acc: 0.8109\n",
      "Epoch 714/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2206e-05 - acc: 1.0000 - val_loss: 1.9583 - val_acc: 0.8109\n",
      "Epoch 715/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0578e-05 - acc: 1.0000 - val_loss: 1.9755 - val_acc: 0.8093\n",
      "Epoch 716/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2909e-05 - acc: 1.0000 - val_loss: 1.9566 - val_acc: 0.8109\n",
      "Epoch 717/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5795e-05 - acc: 1.0000 - val_loss: 1.9049 - val_acc: 0.8109\n",
      "Epoch 718/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2018e-06 - acc: 1.0000 - val_loss: 1.9324 - val_acc: 0.8109\n",
      "Epoch 719/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4774e-06 - acc: 1.0000 - val_loss: 1.9392 - val_acc: 0.8109\n",
      "Epoch 720/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5204e-06 - acc: 1.0000 - val_loss: 1.9514 - val_acc: 0.8109\n",
      "Epoch 721/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4341e-06 - acc: 1.0000 - val_loss: 1.9504 - val_acc: 0.8109\n",
      "Epoch 722/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0774e-06 - acc: 1.0000 - val_loss: 1.9493 - val_acc: 0.8109\n",
      "Epoch 723/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0395e-05 - acc: 1.0000 - val_loss: 1.9803 - val_acc: 0.8109\n",
      "Epoch 724/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2835e-05 - acc: 1.0000 - val_loss: 1.9062 - val_acc: 0.8109\n",
      "Epoch 725/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0259e-06 - acc: 1.0000 - val_loss: 1.9297 - val_acc: 0.8109\n",
      "Epoch 726/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1142e-06 - acc: 1.0000 - val_loss: 1.9468 - val_acc: 0.8109\n",
      "Epoch 727/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.9547e-06 - acc: 1.0000 - val_loss: 1.8973 - val_acc: 0.8109\n",
      "Epoch 728/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.8841e-06 - acc: 1.0000 - val_loss: 1.9326 - val_acc: 0.8093\n",
      "Epoch 729/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0437e-06 - acc: 1.0000 - val_loss: 1.9425 - val_acc: 0.8077\n",
      "Epoch 730/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.7259e-06 - acc: 1.0000 - val_loss: 1.9982 - val_acc: 0.8077\n",
      "Epoch 731/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3706e-06 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 0.8093\n",
      "Epoch 732/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3255e-06 - acc: 1.0000 - val_loss: 2.0018 - val_acc: 0.8077\n",
      "Epoch 733/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3431e-06 - acc: 1.0000 - val_loss: 1.9927 - val_acc: 0.8077\n",
      "Epoch 734/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3561e-06 - acc: 1.0000 - val_loss: 1.9975 - val_acc: 0.8077\n",
      "Epoch 735/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8005e-05 - acc: 1.0000 - val_loss: 2.1691 - val_acc: 0.8013\n",
      "Epoch 736/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1648e-05 - acc: 1.0000 - val_loss: 1.9785 - val_acc: 0.8093\n",
      "Epoch 737/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6360e-06 - acc: 1.0000 - val_loss: 1.9727 - val_acc: 0.8093\n",
      "Epoch 738/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7726e-06 - acc: 1.0000 - val_loss: 1.9981 - val_acc: 0.8093\n",
      "Epoch 739/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0109e-06 - acc: 1.0000 - val_loss: 2.0112 - val_acc: 0.8093\n",
      "Epoch 740/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0715e-06 - acc: 1.0000 - val_loss: 2.0209 - val_acc: 0.8093\n",
      "Epoch 741/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.5083e-07 - acc: 1.0000 - val_loss: 2.0254 - val_acc: 0.8093\n",
      "Epoch 742/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.8398e-07 - acc: 1.0000 - val_loss: 2.0334 - val_acc: 0.8093\n",
      "Epoch 743/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2479e-06 - acc: 1.0000 - val_loss: 2.0197 - val_acc: 0.8093\n",
      "Epoch 744/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.9192e-07 - acc: 1.0000 - val_loss: 2.0254 - val_acc: 0.8093\n",
      "Epoch 745/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8079e-06 - acc: 1.0000 - val_loss: 2.0436 - val_acc: 0.8093\n",
      "Epoch 746/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.2299e-07 - acc: 1.0000 - val_loss: 2.0328 - val_acc: 0.8093\n",
      "Epoch 747/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8111e-07 - acc: 1.0000 - val_loss: 2.0423 - val_acc: 0.8093\n",
      "Epoch 748/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.0823e-07 - acc: 1.0000 - val_loss: 2.0394 - val_acc: 0.8093\n",
      "Epoch 749/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6727e-07 - acc: 1.0000 - val_loss: 2.0273 - val_acc: 0.8093\n",
      "Epoch 750/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3404e-07 - acc: 1.0000 - val_loss: 2.0332 - val_acc: 0.8093\n",
      "Epoch 751/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4169e-07 - acc: 1.0000 - val_loss: 2.0527 - val_acc: 0.8093\n",
      "Epoch 752/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.0476e-07 - acc: 1.0000 - val_loss: 2.0318 - val_acc: 0.8125\n",
      "Epoch 753/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.0138e-07 - acc: 1.0000 - val_loss: 2.0274 - val_acc: 0.8125\n",
      "Epoch 754/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.9256e-07 - acc: 1.0000 - val_loss: 2.0152 - val_acc: 0.8125\n",
      "Epoch 755/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7052e-07 - acc: 1.0000 - val_loss: 2.0488 - val_acc: 0.8125\n",
      "Epoch 756/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8725e-05 - acc: 1.0000 - val_loss: 2.5984 - val_acc: 0.7788\n",
      "Epoch 757/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0477 - acc: 0.9914 - val_loss: 1.5913 - val_acc: 0.8413\n",
      "Epoch 758/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 2.2003 - val_acc: 0.7837\n",
      "Epoch 759/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3229e-04 - acc: 1.0000 - val_loss: 2.0215 - val_acc: 0.8029\n",
      "Epoch 760/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3522e-04 - acc: 1.0000 - val_loss: 2.1826 - val_acc: 0.7933\n",
      "Epoch 761/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.9488e-04 - acc: 0.9996 - val_loss: 1.4670 - val_acc: 0.8494\n",
      "Epoch 762/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 1.7057 - val_acc: 0.8317\n",
      "Epoch 763/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.6121e-05 - acc: 1.0000 - val_loss: 1.7409 - val_acc: 0.8253\n",
      "Epoch 764/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1585e-05 - acc: 1.0000 - val_loss: 1.7876 - val_acc: 0.8253\n",
      "Epoch 765/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4224e-05 - acc: 1.0000 - val_loss: 1.7967 - val_acc: 0.8253\n",
      "Epoch 766/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9913e-05 - acc: 1.0000 - val_loss: 1.8120 - val_acc: 0.8253\n",
      "Epoch 767/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5459e-05 - acc: 1.0000 - val_loss: 1.8400 - val_acc: 0.8253\n",
      "Epoch 768/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.8893e-05 - acc: 1.0000 - val_loss: 1.5920 - val_acc: 0.8397\n",
      "Epoch 769/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5928e-05 - acc: 1.0000 - val_loss: 1.6702 - val_acc: 0.8349\n",
      "Epoch 770/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4180e-05 - acc: 1.0000 - val_loss: 1.7906 - val_acc: 0.8253\n",
      "Epoch 771/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3649e-06 - acc: 1.0000 - val_loss: 1.8272 - val_acc: 0.8237\n",
      "Epoch 772/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8546e-05 - acc: 1.0000 - val_loss: 1.8088 - val_acc: 0.8237\n",
      "Epoch 773/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.1752e-06 - acc: 1.0000 - val_loss: 1.8258 - val_acc: 0.8237\n",
      "Epoch 774/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.5806e-06 - acc: 1.0000 - val_loss: 1.8423 - val_acc: 0.8237\n",
      "Epoch 775/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9602e-06 - acc: 1.0000 - val_loss: 1.8583 - val_acc: 0.8237\n",
      "Epoch 776/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.6350e-06 - acc: 1.0000 - val_loss: 1.8364 - val_acc: 0.8237\n",
      "Epoch 777/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3836e-06 - acc: 1.0000 - val_loss: 1.8599 - val_acc: 0.8237\n",
      "Epoch 778/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1828e-06 - acc: 1.0000 - val_loss: 1.8254 - val_acc: 0.8237\n",
      "Epoch 779/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3566e-06 - acc: 1.0000 - val_loss: 1.8362 - val_acc: 0.8237\n",
      "Epoch 780/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0605e-05 - acc: 1.0000 - val_loss: 1.9214 - val_acc: 0.8173\n",
      "Epoch 781/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6701e-06 - acc: 1.0000 - val_loss: 1.9177 - val_acc: 0.8173\n",
      "Epoch 782/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9903e-06 - acc: 1.0000 - val_loss: 1.9174 - val_acc: 0.8173\n",
      "Epoch 783/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3270e-06 - acc: 1.0000 - val_loss: 1.9238 - val_acc: 0.8173\n",
      "Epoch 784/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5329e-06 - acc: 1.0000 - val_loss: 1.9510 - val_acc: 0.8173\n",
      "Epoch 785/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7269e-05 - acc: 1.0000 - val_loss: 1.8420 - val_acc: 0.8301\n",
      "Epoch 786/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7616e-06 - acc: 1.0000 - val_loss: 1.8955 - val_acc: 0.8253\n",
      "Epoch 787/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7843e-06 - acc: 1.0000 - val_loss: 1.9277 - val_acc: 0.8237\n",
      "Epoch 788/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1727e-06 - acc: 1.0000 - val_loss: 1.9579 - val_acc: 0.8221\n",
      "Epoch 789/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6002e-06 - acc: 1.0000 - val_loss: 1.9579 - val_acc: 0.8221\n",
      "Epoch 790/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.5027e-06 - acc: 1.0000 - val_loss: 1.9148 - val_acc: 0.8269\n",
      "Epoch 791/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.7795e-06 - acc: 1.0000 - val_loss: 1.9354 - val_acc: 0.8237\n",
      "Epoch 792/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3478e-06 - acc: 1.0000 - val_loss: 1.9634 - val_acc: 0.8221\n",
      "Epoch 793/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1748e-06 - acc: 1.0000 - val_loss: 2.0202 - val_acc: 0.8173\n",
      "Epoch 794/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.0456e-07 - acc: 1.0000 - val_loss: 2.0272 - val_acc: 0.8173\n",
      "Epoch 795/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3619e-06 - acc: 1.0000 - val_loss: 2.0381 - val_acc: 0.8189\n",
      "Epoch 796/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.1994e-07 - acc: 1.0000 - val_loss: 2.0340 - val_acc: 0.8189\n",
      "Epoch 797/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2608e-07 - acc: 1.0000 - val_loss: 2.0381 - val_acc: 0.8189\n",
      "Epoch 798/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.8338e-07 - acc: 1.0000 - val_loss: 2.0464 - val_acc: 0.8189\n",
      "Epoch 799/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.7220e-07 - acc: 1.0000 - val_loss: 2.0426 - val_acc: 0.8189\n",
      "Epoch 800/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7318e-07 - acc: 1.0000 - val_loss: 2.0460 - val_acc: 0.8189\n",
      "Epoch 801/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9945e-07 - acc: 1.0000 - val_loss: 2.0500 - val_acc: 0.8189\n",
      "Epoch 802/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2010e-06 - acc: 1.0000 - val_loss: 2.0707 - val_acc: 0.8173\n",
      "Epoch 803/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.8365e-07 - acc: 1.0000 - val_loss: 2.0772 - val_acc: 0.8189\n",
      "Epoch 804/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2006e-06 - acc: 1.0000 - val_loss: 2.0532 - val_acc: 0.8205\n",
      "Epoch 805/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.3447e-07 - acc: 1.0000 - val_loss: 2.0555 - val_acc: 0.8205\n",
      "Epoch 806/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3753e-07 - acc: 1.0000 - val_loss: 2.0440 - val_acc: 0.8205\n",
      "Epoch 807/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.0708e-07 - acc: 1.0000 - val_loss: 2.0540 - val_acc: 0.8205\n",
      "Epoch 808/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8973e-07 - acc: 1.0000 - val_loss: 2.0686 - val_acc: 0.8189\n",
      "Epoch 809/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.2693e-07 - acc: 1.0000 - val_loss: 2.1169 - val_acc: 0.8109\n",
      "Epoch 810/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2137e-07 - acc: 1.0000 - val_loss: 2.0704 - val_acc: 0.8205\n",
      "Epoch 811/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4352e-07 - acc: 1.0000 - val_loss: 2.0833 - val_acc: 0.8189\n",
      "Epoch 812/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4110e-07 - acc: 1.0000 - val_loss: 2.0889 - val_acc: 0.8189\n",
      "Epoch 813/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 1.6276 - val_acc: 0.7772\n",
      "Epoch 814/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0504 - acc: 0.9914 - val_loss: 1.9792 - val_acc: 0.7837\n",
      "Epoch 815/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 1.4390 - val_acc: 0.8205\n",
      "Epoch 816/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 1.4385 - val_acc: 0.8221\n",
      "Epoch 817/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0228e-04 - acc: 1.0000 - val_loss: 1.4914 - val_acc: 0.8205\n",
      "Epoch 818/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1243e-04 - acc: 1.0000 - val_loss: 1.5668 - val_acc: 0.8189\n",
      "Epoch 819/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4134e-04 - acc: 0.9996 - val_loss: 1.0619 - val_acc: 0.8462\n",
      "Epoch 820/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0158e-04 - acc: 1.0000 - val_loss: 1.8361 - val_acc: 0.7901\n",
      "Epoch 821/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0240e-04 - acc: 1.0000 - val_loss: 1.7583 - val_acc: 0.8061\n",
      "Epoch 822/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2312e-04 - acc: 0.9996 - val_loss: 1.6734 - val_acc: 0.7853\n",
      "Epoch 823/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 2.2463 - val_acc: 0.7740\n",
      "Epoch 824/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7818e-05 - acc: 1.0000 - val_loss: 2.1938 - val_acc: 0.7772\n",
      "Epoch 825/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.6226e-05 - acc: 1.0000 - val_loss: 2.1618 - val_acc: 0.7853\n",
      "Epoch 826/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9207e-05 - acc: 1.0000 - val_loss: 2.1462 - val_acc: 0.7837\n",
      "Epoch 827/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3110e-05 - acc: 1.0000 - val_loss: 2.1182 - val_acc: 0.7869\n",
      "Epoch 828/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2226e-05 - acc: 1.0000 - val_loss: 2.0194 - val_acc: 0.7869\n",
      "Epoch 829/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0503e-05 - acc: 1.0000 - val_loss: 2.0312 - val_acc: 0.7917\n",
      "Epoch 830/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.5913e-06 - acc: 1.0000 - val_loss: 2.0327 - val_acc: 0.7917\n",
      "Epoch 831/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2790e-05 - acc: 1.0000 - val_loss: 2.0559 - val_acc: 0.7917\n",
      "Epoch 832/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.8283e-06 - acc: 1.0000 - val_loss: 2.0550 - val_acc: 0.7917\n",
      "Epoch 833/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7023e-06 - acc: 1.0000 - val_loss: 2.0692 - val_acc: 0.7917\n",
      "Epoch 834/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.5731e-06 - acc: 1.0000 - val_loss: 2.0831 - val_acc: 0.7901\n",
      "Epoch 835/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7123e-06 - acc: 1.0000 - val_loss: 2.0846 - val_acc: 0.7933\n",
      "Epoch 836/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.8493e-06 - acc: 1.0000 - val_loss: 2.0996 - val_acc: 0.7917\n",
      "Epoch 837/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.4657e-06 - acc: 1.0000 - val_loss: 2.1023 - val_acc: 0.7917\n",
      "Epoch 838/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4070e-06 - acc: 1.0000 - val_loss: 2.0828 - val_acc: 0.7933\n",
      "Epoch 839/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6066e-06 - acc: 1.0000 - val_loss: 2.0823 - val_acc: 0.7933\n",
      "Epoch 840/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2126e-06 - acc: 1.0000 - val_loss: 2.1259 - val_acc: 0.7901\n",
      "Epoch 841/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0010e-06 - acc: 1.0000 - val_loss: 2.1477 - val_acc: 0.7885\n",
      "Epoch 842/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4356e-06 - acc: 1.0000 - val_loss: 2.1338 - val_acc: 0.7901\n",
      "Epoch 843/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3347e-06 - acc: 1.0000 - val_loss: 2.1470 - val_acc: 0.7901\n",
      "Epoch 844/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.7151e-06 - acc: 1.0000 - val_loss: 2.1850 - val_acc: 0.7869\n",
      "Epoch 845/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2552e-06 - acc: 1.0000 - val_loss: 2.1971 - val_acc: 0.7869\n",
      "Epoch 846/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5835e-06 - acc: 1.0000 - val_loss: 2.2006 - val_acc: 0.7885\n",
      "Epoch 847/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9913e-06 - acc: 1.0000 - val_loss: 2.2175 - val_acc: 0.7869\n",
      "Epoch 848/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8905e-06 - acc: 1.0000 - val_loss: 2.2086 - val_acc: 0.7885\n",
      "Epoch 849/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2588e-05 - acc: 1.0000 - val_loss: 2.1459 - val_acc: 0.7901\n",
      "Epoch 850/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1321e-06 - acc: 1.0000 - val_loss: 2.2062 - val_acc: 0.7869\n",
      "Epoch 851/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9298e-06 - acc: 1.0000 - val_loss: 2.2259 - val_acc: 0.7853\n",
      "Epoch 852/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1337e-06 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.7917\n",
      "Epoch 853/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0124 - acc: 0.9978 - val_loss: 3.2254 - val_acc: 0.6378\n",
      "Epoch 854/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0075 - acc: 0.9970 - val_loss: 2.2032 - val_acc: 0.7580\n",
      "Epoch 855/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 2.1911 - val_acc: 0.7965\n",
      "Epoch 856/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.6006e-04 - acc: 0.9996 - val_loss: 2.6302 - val_acc: 0.7756\n",
      "Epoch 857/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.1648e-05 - acc: 1.0000 - val_loss: 2.5637 - val_acc: 0.7853\n",
      "Epoch 858/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5528e-04 - acc: 1.0000 - val_loss: 2.6250 - val_acc: 0.7660\n",
      "Epoch 859/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5734e-05 - acc: 1.0000 - val_loss: 2.5093 - val_acc: 0.7885\n",
      "Epoch 860/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8147e-05 - acc: 1.0000 - val_loss: 2.3436 - val_acc: 0.7997\n",
      "Epoch 861/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3623e-05 - acc: 1.0000 - val_loss: 2.3793 - val_acc: 0.7901\n",
      "Epoch 862/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9590e-06 - acc: 1.0000 - val_loss: 2.3846 - val_acc: 0.7901\n",
      "Epoch 863/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0173e-05 - acc: 1.0000 - val_loss: 2.3690 - val_acc: 0.7933\n",
      "Epoch 864/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5164e-05 - acc: 1.0000 - val_loss: 2.3212 - val_acc: 0.7981\n",
      "Epoch 865/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9206e-06 - acc: 1.0000 - val_loss: 2.3267 - val_acc: 0.7997\n",
      "Epoch 866/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9939e-06 - acc: 1.0000 - val_loss: 2.3290 - val_acc: 0.7981\n",
      "Epoch 867/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7332e-06 - acc: 1.0000 - val_loss: 2.3247 - val_acc: 0.7981\n",
      "Epoch 868/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.2759e-06 - acc: 1.0000 - val_loss: 2.3356 - val_acc: 0.7981\n",
      "Epoch 869/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4254e-06 - acc: 1.0000 - val_loss: 2.3443 - val_acc: 0.7949\n",
      "Epoch 870/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.7402e-06 - acc: 1.0000 - val_loss: 2.3111 - val_acc: 0.8013\n",
      "Epoch 871/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9547e-06 - acc: 1.0000 - val_loss: 2.3281 - val_acc: 0.7997\n",
      "Epoch 872/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9520e-05 - acc: 1.0000 - val_loss: 2.5246 - val_acc: 0.7853\n",
      "Epoch 873/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0531e-05 - acc: 1.0000 - val_loss: 2.4505 - val_acc: 0.7901\n",
      "Epoch 874/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.7847e-06 - acc: 1.0000 - val_loss: 2.4107 - val_acc: 0.7901\n",
      "Epoch 875/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4540e-06 - acc: 1.0000 - val_loss: 2.4037 - val_acc: 0.7933\n",
      "Epoch 876/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3173e-06 - acc: 1.0000 - val_loss: 2.3988 - val_acc: 0.7933\n",
      "Epoch 877/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0909e-06 - acc: 1.0000 - val_loss: 2.3945 - val_acc: 0.7917\n",
      "Epoch 878/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.8340e-06 - acc: 1.0000 - val_loss: 2.3583 - val_acc: 0.7933\n",
      "Epoch 879/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9515e-06 - acc: 1.0000 - val_loss: 2.3243 - val_acc: 0.7949\n",
      "Epoch 880/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1404e-06 - acc: 1.0000 - val_loss: 2.3380 - val_acc: 0.7949\n",
      "Epoch 881/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6786e-06 - acc: 1.0000 - val_loss: 2.3371 - val_acc: 0.7949\n",
      "Epoch 882/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.9221e-06 - acc: 1.0000 - val_loss: 2.2942 - val_acc: 0.7949\n",
      "Epoch 883/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.8271e-06 - acc: 1.0000 - val_loss: 2.2648 - val_acc: 0.7997\n",
      "Epoch 884/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5402e-04 - acc: 1.0000 - val_loss: 3.4975 - val_acc: 0.7051\n",
      "Epoch 885/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0225 - acc: 0.9925 - val_loss: 1.1490 - val_acc: 0.8173\n",
      "Epoch 886/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 2.0484 - val_acc: 0.7821\n",
      "Epoch 887/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1049e-04 - acc: 0.9996 - val_loss: 1.8193 - val_acc: 0.7885\n",
      "Epoch 888/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.1080e-04 - acc: 1.0000 - val_loss: 1.6980 - val_acc: 0.8109\n",
      "Epoch 889/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.3184e-05 - acc: 1.0000 - val_loss: 1.6610 - val_acc: 0.8141\n",
      "Epoch 890/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3110e-05 - acc: 1.0000 - val_loss: 1.6591 - val_acc: 0.8189\n",
      "Epoch 891/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.6125e-05 - acc: 1.0000 - val_loss: 1.6921 - val_acc: 0.8141\n",
      "Epoch 892/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.5338e-05 - acc: 1.0000 - val_loss: 1.7237 - val_acc: 0.8141\n",
      "Epoch 893/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4944e-05 - acc: 1.0000 - val_loss: 1.7458 - val_acc: 0.8141\n",
      "Epoch 894/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6770e-05 - acc: 1.0000 - val_loss: 1.7291 - val_acc: 0.8189\n",
      "Epoch 895/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.3172e-04 - acc: 0.9993 - val_loss: 2.4361 - val_acc: 0.7724\n",
      "Epoch 896/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0058 - acc: 0.9993 - val_loss: 2.0329 - val_acc: 0.7532\n",
      "Epoch 897/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 1.9581 - val_acc: 0.7917\n",
      "Epoch 898/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.3015e-05 - acc: 1.0000 - val_loss: 1.6803 - val_acc: 0.8173\n",
      "Epoch 899/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.6243e-05 - acc: 1.0000 - val_loss: 1.7045 - val_acc: 0.8205\n",
      "Epoch 900/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.7926e-05 - acc: 1.0000 - val_loss: 1.7950 - val_acc: 0.8141\n",
      "Epoch 901/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5324e-05 - acc: 1.0000 - val_loss: 1.8322 - val_acc: 0.8125\n",
      "Epoch 902/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2210e-04 - acc: 1.0000 - val_loss: 1.8556 - val_acc: 0.8077\n",
      "Epoch 903/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3688e-05 - acc: 1.0000 - val_loss: 1.8512 - val_acc: 0.8093\n",
      "Epoch 904/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4446e-05 - acc: 1.0000 - val_loss: 1.8507 - val_acc: 0.8125\n",
      "Epoch 905/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4322e-05 - acc: 1.0000 - val_loss: 1.8429 - val_acc: 0.8189\n",
      "Epoch 906/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4269e-05 - acc: 1.0000 - val_loss: 1.8525 - val_acc: 0.8173\n",
      "Epoch 907/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6364e-05 - acc: 1.0000 - val_loss: 1.8719 - val_acc: 0.8205\n",
      "Epoch 908/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5247e-05 - acc: 1.0000 - val_loss: 1.8217 - val_acc: 0.8253\n",
      "Epoch 909/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.4922e-06 - acc: 1.0000 - val_loss: 1.8335 - val_acc: 0.8253\n",
      "Epoch 910/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5256e-05 - acc: 1.0000 - val_loss: 1.8261 - val_acc: 0.8253\n",
      "Epoch 911/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.2519e-06 - acc: 1.0000 - val_loss: 1.8791 - val_acc: 0.8237\n",
      "Epoch 912/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 7.1218e-06 - acc: 1.0000 - val_loss: 1.8748 - val_acc: 0.8237\n",
      "Epoch 913/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2313e-06 - acc: 1.0000 - val_loss: 1.8743 - val_acc: 0.8253\n",
      "Epoch 914/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.9716e-06 - acc: 1.0000 - val_loss: 1.8814 - val_acc: 0.8253\n",
      "Epoch 915/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9874e-06 - acc: 1.0000 - val_loss: 1.9037 - val_acc: 0.8237\n",
      "Epoch 916/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9880e-06 - acc: 1.0000 - val_loss: 1.9095 - val_acc: 0.8221\n",
      "Epoch 917/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3909e-06 - acc: 1.0000 - val_loss: 1.9007 - val_acc: 0.8253\n",
      "Epoch 918/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.2803e-06 - acc: 1.0000 - val_loss: 1.9634 - val_acc: 0.8173\n",
      "Epoch 919/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.6736e-06 - acc: 1.0000 - val_loss: 1.9678 - val_acc: 0.8173\n",
      "Epoch 920/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.4858e-06 - acc: 1.0000 - val_loss: 1.9778 - val_acc: 0.8189\n",
      "Epoch 921/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.9639e-06 - acc: 1.0000 - val_loss: 1.9549 - val_acc: 0.8205\n",
      "Epoch 922/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0119e-06 - acc: 1.0000 - val_loss: 1.9756 - val_acc: 0.8189\n",
      "Epoch 923/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5301e-06 - acc: 1.0000 - val_loss: 1.9326 - val_acc: 0.8205\n",
      "Epoch 924/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3943e-06 - acc: 1.0000 - val_loss: 1.9520 - val_acc: 0.8221\n",
      "Epoch 925/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5791e-06 - acc: 1.0000 - val_loss: 1.9647 - val_acc: 0.8221\n",
      "Epoch 926/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8804e-06 - acc: 1.0000 - val_loss: 1.9557 - val_acc: 0.8237\n",
      "Epoch 927/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.1507e-07 - acc: 1.0000 - val_loss: 1.9616 - val_acc: 0.8237\n",
      "Epoch 928/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.4066e-07 - acc: 1.0000 - val_loss: 1.9800 - val_acc: 0.8221\n",
      "Epoch 929/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.4349e-06 - acc: 1.0000 - val_loss: 2.0388 - val_acc: 0.8141\n",
      "Epoch 930/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2574e-06 - acc: 1.0000 - val_loss: 2.0302 - val_acc: 0.8157\n",
      "Epoch 931/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.4961e-07 - acc: 1.0000 - val_loss: 1.9916 - val_acc: 0.8221\n",
      "Epoch 932/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.0596e-06 - acc: 1.0000 - val_loss: 2.0287 - val_acc: 0.8189\n",
      "Epoch 933/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.6306e-07 - acc: 1.0000 - val_loss: 2.0060 - val_acc: 0.8205\n",
      "Epoch 934/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6651e-06 - acc: 1.0000 - val_loss: 1.9911 - val_acc: 0.8253\n",
      "Epoch 935/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.1681e-07 - acc: 1.0000 - val_loss: 2.0169 - val_acc: 0.8221\n",
      "Epoch 936/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.6349e-07 - acc: 1.0000 - val_loss: 2.0544 - val_acc: 0.8173\n",
      "Epoch 937/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2965e-06 - acc: 1.0000 - val_loss: 2.0213 - val_acc: 0.8221\n",
      "Epoch 938/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.2859e-07 - acc: 1.0000 - val_loss: 2.0397 - val_acc: 0.8189\n",
      "Epoch 939/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.3837e-07 - acc: 1.0000 - val_loss: 2.0414 - val_acc: 0.8189\n",
      "Epoch 940/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.1197e-07 - acc: 1.0000 - val_loss: 2.0496 - val_acc: 0.8189\n",
      "Epoch 941/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2314e-06 - acc: 1.0000 - val_loss: 2.1076 - val_acc: 0.8109\n",
      "Epoch 942/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 4.5282e-07 - acc: 1.0000 - val_loss: 2.0898 - val_acc: 0.8125\n",
      "Epoch 943/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4427e-07 - acc: 1.0000 - val_loss: 2.0740 - val_acc: 0.8125\n",
      "Epoch 944/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 5.2331e-07 - acc: 1.0000 - val_loss: 2.0741 - val_acc: 0.8141\n",
      "Epoch 945/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 6.4184e-07 - acc: 1.0000 - val_loss: 2.0266 - val_acc: 0.8205\n",
      "Epoch 946/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2613e-06 - acc: 1.0000 - val_loss: 2.2081 - val_acc: 0.8077\n",
      "Epoch 947/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 8.9613e-07 - acc: 1.0000 - val_loss: 2.1119 - val_acc: 0.8173\n",
      "Epoch 948/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.2819e-07 - acc: 1.0000 - val_loss: 2.0742 - val_acc: 0.8189\n",
      "Epoch 949/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1234e-07 - acc: 1.0000 - val_loss: 2.0781 - val_acc: 0.8189\n",
      "Epoch 950/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5053e-07 - acc: 1.0000 - val_loss: 2.0959 - val_acc: 0.8173\n",
      "Epoch 951/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.4523e-07 - acc: 1.0000 - val_loss: 2.0882 - val_acc: 0.8189\n",
      "Epoch 952/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 9.8098e-07 - acc: 1.0000 - val_loss: 2.0158 - val_acc: 0.8253\n",
      "Epoch 953/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.3221e-07 - acc: 1.0000 - val_loss: 2.0376 - val_acc: 0.8253\n",
      "Epoch 954/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.1190e-07 - acc: 1.0000 - val_loss: 2.0624 - val_acc: 0.8253\n",
      "Epoch 955/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.5302e-07 - acc: 1.0000 - val_loss: 2.0561 - val_acc: 0.8253\n",
      "Epoch 956/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2726e-07 - acc: 1.0000 - val_loss: 2.0719 - val_acc: 0.8237\n",
      "Epoch 957/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1551e-07 - acc: 1.0000 - val_loss: 2.0702 - val_acc: 0.8253\n",
      "Epoch 958/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0671e-07 - acc: 1.0000 - val_loss: 2.0837 - val_acc: 0.8237\n",
      "Epoch 959/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.0541e-07 - acc: 1.0000 - val_loss: 2.0682 - val_acc: 0.8237\n",
      "Epoch 960/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1420e-07 - acc: 1.0000 - val_loss: 2.0812 - val_acc: 0.8237\n",
      "Epoch 961/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 3.7822e-07 - acc: 1.0000 - val_loss: 2.0301 - val_acc: 0.8285\n",
      "Epoch 962/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1066e-07 - acc: 1.0000 - val_loss: 2.0490 - val_acc: 0.8269\n",
      "Epoch 963/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.8253e-07 - acc: 1.0000 - val_loss: 2.0470 - val_acc: 0.8269\n",
      "Epoch 964/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.3536e-07 - acc: 1.0000 - val_loss: 2.0751 - val_acc: 0.8269\n",
      "Epoch 965/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5532e-07 - acc: 1.0000 - val_loss: 2.0793 - val_acc: 0.8269\n",
      "Epoch 966/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6750e-07 - acc: 1.0000 - val_loss: 2.0933 - val_acc: 0.8253\n",
      "Epoch 967/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.1718e-07 - acc: 1.0000 - val_loss: 2.1116 - val_acc: 0.8253\n",
      "Epoch 968/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4810e-07 - acc: 1.0000 - val_loss: 2.1233 - val_acc: 0.8253\n",
      "Epoch 969/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2740e-07 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.8269\n",
      "Epoch 970/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4637e-07 - acc: 1.0000 - val_loss: 2.1365 - val_acc: 0.8221\n",
      "Epoch 971/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6424e-07 - acc: 1.0000 - val_loss: 2.1156 - val_acc: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 972/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.2853e-07 - acc: 1.0000 - val_loss: 2.1255 - val_acc: 0.8253\n",
      "Epoch 973/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4459e-07 - acc: 1.0000 - val_loss: 2.1551 - val_acc: 0.8221\n",
      "Epoch 974/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 2.0380e-07 - acc: 1.0000 - val_loss: 2.1021 - val_acc: 0.8253\n",
      "Epoch 975/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5601e-07 - acc: 1.0000 - val_loss: 2.1466 - val_acc: 0.8253\n",
      "Epoch 976/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6608e-07 - acc: 1.0000 - val_loss: 2.1623 - val_acc: 0.8237\n",
      "Epoch 977/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4592e-07 - acc: 1.0000 - val_loss: 2.1477 - val_acc: 0.8253\n",
      "Epoch 978/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5719e-07 - acc: 1.0000 - val_loss: 2.1734 - val_acc: 0.8237\n",
      "Epoch 979/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4054e-07 - acc: 1.0000 - val_loss: 2.1774 - val_acc: 0.8237\n",
      "Epoch 980/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3192e-07 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.8237\n",
      "Epoch 981/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.4363e-07 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.8237\n",
      "Epoch 982/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3770e-07 - acc: 1.0000 - val_loss: 2.1535 - val_acc: 0.8253\n",
      "Epoch 983/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3823e-07 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.8253\n",
      "Epoch 984/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3674e-07 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.8237\n",
      "Epoch 985/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3199e-07 - acc: 1.0000 - val_loss: 2.1596 - val_acc: 0.8253\n",
      "Epoch 986/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6055e-07 - acc: 1.0000 - val_loss: 2.1949 - val_acc: 0.8221\n",
      "Epoch 987/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2643e-07 - acc: 1.0000 - val_loss: 2.1855 - val_acc: 0.8253\n",
      "Epoch 988/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.6104e-07 - acc: 1.0000 - val_loss: 2.0738 - val_acc: 0.8301\n",
      "Epoch 989/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.9975e-07 - acc: 1.0000 - val_loss: 2.1979 - val_acc: 0.8221\n",
      "Epoch 990/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3372e-07 - acc: 1.0000 - val_loss: 2.1340 - val_acc: 0.8285\n",
      "Epoch 991/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5255e-07 - acc: 1.0000 - val_loss: 2.1333 - val_acc: 0.8285\n",
      "Epoch 992/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3926e-07 - acc: 1.0000 - val_loss: 2.1573 - val_acc: 0.8285\n",
      "Epoch 993/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2263e-07 - acc: 1.0000 - val_loss: 2.1498 - val_acc: 0.8285\n",
      "Epoch 994/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2405e-07 - acc: 1.0000 - val_loss: 2.1696 - val_acc: 0.8269\n",
      "Epoch 995/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2261e-07 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.8253\n",
      "Epoch 996/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3043e-07 - acc: 1.0000 - val_loss: 2.1394 - val_acc: 0.8285\n",
      "Epoch 997/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.5066e-07 - acc: 1.0000 - val_loss: 2.2409 - val_acc: 0.8173\n",
      "Epoch 998/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2928e-07 - acc: 1.0000 - val_loss: 2.1676 - val_acc: 0.8285\n",
      "Epoch 999/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.2632e-07 - acc: 1.0000 - val_loss: 2.1771 - val_acc: 0.8269\n",
      "Epoch 1000/1000\n",
      "2682/2682 [==============================] - 35s 13ms/step - loss: 1.3986e-07 - acc: 1.0000 - val_loss: 2.1258 - val_acc: 0.8221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2efe37ed780>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incmodel.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.55      0.70       234\n",
      "          1       0.79      0.98      0.87       390\n",
      "\n",
      "avg / total       0.85      0.82      0.81       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ef73606f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inc_pred_base = incmodel.predict(X_test)\n",
    "inc_pred = [label_decoder(i) for i in inc_pred_base]\n",
    "inc_cm = confusion_matrix(y_test, inc_pred)\n",
    "sns.heatmap(inc_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, inc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "incmodel2 = keras.applications.InceptionV3(weights=None, classes=2, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "incmodel2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/10\n",
      "2682/2682 [==============================] - 85s 32ms/step - loss: 0.5104 - acc: 0.8367 - val_loss: 2.3805 - val_acc: 0.5849\n",
      "Epoch 2/10\n",
      "2682/2682 [==============================] - 52s 19ms/step - loss: 0.2522 - acc: 0.9053 - val_loss: 4.5370 - val_acc: 0.4119\n",
      "Epoch 3/10\n",
      "2682/2682 [==============================] - 52s 19ms/step - loss: 0.1599 - acc: 0.9433 - val_loss: 2.3138 - val_acc: 0.5449\n",
      "Epoch 4/10\n",
      "2682/2682 [==============================] - 52s 20ms/step - loss: 0.1630 - acc: 0.9471 - val_loss: 2.0750 - val_acc: 0.6522\n",
      "Epoch 5/10\n",
      "2682/2682 [==============================] - 53s 20ms/step - loss: 0.1476 - acc: 0.9441 - val_loss: 4.4335 - val_acc: 0.4744\n",
      "Epoch 6/10\n",
      "2682/2682 [==============================] - 52s 19ms/step - loss: 0.1255 - acc: 0.9489 - val_loss: 0.9174 - val_acc: 0.7019\n",
      "Epoch 7/10\n",
      "2682/2682 [==============================] - 51s 19ms/step - loss: 0.1338 - acc: 0.9497 - val_loss: 0.7906 - val_acc: 0.7388\n",
      "Epoch 8/10\n",
      "2682/2682 [==============================] - 52s 19ms/step - loss: 0.1685 - acc: 0.9418 - val_loss: 1.9533 - val_acc: 0.6346\n",
      "Epoch 9/10\n",
      "2682/2682 [==============================] - 52s 19ms/step - loss: 0.0896 - acc: 0.9679 - val_loss: 0.5209 - val_acc: 0.7885\n",
      "Epoch 10/10\n",
      "2682/2682 [==============================] - 51s 19ms/step - loss: 0.0871 - acc: 0.9698 - val_loss: 2.0747 - val_acc: 0.6282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2f7e8a46d30>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incmodel2.fit(X_train_array, y_train_array_k,\n",
    "          batch_size=8,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_k),\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define image generator with augmentation steps\n",
    "aug = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, \n",
    "                                                   width_shift_range=0.2, \n",
    "                                                   height_shift_range=0.2,\n",
    "                                                   rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fit the undersampled training images to the \n",
    "aug.fit(train_images_usample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.array(train_labels_usample))\n",
    "y_test = keras.utils.to_categorical(np.array(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = aug.flow(np.array(train_images_usample), batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aug = aug.flow(np.array(test_images), batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.ResNet50(include_top=True, weights=None, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.1177 - acc: 0.9522 - val_loss: 1.1688 - val_acc: 0.7083\n",
      "Epoch 2/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.1022 - acc: 0.9638 - val_loss: 0.9880 - val_acc: 0.7644\n",
      "Epoch 3/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0991 - acc: 0.9615 - val_loss: 0.7413 - val_acc: 0.7500\n",
      "Epoch 4/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.1020 - acc: 0.9648 - val_loss: 0.5168 - val_acc: 0.8125\n",
      "Epoch 5/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0936 - acc: 0.9627 - val_loss: 1.0816 - val_acc: 0.7580\n",
      "Epoch 6/500\n",
      "218/217 [==============================] - 80s 368ms/step - loss: 0.0828 - acc: 0.9688 - val_loss: 1.8436 - val_acc: 0.6202\n",
      "Epoch 7/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0941 - acc: 0.9644 - val_loss: 1.7380 - val_acc: 0.6699\n",
      "Epoch 8/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0887 - acc: 0.9637 - val_loss: 0.6960 - val_acc: 0.6490\n",
      "Epoch 9/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0772 - acc: 0.9734 - val_loss: 1.5253 - val_acc: 0.6667\n",
      "Epoch 10/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0825 - acc: 0.9703 - val_loss: 8.5433 - val_acc: 0.3910\n",
      "Epoch 11/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0858 - acc: 0.9681 - val_loss: 1.5427 - val_acc: 0.6843\n",
      "Epoch 12/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0642 - acc: 0.9755 - val_loss: 1.5902 - val_acc: 0.7115\n",
      "Epoch 13/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0728 - acc: 0.9713 - val_loss: 3.1031 - val_acc: 0.6346\n",
      "Epoch 14/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0630 - acc: 0.9751 - val_loss: 1.7684 - val_acc: 0.7067\n",
      "Epoch 15/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0674 - acc: 0.9740 - val_loss: 1.7974 - val_acc: 0.6587\n",
      "Epoch 16/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0800 - acc: 0.9713 - val_loss: 0.8078 - val_acc: 0.7356\n",
      "Epoch 17/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0748 - acc: 0.9708 - val_loss: 2.4252 - val_acc: 0.6314\n",
      "Epoch 18/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0684 - acc: 0.9746 - val_loss: 1.5048 - val_acc: 0.6731\n",
      "Epoch 19/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0566 - acc: 0.9790 - val_loss: 1.1709 - val_acc: 0.7484\n",
      "Epoch 20/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0588 - acc: 0.9787 - val_loss: 0.9919 - val_acc: 0.7260\n",
      "Epoch 21/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0621 - acc: 0.9780 - val_loss: 1.7025 - val_acc: 0.6779\n",
      "Epoch 22/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0666 - acc: 0.9768 - val_loss: 2.5034 - val_acc: 0.6282\n",
      "Epoch 23/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0554 - acc: 0.9787 - val_loss: 3.5283 - val_acc: 0.6298\n",
      "Epoch 24/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0517 - acc: 0.9808 - val_loss: 1.2954 - val_acc: 0.6955\n",
      "Epoch 25/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0563 - acc: 0.9781 - val_loss: 1.0564 - val_acc: 0.7051\n",
      "Epoch 26/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0546 - acc: 0.9808 - val_loss: 1.7222 - val_acc: 0.6987\n",
      "Epoch 27/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0525 - acc: 0.9808 - val_loss: 2.8118 - val_acc: 0.6266\n",
      "Epoch 28/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0419 - acc: 0.9832 - val_loss: 2.1956 - val_acc: 0.6907\n",
      "Epoch 29/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0511 - acc: 0.9815 - val_loss: 1.7898 - val_acc: 0.6635\n",
      "Epoch 30/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0361 - acc: 0.9862 - val_loss: 2.6713 - val_acc: 0.6426\n",
      "Epoch 31/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0427 - acc: 0.9846 - val_loss: 2.7814 - val_acc: 0.6506\n",
      "Epoch 32/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0437 - acc: 0.9832 - val_loss: 1.1401 - val_acc: 0.7003\n",
      "Epoch 33/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0388 - acc: 0.9857 - val_loss: 3.3360 - val_acc: 0.6506\n",
      "Epoch 34/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0406 - acc: 0.9839 - val_loss: 2.2153 - val_acc: 0.6554\n",
      "Epoch 35/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0303 - acc: 0.9881 - val_loss: 4.5330 - val_acc: 0.6330\n",
      "Epoch 36/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0404 - acc: 0.9864 - val_loss: 2.9666 - val_acc: 0.6490\n",
      "Epoch 37/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0316 - acc: 0.9883 - val_loss: 3.3299 - val_acc: 0.6426\n",
      "Epoch 38/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0334 - acc: 0.9880 - val_loss: 1.3313 - val_acc: 0.6971\n",
      "Epoch 39/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0320 - acc: 0.9899 - val_loss: 2.7318 - val_acc: 0.6410\n",
      "Epoch 40/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0392 - acc: 0.9860 - val_loss: 0.9768 - val_acc: 0.8093\n",
      "Epoch 41/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0329 - acc: 0.9872 - val_loss: 2.0529 - val_acc: 0.6587\n",
      "Epoch 42/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0256 - acc: 0.9893 - val_loss: 1.7502 - val_acc: 0.7163\n",
      "Epoch 43/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0278 - acc: 0.9889 - val_loss: 3.0579 - val_acc: 0.6603\n",
      "Epoch 44/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0291 - acc: 0.9891 - val_loss: 3.3517 - val_acc: 0.6442\n",
      "Epoch 45/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0241 - acc: 0.9904 - val_loss: 2.1657 - val_acc: 0.6715\n",
      "Epoch 46/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0270 - acc: 0.9901 - val_loss: 2.0654 - val_acc: 0.6651\n",
      "Epoch 47/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0289 - acc: 0.9899 - val_loss: 2.7750 - val_acc: 0.6522\n",
      "Epoch 48/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0262 - acc: 0.9903 - val_loss: 3.1372 - val_acc: 0.6458\n",
      "Epoch 49/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0207 - acc: 0.9910 - val_loss: 2.7676 - val_acc: 0.6266\n",
      "Epoch 50/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0246 - acc: 0.9903 - val_loss: 1.9604 - val_acc: 0.6907\n",
      "Epoch 51/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0225 - acc: 0.9899 - val_loss: 2.9215 - val_acc: 0.6923\n",
      "Epoch 52/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0191 - acc: 0.9931 - val_loss: 2.2097 - val_acc: 0.7067\n",
      "Epoch 53/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0225 - acc: 0.9914 - val_loss: 3.5784 - val_acc: 0.6490\n",
      "Epoch 54/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0240 - acc: 0.9920 - val_loss: 3.6612 - val_acc: 0.6442\n",
      "Epoch 55/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0252 - acc: 0.9918 - val_loss: 1.8364 - val_acc: 0.7324\n",
      "Epoch 56/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0258 - acc: 0.9897 - val_loss: 3.4863 - val_acc: 0.6490\n",
      "Epoch 57/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0224 - acc: 0.9918 - val_loss: 3.4654 - val_acc: 0.6458\n",
      "Epoch 58/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 3.7548 - val_acc: 0.6490\n",
      "Epoch 59/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0262 - acc: 0.9905 - val_loss: 1.7053 - val_acc: 0.7260\n",
      "Epoch 60/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0136 - acc: 0.9952 - val_loss: 3.4248 - val_acc: 0.6683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0252 - acc: 0.9901 - val_loss: 3.4178 - val_acc: 0.6490\n",
      "Epoch 62/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0126 - acc: 0.9956 - val_loss: 4.3809 - val_acc: 0.6426\n",
      "Epoch 63/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0124 - acc: 0.9957 - val_loss: 1.1311 - val_acc: 0.7163\n",
      "Epoch 64/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0290 - acc: 0.9895 - val_loss: 1.7143 - val_acc: 0.6955\n",
      "Epoch 65/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0093 - acc: 0.9971 - val_loss: 3.2179 - val_acc: 0.6635\n",
      "Epoch 66/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0200 - acc: 0.9937 - val_loss: 2.3764 - val_acc: 0.5577\n",
      "Epoch 67/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0137 - acc: 0.9952 - val_loss: 2.4014 - val_acc: 0.6651\n",
      "Epoch 68/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0154 - acc: 0.9943 - val_loss: 3.8708 - val_acc: 0.6506\n",
      "Epoch 69/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0085 - acc: 0.9973 - val_loss: 3.3628 - val_acc: 0.6651\n",
      "Epoch 70/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0210 - acc: 0.9929 - val_loss: 1.7505 - val_acc: 0.7484\n",
      "Epoch 71/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0246 - acc: 0.9924 - val_loss: 2.7523 - val_acc: 0.6538\n",
      "Epoch 72/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0159 - acc: 0.9948 - val_loss: 2.8569 - val_acc: 0.6587\n",
      "Epoch 73/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0072 - acc: 0.9973 - val_loss: 2.9509 - val_acc: 0.6779\n",
      "Epoch 74/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0181 - acc: 0.9937 - val_loss: 3.0551 - val_acc: 0.6490\n",
      "Epoch 75/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0122 - acc: 0.9959 - val_loss: 3.3620 - val_acc: 0.6667\n",
      "Epoch 76/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0158 - acc: 0.9935 - val_loss: 3.2738 - val_acc: 0.6587\n",
      "Epoch 77/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0087 - acc: 0.9969 - val_loss: 3.3809 - val_acc: 0.6667\n",
      "Epoch 78/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0143 - acc: 0.9943 - val_loss: 2.1568 - val_acc: 0.7179\n",
      "Epoch 79/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0164 - acc: 0.9945 - val_loss: 4.4183 - val_acc: 0.6442\n",
      "Epoch 80/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0176 - acc: 0.9935 - val_loss: 2.4472 - val_acc: 0.6795\n",
      "Epoch 81/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0088 - acc: 0.9964 - val_loss: 2.0678 - val_acc: 0.7244\n",
      "Epoch 82/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0191 - acc: 0.9926 - val_loss: 3.6677 - val_acc: 0.6907\n",
      "Epoch 83/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0119 - acc: 0.9953 - val_loss: 2.3837 - val_acc: 0.6955\n",
      "Epoch 84/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 3.4787 - val_acc: 0.6651\n",
      "Epoch 85/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0084 - acc: 0.9966 - val_loss: 3.9183 - val_acc: 0.6571\n",
      "Epoch 86/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 2.1830 - val_acc: 0.7244\n",
      "Epoch 87/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0128 - acc: 0.9941 - val_loss: 3.3592 - val_acc: 0.6619\n",
      "Epoch 88/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0092 - acc: 0.9983 - val_loss: 4.3853 - val_acc: 0.6490\n",
      "Epoch 89/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0155 - acc: 0.9943 - val_loss: 2.3524 - val_acc: 0.6923\n",
      "Epoch 90/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0138 - acc: 0.9948 - val_loss: 3.2640 - val_acc: 0.6763\n",
      "Epoch 91/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0073 - acc: 0.9981 - val_loss: 5.0644 - val_acc: 0.6378\n",
      "Epoch 92/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0083 - acc: 0.9968 - val_loss: 4.0831 - val_acc: 0.6571\n",
      "Epoch 93/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0091 - acc: 0.9971 - val_loss: 2.6254 - val_acc: 0.6827\n",
      "Epoch 94/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 2.9716 - val_acc: 0.6843\n",
      "Epoch 95/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 3.5133 - val_acc: 0.6875\n",
      "Epoch 96/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 3.7296 - val_acc: 0.6747\n",
      "Epoch 97/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0184 - acc: 0.9929 - val_loss: 3.3983 - val_acc: 0.6715\n",
      "Epoch 98/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0163 - acc: 0.9943 - val_loss: 4.0015 - val_acc: 0.6619\n",
      "Epoch 99/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 4.7527 - val_acc: 0.6458\n",
      "Epoch 100/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 3.9791 - val_acc: 0.6635\n",
      "Epoch 101/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0166 - acc: 0.9929 - val_loss: 2.7410 - val_acc: 0.6859\n",
      "Epoch 102/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0101 - acc: 0.9964 - val_loss: 1.5417 - val_acc: 0.7244\n",
      "Epoch 103/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0117 - acc: 0.9964 - val_loss: 2.0032 - val_acc: 0.7212\n",
      "Epoch 104/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0061 - acc: 0.9987 - val_loss: 2.5889 - val_acc: 0.6939\n",
      "Epoch 105/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 3.7153 - val_acc: 0.6715\n",
      "Epoch 106/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0232 - acc: 0.9922 - val_loss: 3.1509 - val_acc: 0.6522\n",
      "Epoch 107/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0046 - acc: 0.9979 - val_loss: 3.4606 - val_acc: 0.6619\n",
      "Epoch 108/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 3.3428 - val_acc: 0.6490\n",
      "Epoch 109/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0134 - acc: 0.9954 - val_loss: 3.1539 - val_acc: 0.6538\n",
      "Epoch 110/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 4.3406 - val_acc: 0.6506\n",
      "Epoch 111/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0125 - acc: 0.9956 - val_loss: 2.7676 - val_acc: 0.6891\n",
      "Epoch 112/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0091 - acc: 0.9968 - val_loss: 2.9254 - val_acc: 0.6859\n",
      "Epoch 113/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0090 - acc: 0.9964 - val_loss: 3.1604 - val_acc: 0.6699\n",
      "Epoch 114/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 3.8351 - val_acc: 0.6554\n",
      "Epoch 115/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0067 - acc: 0.9969 - val_loss: 3.9780 - val_acc: 0.6571\n",
      "Epoch 116/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0074 - acc: 0.9969 - val_loss: 3.4538 - val_acc: 0.6683\n",
      "Epoch 117/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 3.4074 - val_acc: 0.6891\n",
      "Epoch 118/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 3.1956 - val_acc: 0.7067\n",
      "Epoch 119/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 3.0485 - val_acc: 0.7099\n",
      "Epoch 120/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0103 - acc: 0.9966 - val_loss: 3.2091 - val_acc: 0.6891\n",
      "Epoch 121/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0128 - acc: 0.9956 - val_loss: 3.8158 - val_acc: 0.6667\n",
      "Epoch 122/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0179 - acc: 0.9935 - val_loss: 3.9231 - val_acc: 0.6538\n",
      "Epoch 123/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 4.0763 - val_acc: 0.6522\n",
      "Epoch 124/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 4.0953 - val_acc: 0.6587\n",
      "Epoch 125/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 4.1625 - val_acc: 0.6747\n",
      "Epoch 126/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0102 - acc: 0.9968 - val_loss: 3.4031 - val_acc: 0.6731\n",
      "Epoch 127/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 4.0836 - val_acc: 0.6474\n",
      "Epoch 128/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0115 - acc: 0.9962 - val_loss: 2.6584 - val_acc: 0.7244\n",
      "Epoch 129/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 3.5875 - val_acc: 0.6875\n",
      "Epoch 130/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 4.2703 - val_acc: 0.6587\n",
      "Epoch 131/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 1.9361 - val_acc: 0.7244\n",
      "Epoch 132/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0139 - acc: 0.9941 - val_loss: 3.7805 - val_acc: 0.6603\n",
      "Epoch 133/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 2.6033 - val_acc: 0.7147\n",
      "Epoch 134/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 4.0159 - val_acc: 0.6587\n",
      "Epoch 135/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 8.8450e-04 - acc: 0.9998 - val_loss: 3.5938 - val_acc: 0.6667\n",
      "Epoch 136/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 4.1147 - val_acc: 0.6522\n",
      "Epoch 137/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 5.1154 - val_acc: 0.6314\n",
      "Epoch 138/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0156 - acc: 0.9943 - val_loss: 3.0625 - val_acc: 0.6747\n",
      "Epoch 139/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0110 - acc: 0.9960 - val_loss: 2.3310 - val_acc: 0.7147\n",
      "Epoch 140/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 3.3547 - val_acc: 0.6587\n",
      "Epoch 141/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0066 - acc: 0.9979 - val_loss: 2.9295 - val_acc: 0.6587\n",
      "Epoch 142/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0138 - acc: 0.9946 - val_loss: 4.4004 - val_acc: 0.6394\n",
      "Epoch 143/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 3.6970 - val_acc: 0.6635\n",
      "Epoch 144/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0051 - acc: 0.9973 - val_loss: 4.0166 - val_acc: 0.6667\n",
      "Epoch 145/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0029 - acc: 0.9989 - val_loss: 3.8375 - val_acc: 0.6763\n",
      "Epoch 146/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 4.2984 - val_acc: 0.6490\n",
      "Epoch 147/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0068 - acc: 0.9973 - val_loss: 3.3071 - val_acc: 0.6827\n",
      "Epoch 148/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0086 - acc: 0.9968 - val_loss: 4.2629 - val_acc: 0.6651\n",
      "Epoch 149/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.0775 - val_acc: 0.6971\n",
      "Epoch 150/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0018 - acc: 0.9992 - val_loss: 3.6255 - val_acc: 0.6699\n",
      "Epoch 151/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0108 - acc: 0.9962 - val_loss: 3.3404 - val_acc: 0.6779\n",
      "Epoch 152/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 3.7230 - val_acc: 0.6811\n",
      "Epoch 153/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.2923 - val_acc: 0.6554\n",
      "Epoch 154/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 3.4438 - val_acc: 0.6827\n",
      "Epoch 155/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0160 - acc: 0.9943 - val_loss: 2.4360 - val_acc: 0.7324\n",
      "Epoch 156/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 3.5161 - val_acc: 0.6715\n",
      "Epoch 157/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 2.6589 - val_acc: 0.6891\n",
      "Epoch 158/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 3.6162 - val_acc: 0.6715\n",
      "Epoch 159/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 4.2301 - val_acc: 0.6603\n",
      "Epoch 160/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 2.8176 - val_acc: 0.7019\n",
      "Epoch 161/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 3.8897 - val_acc: 0.6651\n",
      "Epoch 162/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 6.3373e-04 - acc: 1.0000 - val_loss: 3.5979 - val_acc: 0.6747\n",
      "Epoch 163/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 2.8034e-04 - acc: 1.0000 - val_loss: 3.3490 - val_acc: 0.6875\n",
      "Epoch 164/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 4.4236 - val_acc: 0.6490\n",
      "Epoch 165/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0050 - acc: 0.9975 - val_loss: 3.3689 - val_acc: 0.6635\n",
      "Epoch 166/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 2.6246 - val_acc: 0.7099\n",
      "Epoch 167/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 2.2864 - val_acc: 0.7147\n",
      "Epoch 168/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0173 - acc: 0.9946 - val_loss: 1.9078 - val_acc: 0.7420\n",
      "Epoch 169/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 3.8483 - val_acc: 0.6554\n",
      "Epoch 170/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0077 - acc: 0.9965 - val_loss: 2.5856 - val_acc: 0.6987\n",
      "Epoch 171/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 3.3891 - val_acc: 0.6859\n",
      "Epoch 172/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.6499e-04 - acc: 1.0000 - val_loss: 3.4952 - val_acc: 0.6747\n",
      "Epoch 173/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.1534e-04 - acc: 1.0000 - val_loss: 3.6838 - val_acc: 0.6875\n",
      "Epoch 174/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 4.2998 - val_acc: 0.6651\n",
      "Epoch 175/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0137 - acc: 0.9946 - val_loss: 3.8330 - val_acc: 0.6747\n",
      "Epoch 176/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0089 - acc: 0.9975 - val_loss: 3.8817 - val_acc: 0.6587\n",
      "Epoch 177/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 3.7482 - val_acc: 0.6731\n",
      "Epoch 178/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 3.3804 - val_acc: 0.6699\n",
      "Epoch 179/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0058 - acc: 0.9978 - val_loss: 4.2558 - val_acc: 0.6715\n",
      "Epoch 180/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 4.3204 - val_acc: 0.6538\n",
      "Epoch 181/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 2.9662 - val_acc: 0.7131\n",
      "Epoch 182/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 2.8569 - val_acc: 0.6875\n",
      "Epoch 183/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0094 - acc: 0.9973 - val_loss: 2.3250 - val_acc: 0.7244\n",
      "Epoch 184/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0061 - acc: 0.9990 - val_loss: 3.7359 - val_acc: 0.6651\n",
      "Epoch 185/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 3.3469 - val_acc: 0.6811\n",
      "Epoch 186/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0025 - acc: 0.9990 - val_loss: 3.5107 - val_acc: 0.6779\n",
      "Epoch 187/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 7.3382e-04 - acc: 0.9998 - val_loss: 3.5628 - val_acc: 0.6795\n",
      "Epoch 188/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 7.5754e-04 - acc: 0.9998 - val_loss: 4.7880 - val_acc: 0.6426\n",
      "Epoch 189/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.4495e-04 - acc: 1.0000 - val_loss: 3.9968 - val_acc: 0.6715\n",
      "Epoch 190/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 4.8864 - val_acc: 0.6410\n",
      "Epoch 191/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0196 - acc: 0.9933 - val_loss: 2.6974 - val_acc: 0.6907\n",
      "Epoch 192/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 3.8001 - val_acc: 0.6603\n",
      "Epoch 193/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 3.7663 - val_acc: 0.6715\n",
      "Epoch 194/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.5402e-04 - acc: 1.0000 - val_loss: 3.5944 - val_acc: 0.6907\n",
      "Epoch 195/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0010 - acc: 0.9994 - val_loss: 4.1687 - val_acc: 0.6715\n",
      "Epoch 196/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 3.0563 - val_acc: 0.7147\n",
      "Epoch 197/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0094 - acc: 0.9964 - val_loss: 3.3947 - val_acc: 0.6875\n",
      "Epoch 198/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 2.6376 - val_acc: 0.7099\n",
      "Epoch 199/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 3.1378 - val_acc: 0.7019\n",
      "Epoch 200/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 4.3602 - val_acc: 0.6587\n",
      "Epoch 201/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.6754e-04 - acc: 0.9996 - val_loss: 4.5910 - val_acc: 0.6506\n",
      "Epoch 202/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 7.9876e-04 - acc: 0.9998 - val_loss: 3.9790 - val_acc: 0.6715\n",
      "Epoch 203/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0110 - acc: 0.9960 - val_loss: 3.1737 - val_acc: 0.6939\n",
      "Epoch 204/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 3.2914 - val_acc: 0.6875\n",
      "Epoch 205/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 2.1567 - val_acc: 0.7436\n",
      "Epoch 206/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 3.2153 - val_acc: 0.6811\n",
      "Epoch 207/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0060 - acc: 0.9979 - val_loss: 3.7217 - val_acc: 0.6538\n",
      "Epoch 208/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 3.4705 - val_acc: 0.6811\n",
      "Epoch 209/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 7.5304e-04 - acc: 0.9998 - val_loss: 3.3320 - val_acc: 0.6955\n",
      "Epoch 210/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0102 - acc: 0.9971 - val_loss: 3.6260 - val_acc: 0.6827\n",
      "Epoch 211/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 3.8267 - val_acc: 0.6683\n",
      "Epoch 212/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 3.1955 - val_acc: 0.6811\n",
      "Epoch 213/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 2.7656 - val_acc: 0.6923\n",
      "Epoch 214/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 3.7546 - val_acc: 0.6667\n",
      "Epoch 215/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 6.6122e-04 - acc: 0.9998 - val_loss: 3.8871 - val_acc: 0.6667\n",
      "Epoch 216/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0143 - acc: 0.9958 - val_loss: 3.7777 - val_acc: 0.6811\n",
      "Epoch 217/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0047 - acc: 0.9990 - val_loss: 2.5464 - val_acc: 0.7067\n",
      "Epoch 218/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0087 - acc: 0.9981 - val_loss: 2.3602 - val_acc: 0.6987\n",
      "Epoch 219/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 3.4694 - val_acc: 0.6683\n",
      "Epoch 220/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 2.9461 - val_acc: 0.6859\n",
      "Epoch 221/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 3.2405 - val_acc: 0.6811\n",
      "Epoch 222/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.7118e-04 - acc: 1.0000 - val_loss: 3.4542 - val_acc: 0.6747\n",
      "Epoch 223/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 3.0914 - val_acc: 0.6747\n",
      "Epoch 224/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0138 - acc: 0.9956 - val_loss: 1.4325 - val_acc: 0.7740\n",
      "Epoch 225/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0051 - acc: 0.9980 - val_loss: 2.9020 - val_acc: 0.6859\n",
      "Epoch 226/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 2.5314 - val_acc: 0.7292\n",
      "Epoch 227/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 2.7481 - val_acc: 0.7147\n",
      "Epoch 228/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0021 - acc: 0.9990 - val_loss: 3.4712 - val_acc: 0.6795\n",
      "Epoch 229/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 4.0306 - val_acc: 0.6635\n",
      "Epoch 230/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 3.2680 - val_acc: 0.7051\n",
      "Epoch 231/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 3.3316 - val_acc: 0.7099\n",
      "Epoch 232/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.3317 - val_acc: 0.6891\n",
      "Epoch 233/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 7.3745e-04 - acc: 0.9998 - val_loss: 3.8840 - val_acc: 0.6731\n",
      "Epoch 234/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 3.6190 - val_acc: 0.6811\n",
      "Epoch 235/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 4.0301 - val_acc: 0.6651\n",
      "Epoch 236/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 3.9539 - val_acc: 0.6923\n",
      "Epoch 237/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0095 - acc: 0.9971 - val_loss: 3.1045 - val_acc: 0.6875\n",
      "Epoch 238/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 3.5590 - val_acc: 0.6715\n",
      "Epoch 239/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0050 - acc: 0.9985 - val_loss: 2.4108 - val_acc: 0.7324\n",
      "Epoch 240/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 8.9063e-04 - acc: 1.0000 - val_loss: 3.4141 - val_acc: 0.6907\n",
      "Epoch 241/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 5.2230e-04 - acc: 0.9998 - val_loss: 3.9487 - val_acc: 0.6763\n",
      "Epoch 242/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0041 - acc: 0.9977 - val_loss: 4.0950 - val_acc: 0.6474\n",
      "Epoch 243/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.6478e-04 - acc: 1.0000 - val_loss: 4.0743 - val_acc: 0.6603\n",
      "Epoch 244/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.9065e-04 - acc: 1.0000 - val_loss: 3.9158 - val_acc: 0.6667\n",
      "Epoch 245/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.0848e-04 - acc: 1.0000 - val_loss: 4.2893 - val_acc: 0.6538\n",
      "Epoch 246/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.7372e-05 - acc: 1.0000 - val_loss: 4.2884 - val_acc: 0.6635\n",
      "Epoch 247/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0141 - acc: 0.9960 - val_loss: 4.1429 - val_acc: 0.6731\n",
      "Epoch 248/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0105 - acc: 0.9971 - val_loss: 2.9981 - val_acc: 0.6843\n",
      "Epoch 249/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 3.5465 - val_acc: 0.6715\n",
      "Epoch 250/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 2.5581e-04 - acc: 1.0000 - val_loss: 3.4945 - val_acc: 0.6779\n",
      "Epoch 251/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 6.8005e-04 - acc: 1.0000 - val_loss: 3.5016 - val_acc: 0.6747\n",
      "Epoch 252/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.6750e-04 - acc: 1.0000 - val_loss: 3.9739 - val_acc: 0.6603\n",
      "Epoch 253/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 1.6712 - val_acc: 0.7788\n",
      "Epoch 254/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0087 - acc: 0.9968 - val_loss: 3.7350 - val_acc: 0.6715\n",
      "Epoch 255/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0053 - acc: 0.9979 - val_loss: 3.3250 - val_acc: 0.7115\n",
      "Epoch 256/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 2.4653 - val_acc: 0.7147\n",
      "Epoch 257/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 3.8542 - val_acc: 0.6699\n",
      "Epoch 258/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 3.6972 - val_acc: 0.6522\n",
      "Epoch 259/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 4.1029 - val_acc: 0.6506\n",
      "Epoch 260/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 3.2259 - val_acc: 0.6907\n",
      "Epoch 261/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 3.6426 - val_acc: 0.6747\n",
      "Epoch 262/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 3.7741 - val_acc: 0.6891\n",
      "Epoch 263/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.4059 - val_acc: 0.6955\n",
      "Epoch 264/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.4924e-04 - acc: 1.0000 - val_loss: 3.2212 - val_acc: 0.7228\n",
      "Epoch 265/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.5517e-04 - acc: 1.0000 - val_loss: 3.2144 - val_acc: 0.7083\n",
      "Epoch 266/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.2350e-04 - acc: 1.0000 - val_loss: 3.5255 - val_acc: 0.6955\n",
      "Epoch 267/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.1091e-04 - acc: 1.0000 - val_loss: 4.1592 - val_acc: 0.6715\n",
      "Epoch 268/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 3.2181 - val_acc: 0.7212\n",
      "Epoch 269/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0125 - acc: 0.9964 - val_loss: 1.5019 - val_acc: 0.7228\n",
      "Epoch 270/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0055 - acc: 0.9979 - val_loss: 3.7176 - val_acc: 0.6571\n",
      "Epoch 271/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 3.3641 - val_acc: 0.6859\n",
      "Epoch 272/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 7.5267e-04 - acc: 0.9996 - val_loss: 3.3900 - val_acc: 0.6875\n",
      "Epoch 273/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 5.5672e-04 - acc: 0.9998 - val_loss: 3.5186 - val_acc: 0.6859\n",
      "Epoch 274/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.2582e-04 - acc: 1.0000 - val_loss: 3.9128 - val_acc: 0.6779\n",
      "Epoch 275/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.3144e-05 - acc: 1.0000 - val_loss: 4.1034 - val_acc: 0.6619\n",
      "Epoch 276/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0143 - acc: 0.9952 - val_loss: 3.0001 - val_acc: 0.6779\n",
      "Epoch 277/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0018 - acc: 0.9998 - val_loss: 3.5212 - val_acc: 0.6731\n",
      "Epoch 278/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.2721e-04 - acc: 1.0000 - val_loss: 3.1437 - val_acc: 0.7067\n",
      "Epoch 279/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 2.1880e-04 - acc: 1.0000 - val_loss: 3.6183 - val_acc: 0.6891\n",
      "Epoch 280/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 5.1499e-04 - acc: 0.9998 - val_loss: 4.4912 - val_acc: 0.6571\n",
      "Epoch 281/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.4975 - val_acc: 0.6667\n",
      "Epoch 282/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0019 - acc: 0.9992 - val_loss: 3.4822 - val_acc: 0.6891\n",
      "Epoch 283/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0037 - acc: 0.9990 - val_loss: 2.5136 - val_acc: 0.7244\n",
      "Epoch 284/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0040 - acc: 0.9985 - val_loss: 4.1895 - val_acc: 0.6587\n",
      "Epoch 285/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 4.2336 - val_acc: 0.6474\n",
      "Epoch 286/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0072 - acc: 0.9971 - val_loss: 3.0237 - val_acc: 0.6923\n",
      "Epoch 287/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.1730 - val_acc: 0.6955\n",
      "Epoch 288/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0042 - acc: 0.9979 - val_loss: 4.6288 - val_acc: 0.6554\n",
      "Epoch 289/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 3.9325 - val_acc: 0.6619\n",
      "Epoch 290/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0028 - acc: 0.9983 - val_loss: 3.2902 - val_acc: 0.6763\n",
      "Epoch 291/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 2.7983 - val_acc: 0.7115\n",
      "Epoch 292/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 3.1541 - val_acc: 0.6923\n",
      "Epoch 293/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 3.0208 - val_acc: 0.7115\n",
      "Epoch 294/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 2.8509 - val_acc: 0.7196\n",
      "Epoch 295/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.4979e-04 - acc: 1.0000 - val_loss: 3.5153 - val_acc: 0.6891\n",
      "Epoch 296/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0012 - acc: 0.9992 - val_loss: 4.5711 - val_acc: 0.6490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0234 - acc: 0.9935 - val_loss: 4.4021 - val_acc: 0.6522\n",
      "Epoch 298/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 3.6797 - val_acc: 0.6619\n",
      "Epoch 299/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 8.5279e-04 - acc: 0.9998 - val_loss: 3.6594 - val_acc: 0.6747\n",
      "Epoch 300/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 2.9613 - val_acc: 0.7003\n",
      "Epoch 301/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.6254e-04 - acc: 1.0000 - val_loss: 3.4765 - val_acc: 0.6795\n",
      "Epoch 302/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.1346e-04 - acc: 1.0000 - val_loss: 3.5819 - val_acc: 0.6763\n",
      "Epoch 303/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.1957e-04 - acc: 1.0000 - val_loss: 3.8309 - val_acc: 0.6619\n",
      "Epoch 304/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.4843e-04 - acc: 1.0000 - val_loss: 3.9600 - val_acc: 0.6603\n",
      "Epoch 305/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.5694e-04 - acc: 0.9998 - val_loss: 3.3379 - val_acc: 0.6987\n",
      "Epoch 306/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0073 - acc: 0.9969 - val_loss: 4.2538 - val_acc: 0.6458\n",
      "Epoch 307/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0029 - acc: 0.9989 - val_loss: 4.1668 - val_acc: 0.6490\n",
      "Epoch 308/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0015 - acc: 0.9990 - val_loss: 4.2723 - val_acc: 0.6458\n",
      "Epoch 309/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.3003e-04 - acc: 0.9998 - val_loss: 3.5740 - val_acc: 0.6843\n",
      "Epoch 310/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.9447e-04 - acc: 1.0000 - val_loss: 4.2297 - val_acc: 0.6763\n",
      "Epoch 311/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 5.2142 - val_acc: 0.6378\n",
      "Epoch 312/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0084 - acc: 0.9973 - val_loss: 2.7949 - val_acc: 0.6923\n",
      "Epoch 313/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 2.9622 - val_acc: 0.7035\n",
      "Epoch 314/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 4.3299 - val_acc: 0.6603\n",
      "Epoch 315/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 3.0605 - val_acc: 0.6875\n",
      "Epoch 316/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 6.4299e-04 - acc: 1.0000 - val_loss: 3.2240 - val_acc: 0.6939\n",
      "Epoch 317/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 2.7677 - val_acc: 0.7163\n",
      "Epoch 318/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 2.2338 - val_acc: 0.7212\n",
      "Epoch 319/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.3287 - val_acc: 0.6907\n",
      "Epoch 320/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 4.0381 - val_acc: 0.6747\n",
      "Epoch 321/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.5850e-04 - acc: 0.9998 - val_loss: 4.1282 - val_acc: 0.6635\n",
      "Epoch 322/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.0234e-04 - acc: 1.0000 - val_loss: 3.7300 - val_acc: 0.6811\n",
      "Epoch 323/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.9560e-05 - acc: 1.0000 - val_loss: 3.7744 - val_acc: 0.6843\n",
      "Epoch 324/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 3.9133e-05 - acc: 1.0000 - val_loss: 3.8302 - val_acc: 0.6779\n",
      "Epoch 325/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.7553e-05 - acc: 1.0000 - val_loss: 3.9757 - val_acc: 0.6699\n",
      "Epoch 326/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.7163e-05 - acc: 1.0000 - val_loss: 4.0736 - val_acc: 0.6779\n",
      "Epoch 327/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.1976e-04 - acc: 1.0000 - val_loss: 4.1590 - val_acc: 0.6683\n",
      "Epoch 328/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0062 - acc: 0.9990 - val_loss: 1.8623 - val_acc: 0.7853\n",
      "Epoch 329/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0154 - acc: 0.9946 - val_loss: 2.2140 - val_acc: 0.6859\n",
      "Epoch 330/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 3.1008 - val_acc: 0.6747\n",
      "Epoch 331/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 2.8273 - val_acc: 0.6939\n",
      "Epoch 332/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0064 - acc: 0.9990 - val_loss: 3.7002 - val_acc: 0.6699\n",
      "Epoch 333/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0074 - acc: 0.9969 - val_loss: 2.3421 - val_acc: 0.7292\n",
      "Epoch 334/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 3.6536 - val_acc: 0.6731\n",
      "Epoch 335/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 2.8911 - val_acc: 0.6923\n",
      "Epoch 336/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 3.6462 - val_acc: 0.6715\n",
      "Epoch 337/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 3.5145e-04 - acc: 1.0000 - val_loss: 3.2165 - val_acc: 0.6987\n",
      "Epoch 338/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.4211e-04 - acc: 1.0000 - val_loss: 3.4685 - val_acc: 0.6875\n",
      "Epoch 339/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 3.2373 - val_acc: 0.6763\n",
      "Epoch 340/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 2.1907 - val_acc: 0.7083\n",
      "Epoch 341/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0060 - acc: 0.9977 - val_loss: 2.9000 - val_acc: 0.7196\n",
      "Epoch 342/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 7.8384e-04 - acc: 0.9998 - val_loss: 2.9864 - val_acc: 0.6859\n",
      "Epoch 343/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.6511e-04 - acc: 0.9998 - val_loss: 3.4828 - val_acc: 0.6795\n",
      "Epoch 344/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.5227e-04 - acc: 1.0000 - val_loss: 3.3866 - val_acc: 0.6843\n",
      "Epoch 345/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.5746e-04 - acc: 0.9998 - val_loss: 3.1797 - val_acc: 0.7019\n",
      "Epoch 346/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.3452e-04 - acc: 1.0000 - val_loss: 3.5815 - val_acc: 0.6939\n",
      "Epoch 347/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 2.7632e-04 - acc: 0.9998 - val_loss: 3.4692 - val_acc: 0.6987\n",
      "Epoch 348/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0117 - acc: 0.9966 - val_loss: 3.4782 - val_acc: 0.6875\n",
      "Epoch 349/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0049 - acc: 0.9975 - val_loss: 2.9195 - val_acc: 0.7356\n",
      "Epoch 350/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.5463 - val_acc: 0.6923\n",
      "Epoch 351/500\n",
      "218/217 [==============================] - 80s 368ms/step - loss: 1.6936e-04 - acc: 1.0000 - val_loss: 3.5570 - val_acc: 0.6923\n",
      "Epoch 352/500\n",
      "218/217 [==============================] - 80s 367ms/step - loss: 6.2517e-04 - acc: 0.9998 - val_loss: 3.3806 - val_acc: 0.7035\n",
      "Epoch 353/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 6.2772e-04 - acc: 0.9996 - val_loss: 3.5010 - val_acc: 0.7019\n",
      "Epoch 354/500\n",
      "218/217 [==============================] - 80s 367ms/step - loss: 0.0035 - acc: 0.9985 - val_loss: 3.3881 - val_acc: 0.7147\n",
      "Epoch 355/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0079 - acc: 0.9981 - val_loss: 2.5087 - val_acc: 0.7212\n",
      "Epoch 356/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 2.7664 - val_acc: 0.6971\n",
      "Epoch 357/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 3.7117 - val_acc: 0.6811\n",
      "Epoch 358/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0015 - acc: 0.9994 - val_loss: 4.0297 - val_acc: 0.6747\n",
      "Epoch 359/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 1.5358e-04 - acc: 1.0000 - val_loss: 4.0550 - val_acc: 0.6763\n",
      "Epoch 360/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 7.5883e-04 - acc: 0.9996 - val_loss: 3.7754 - val_acc: 0.6843\n",
      "Epoch 361/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 4.2594 - val_acc: 0.6651\n",
      "Epoch 362/500\n",
      "218/217 [==============================] - 80s 366ms/step - loss: 6.5495e-04 - acc: 1.0000 - val_loss: 3.4761 - val_acc: 0.7035\n",
      "Epoch 363/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 3.0295 - val_acc: 0.7212\n",
      "Epoch 364/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 3.2463 - val_acc: 0.7019\n",
      "Epoch 365/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0131 - acc: 0.9962 - val_loss: 2.6610 - val_acc: 0.7163\n",
      "Epoch 366/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0017 - acc: 0.9998 - val_loss: 3.3391 - val_acc: 0.6987\n",
      "Epoch 367/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 8.9951e-04 - acc: 0.9996 - val_loss: 3.8146 - val_acc: 0.6683\n",
      "Epoch 368/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 6.4824e-04 - acc: 0.9998 - val_loss: 3.6089 - val_acc: 0.6843\n",
      "Epoch 369/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.3959e-04 - acc: 1.0000 - val_loss: 4.2596 - val_acc: 0.6603\n",
      "Epoch 370/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.4461e-04 - acc: 1.0000 - val_loss: 4.2327 - val_acc: 0.6699\n",
      "Epoch 371/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 8.4656e-04 - acc: 0.9996 - val_loss: 4.4778 - val_acc: 0.6571\n",
      "Epoch 372/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 9.0062e-04 - acc: 0.9996 - val_loss: 4.1529 - val_acc: 0.6811\n",
      "Epoch 373/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0057 - acc: 0.9989 - val_loss: 2.7489 - val_acc: 0.7260\n",
      "Epoch 374/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 2.7370 - val_acc: 0.7099\n",
      "Epoch 375/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 2.8337 - val_acc: 0.7099\n",
      "Epoch 376/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.3899e-04 - acc: 1.0000 - val_loss: 3.1655 - val_acc: 0.6955\n",
      "Epoch 377/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0013 - acc: 0.9992 - val_loss: 3.3459 - val_acc: 0.6875\n",
      "Epoch 378/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 3.7463 - val_acc: 0.6763\n",
      "Epoch 379/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 2.0620e-04 - acc: 1.0000 - val_loss: 3.4202 - val_acc: 0.6987\n",
      "Epoch 380/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0042 - acc: 0.9992 - val_loss: 2.6960 - val_acc: 0.7228\n",
      "Epoch 381/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 3.0760 - val_acc: 0.6955\n",
      "Epoch 382/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0020 - acc: 0.9990 - val_loss: 3.5766 - val_acc: 0.6635\n",
      "Epoch 383/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.4681e-04 - acc: 1.0000 - val_loss: 3.7473 - val_acc: 0.6811\n",
      "Epoch 384/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.9658e-04 - acc: 1.0000 - val_loss: 4.0669 - val_acc: 0.6699\n",
      "Epoch 385/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 2.7231 - val_acc: 0.7372\n",
      "Epoch 386/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 3.5206 - val_acc: 0.7163\n",
      "Epoch 387/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 6.3363e-04 - acc: 1.0000 - val_loss: 3.2746 - val_acc: 0.7179\n",
      "Epoch 388/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.0933e-04 - acc: 1.0000 - val_loss: 3.6300 - val_acc: 0.7115\n",
      "Epoch 389/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 4.9491e-05 - acc: 1.0000 - val_loss: 3.4939 - val_acc: 0.7099\n",
      "Epoch 390/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 8.7644e-05 - acc: 1.0000 - val_loss: 3.5131 - val_acc: 0.7035\n",
      "Epoch 391/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.4476e-04 - acc: 1.0000 - val_loss: 3.6588 - val_acc: 0.7115\n",
      "Epoch 392/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.9170e-05 - acc: 1.0000 - val_loss: 3.9880 - val_acc: 0.6923\n",
      "Epoch 393/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 3.8144 - val_acc: 0.6651\n",
      "Epoch 394/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0098 - acc: 0.9966 - val_loss: 3.5043 - val_acc: 0.6827\n",
      "Epoch 395/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0053 - acc: 0.9989 - val_loss: 3.0186 - val_acc: 0.7003\n",
      "Epoch 396/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 2.8358 - val_acc: 0.7228\n",
      "Epoch 397/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 6.6202e-04 - acc: 1.0000 - val_loss: 3.0523 - val_acc: 0.7099\n",
      "Epoch 398/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 2.6324 - val_acc: 0.7372\n",
      "Epoch 399/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 7.8586e-04 - acc: 0.9998 - val_loss: 3.4816 - val_acc: 0.6939\n",
      "Epoch 400/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.6087e-05 - acc: 1.0000 - val_loss: 3.2921 - val_acc: 0.7067\n",
      "Epoch 401/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 6.1278e-04 - acc: 0.9998 - val_loss: 3.5177 - val_acc: 0.6939\n",
      "Epoch 402/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.4569e-04 - acc: 1.0000 - val_loss: 3.3622 - val_acc: 0.7083\n",
      "Epoch 403/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 1.3913 - val_acc: 0.7708\n",
      "Epoch 404/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 2.7629 - val_acc: 0.6859\n",
      "Epoch 405/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 3.1962 - val_acc: 0.6939\n",
      "Epoch 406/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.2033e-04 - acc: 0.9994 - val_loss: 3.5053 - val_acc: 0.6763\n",
      "Epoch 407/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 3.3009 - val_acc: 0.6747\n",
      "Epoch 408/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.8731e-04 - acc: 0.9996 - val_loss: 2.9618 - val_acc: 0.7115\n",
      "Epoch 409/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.5260e-04 - acc: 1.0000 - val_loss: 3.5704 - val_acc: 0.6907\n",
      "Epoch 410/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 3.6704 - val_acc: 0.6827\n",
      "Epoch 411/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 4.6073 - val_acc: 0.6426\n",
      "Epoch 412/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 3.4089 - val_acc: 0.6843\n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/217 [==============================] - 80s 365ms/step - loss: 4.2076e-04 - acc: 0.9998 - val_loss: 3.5786 - val_acc: 0.6875\n",
      "Epoch 414/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 3.3423 - val_acc: 0.7003\n",
      "Epoch 415/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0021 - acc: 0.9990 - val_loss: 4.2544 - val_acc: 0.6635\n",
      "Epoch 416/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.0691e-04 - acc: 1.0000 - val_loss: 3.9801 - val_acc: 0.6667\n",
      "Epoch 417/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.9388e-04 - acc: 1.0000 - val_loss: 3.8915 - val_acc: 0.6651\n",
      "Epoch 418/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.1398e-04 - acc: 1.0000 - val_loss: 4.1665 - val_acc: 0.6747\n",
      "Epoch 419/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.3448e-04 - acc: 1.0000 - val_loss: 4.4731 - val_acc: 0.6603\n",
      "Epoch 420/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.4449e-04 - acc: 1.0000 - val_loss: 4.0530 - val_acc: 0.6635\n",
      "Epoch 421/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0055 - acc: 0.9990 - val_loss: 3.9926 - val_acc: 0.6458\n",
      "Epoch 422/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0067 - acc: 0.9981 - val_loss: 4.0198 - val_acc: 0.6699\n",
      "Epoch 423/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 6.2870e-04 - acc: 0.9998 - val_loss: 3.6652 - val_acc: 0.6763\n",
      "Epoch 424/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.7667e-04 - acc: 1.0000 - val_loss: 3.7323 - val_acc: 0.6843\n",
      "Epoch 425/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 4.9656 - val_acc: 0.6442\n",
      "Epoch 426/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 8.8932e-04 - acc: 0.9994 - val_loss: 3.2635 - val_acc: 0.7003\n",
      "Epoch 427/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0059 - acc: 0.9973 - val_loss: 3.4702 - val_acc: 0.7035\n",
      "Epoch 428/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 2.2234 - val_acc: 0.7404\n",
      "Epoch 429/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.5853e-04 - acc: 1.0000 - val_loss: 3.4362 - val_acc: 0.6875\n",
      "Epoch 430/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 6.7719e-04 - acc: 0.9994 - val_loss: 2.3200 - val_acc: 0.7452\n",
      "Epoch 431/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 3.9264 - val_acc: 0.6587\n",
      "Epoch 432/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.1582e-04 - acc: 0.9998 - val_loss: 3.6516 - val_acc: 0.6987\n",
      "Epoch 433/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0072 - acc: 0.9981 - val_loss: 4.6788 - val_acc: 0.6474\n",
      "Epoch 434/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 3.4828 - val_acc: 0.6747\n",
      "Epoch 435/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 8.8918e-04 - acc: 1.0000 - val_loss: 3.6384 - val_acc: 0.6667\n",
      "Epoch 436/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 2.5257e-04 - acc: 1.0000 - val_loss: 3.4358 - val_acc: 0.6747\n",
      "Epoch 437/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0056 - acc: 0.9989 - val_loss: 4.1265 - val_acc: 0.6603\n",
      "Epoch 438/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.7246e-04 - acc: 1.0000 - val_loss: 3.9141 - val_acc: 0.6747\n",
      "Epoch 439/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 9.7302e-05 - acc: 1.0000 - val_loss: 3.8279 - val_acc: 0.6779\n",
      "Epoch 440/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 4.2272e-05 - acc: 1.0000 - val_loss: 3.9788 - val_acc: 0.6747\n",
      "Epoch 441/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 3.7131 - val_acc: 0.7099\n",
      "Epoch 442/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.6210e-04 - acc: 1.0000 - val_loss: 3.4828 - val_acc: 0.7099\n",
      "Epoch 443/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 3.5498 - val_acc: 0.6907\n",
      "Epoch 444/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 2.3739 - val_acc: 0.7308\n",
      "Epoch 445/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 6.2705e-04 - acc: 1.0000 - val_loss: 3.4378 - val_acc: 0.6843\n",
      "Epoch 446/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.1842e-04 - acc: 1.0000 - val_loss: 3.4102 - val_acc: 0.6955\n",
      "Epoch 447/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.2892e-04 - acc: 1.0000 - val_loss: 3.7541 - val_acc: 0.6779\n",
      "Epoch 448/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.7882e-05 - acc: 1.0000 - val_loss: 3.5866 - val_acc: 0.6923\n",
      "Epoch 449/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.4545e-05 - acc: 1.0000 - val_loss: 3.5817 - val_acc: 0.6923\n",
      "Epoch 450/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.5576e-04 - acc: 0.9998 - val_loss: 4.2894 - val_acc: 0.6587\n",
      "Epoch 451/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 4.0011 - val_acc: 0.7019\n",
      "Epoch 452/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0093 - acc: 0.9973 - val_loss: 4.4776 - val_acc: 0.6554\n",
      "Epoch 453/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0055 - acc: 0.9978 - val_loss: 2.9953 - val_acc: 0.6891\n",
      "Epoch 454/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 0.0047 - acc: 0.9979 - val_loss: 3.3320 - val_acc: 0.6795\n",
      "Epoch 455/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.7393e-04 - acc: 1.0000 - val_loss: 2.8703 - val_acc: 0.7003\n",
      "Epoch 456/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 4.1477e-04 - acc: 1.0000 - val_loss: 2.7654 - val_acc: 0.6875\n",
      "Epoch 457/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 9.2852e-05 - acc: 1.0000 - val_loss: 3.2318 - val_acc: 0.6747\n",
      "Epoch 458/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.1076e-04 - acc: 1.0000 - val_loss: 3.2538 - val_acc: 0.6891\n",
      "Epoch 459/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.1755e-04 - acc: 1.0000 - val_loss: 3.3374 - val_acc: 0.6731\n",
      "Epoch 460/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 8.8153e-05 - acc: 1.0000 - val_loss: 3.6625 - val_acc: 0.6699\n",
      "Epoch 461/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.2846e-04 - acc: 1.0000 - val_loss: 3.8994 - val_acc: 0.6635\n",
      "Epoch 462/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.1684e-04 - acc: 1.0000 - val_loss: 3.7811 - val_acc: 0.6635\n",
      "Epoch 463/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 2.1035e-04 - acc: 1.0000 - val_loss: 4.1252 - val_acc: 0.6587\n",
      "Epoch 464/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 3.0647e-05 - acc: 1.0000 - val_loss: 4.1306 - val_acc: 0.6571\n",
      "Epoch 465/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 1.5342e-05 - acc: 1.0000 - val_loss: 4.1086 - val_acc: 0.6651\n",
      "Epoch 466/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.2519e-05 - acc: 1.0000 - val_loss: 4.2152 - val_acc: 0.6587\n",
      "Epoch 467/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0068 - acc: 0.9979 - val_loss: 3.4668 - val_acc: 0.6699\n",
      "Epoch 468/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 4.0952 - val_acc: 0.6538\n",
      "Epoch 469/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 3.4880 - val_acc: 0.6795\n",
      "Epoch 470/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.0128e-04 - acc: 0.9998 - val_loss: 4.0134 - val_acc: 0.6651\n",
      "Epoch 471/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.5183e-04 - acc: 1.0000 - val_loss: 3.9212 - val_acc: 0.6603\n",
      "Epoch 472/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 3.5848 - val_acc: 0.6811\n",
      "Epoch 473/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 3.7227 - val_acc: 0.6747\n",
      "Epoch 474/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.4426e-04 - acc: 1.0000 - val_loss: 4.5758 - val_acc: 0.6474\n",
      "Epoch 475/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 1.5674e-04 - acc: 1.0000 - val_loss: 3.9831 - val_acc: 0.6731\n",
      "Epoch 476/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 3.5768 - val_acc: 0.6779\n",
      "Epoch 477/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 3.0299 - val_acc: 0.7179\n",
      "Epoch 478/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 3.0143 - val_acc: 0.6971\n",
      "Epoch 479/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 3.0062 - val_acc: 0.7003\n",
      "Epoch 480/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 4.0449 - val_acc: 0.6635\n",
      "Epoch 481/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.6463e-04 - acc: 0.9998 - val_loss: 3.8553 - val_acc: 0.6667\n",
      "Epoch 482/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 3.4146e-04 - acc: 1.0000 - val_loss: 4.0605 - val_acc: 0.6619\n",
      "Epoch 483/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 4.0465 - val_acc: 0.6587\n",
      "Epoch 484/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 4.3565 - val_acc: 0.6506\n",
      "Epoch 485/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 9.9082e-05 - acc: 1.0000 - val_loss: 4.5258 - val_acc: 0.6571\n",
      "Epoch 486/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 5.2467e-05 - acc: 1.0000 - val_loss: 4.2711 - val_acc: 0.6603\n",
      "Epoch 487/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 3.5890e-04 - acc: 0.9998 - val_loss: 4.5492 - val_acc: 0.6538\n",
      "Epoch 488/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 0.0035 - acc: 0.9987 - val_loss: 1.7629 - val_acc: 0.7804\n",
      "Epoch 489/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0060 - acc: 0.9990 - val_loss: 3.7607 - val_acc: 0.6859\n",
      "Epoch 490/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 2.8057 - val_acc: 0.7244\n",
      "Epoch 491/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 3.5829 - val_acc: 0.6795\n",
      "Epoch 492/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 2.0118e-04 - acc: 1.0000 - val_loss: 3.6848 - val_acc: 0.6779\n",
      "Epoch 493/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 3.4605e-04 - acc: 0.9998 - val_loss: 3.3729 - val_acc: 0.7067\n",
      "Epoch 494/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0010 - acc: 0.9994 - val_loss: 3.1398 - val_acc: 0.7003\n",
      "Epoch 495/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 7.5136e-04 - acc: 0.9996 - val_loss: 3.5749 - val_acc: 0.6747\n",
      "Epoch 496/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 3.2680 - val_acc: 0.6651\n",
      "Epoch 497/500\n",
      "218/217 [==============================] - 79s 364ms/step - loss: 0.0019 - acc: 0.9996 - val_loss: 3.3879 - val_acc: 0.6715\n",
      "Epoch 498/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 7.1767e-04 - acc: 0.9996 - val_loss: 3.0123 - val_acc: 0.6907\n",
      "Epoch 499/500\n",
      "218/217 [==============================] - 79s 365ms/step - loss: 1.7653e-04 - acc: 1.0000 - val_loss: 3.2417 - val_acc: 0.6923\n",
      "Epoch 500/500\n",
      "218/217 [==============================] - 80s 365ms/step - loss: 5.5054e-04 - acc: 0.9996 - val_loss: 2.6672 - val_acc: 0.7019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1c2d9fd8160>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_aug, steps_per_epoch=len(np.array(train_images)) / 24, epochs=500, \n",
    "                    validation_data=test_aug, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving using save_model was taking too long, so I'm saving weights and model architecture separately\n",
    "model.save_weights('res_aug_500.h5')\n",
    "model_json = model.to_json()\n",
    "with open('res_aug_500.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.22      0.36       234\n",
      "          1       0.68      1.00      0.81       390\n",
      "\n",
      "avg / total       0.80      0.71      0.64       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c34405dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resaug_pred_base = model.predict(X_test)\n",
    "resaug_pred = [label_decoder(i) for i in resaug_pred_base]\n",
    "resaug_cm = confusion_matrix(np.array(test_labels), resaug_pred)\n",
    "sns.heatmap(resaug_cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(np.array(test_labels), resaug_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained weights for resnet50 trained on ImageNet\n",
    "weight_path = 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "base_resnet = keras.applications.ResNet50(include_top=False, weights=weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features = base_resnet.predict(X_train_array)\n",
    "train_features_r = np.reshape(train_features, (-1, 1, 1, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = base_resnet.predict(X_test)\n",
    "test_features = np.reshape(test_features, (-1, 1, 1, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 1, 1, 2048)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation='relu', input_dim=1*1*2048))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying some new optimizer and loss variables\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.1190 - acc: 0.9582 - val_loss: 1.3671 - val_acc: 0.7155\n",
      "Epoch 2/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.1333 - acc: 0.9551 - val_loss: 1.5644 - val_acc: 0.6771\n",
      "Epoch 3/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.1131 - acc: 0.9592 - val_loss: 1.5553 - val_acc: 0.7155\n",
      "Epoch 4/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.1193 - acc: 0.9581 - val_loss: 2.0014 - val_acc: 0.6619\n",
      "Epoch 5/1000\n",
      "2682/2682 [==============================] - 1s 298us/step - loss: 0.1083 - acc: 0.9622 - val_loss: 0.9959 - val_acc: 0.7772\n",
      "Epoch 6/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.1176 - acc: 0.9562 - val_loss: 1.3946 - val_acc: 0.7212\n",
      "Epoch 7/1000\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 0.1134 - acc: 0.9625 - val_loss: 1.2576 - val_acc: 0.7532\n",
      "Epoch 8/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.1074 - acc: 0.9607 - val_loss: 1.0008 - val_acc: 0.7837\n",
      "Epoch 9/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.8690 - val_acc: 0.7837\n",
      "Epoch 10/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.1068 - acc: 0.9646 - val_loss: 1.8958 - val_acc: 0.7139\n",
      "Epoch 11/1000\n",
      "2682/2682 [==============================] - 1s 296us/step - loss: 0.1048 - acc: 0.9655 - val_loss: 1.5188 - val_acc: 0.7083\n",
      "Epoch 12/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.1095 - acc: 0.9610 - val_loss: 2.0786 - val_acc: 0.6723\n",
      "Epoch 13/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.1044 - acc: 0.9661 - val_loss: 1.5802 - val_acc: 0.7252\n",
      "Epoch 14/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0978 - acc: 0.9651 - val_loss: 1.5844 - val_acc: 0.7292\n",
      "Epoch 15/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.1055 - acc: 0.9653 - val_loss: 2.7145 - val_acc: 0.6627\n",
      "Epoch 16/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.1007 - acc: 0.9670 - val_loss: 2.3189 - val_acc: 0.6787\n",
      "Epoch 17/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0945 - acc: 0.9681 - val_loss: 2.0801 - val_acc: 0.6907\n",
      "Epoch 18/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0866 - acc: 0.9711 - val_loss: 1.5597 - val_acc: 0.7628\n",
      "Epoch 19/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0955 - acc: 0.9642 - val_loss: 3.2674 - val_acc: 0.6514\n",
      "Epoch 20/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.1014 - acc: 0.9640 - val_loss: 2.0360 - val_acc: 0.6963\n",
      "Epoch 21/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0851 - acc: 0.9717 - val_loss: 1.7758 - val_acc: 0.7356\n",
      "Epoch 22/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0956 - acc: 0.9629 - val_loss: 2.7007 - val_acc: 0.6627\n",
      "Epoch 23/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0968 - acc: 0.9623 - val_loss: 1.7871 - val_acc: 0.7220\n",
      "Epoch 24/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0911 - acc: 0.9670 - val_loss: 1.9984 - val_acc: 0.7027\n",
      "Epoch 25/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0948 - acc: 0.9676 - val_loss: 1.9214 - val_acc: 0.7147\n",
      "Epoch 26/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0966 - acc: 0.9715 - val_loss: 1.6398 - val_acc: 0.7340\n",
      "Epoch 27/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0933 - acc: 0.9696 - val_loss: 1.9936 - val_acc: 0.7252\n",
      "Epoch 28/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0836 - acc: 0.9717 - val_loss: 2.0410 - val_acc: 0.7308\n",
      "Epoch 29/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0939 - acc: 0.9691 - val_loss: 2.1292 - val_acc: 0.6987\n",
      "Epoch 30/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0781 - acc: 0.9761 - val_loss: 1.3355 - val_acc: 0.7869\n",
      "Epoch 31/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0945 - acc: 0.9644 - val_loss: 2.3088 - val_acc: 0.6891\n",
      "Epoch 32/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0889 - acc: 0.9724 - val_loss: 1.7052 - val_acc: 0.7756\n",
      "Epoch 33/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0848 - acc: 0.9711 - val_loss: 1.8181 - val_acc: 0.7460\n",
      "Epoch 34/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0820 - acc: 0.9711 - val_loss: 4.2591 - val_acc: 0.6298\n",
      "Epoch 35/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0864 - acc: 0.9711 - val_loss: 2.5555 - val_acc: 0.6627\n",
      "Epoch 36/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0803 - acc: 0.9724 - val_loss: 2.5638 - val_acc: 0.6707\n",
      "Epoch 37/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0841 - acc: 0.9752 - val_loss: 1.2543 - val_acc: 0.7893\n",
      "Epoch 38/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0810 - acc: 0.9761 - val_loss: 1.9711 - val_acc: 0.7356\n",
      "Epoch 39/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0758 - acc: 0.9767 - val_loss: 1.7327 - val_acc: 0.7620\n",
      "Epoch 40/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0822 - acc: 0.9726 - val_loss: 1.8240 - val_acc: 0.7468\n",
      "Epoch 41/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0733 - acc: 0.9760 - val_loss: 2.2637 - val_acc: 0.7035\n",
      "Epoch 42/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0760 - acc: 0.9763 - val_loss: 1.5733 - val_acc: 0.7660\n",
      "Epoch 43/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0807 - acc: 0.9739 - val_loss: 1.8469 - val_acc: 0.7484\n",
      "Epoch 44/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0798 - acc: 0.9707 - val_loss: 2.4578 - val_acc: 0.6939\n",
      "Epoch 45/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0794 - acc: 0.9739 - val_loss: 1.4108 - val_acc: 0.7796\n",
      "Epoch 46/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0616 - acc: 0.9773 - val_loss: 1.8494 - val_acc: 0.7516\n",
      "Epoch 47/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0738 - acc: 0.9776 - val_loss: 3.1726 - val_acc: 0.6667\n",
      "Epoch 48/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0716 - acc: 0.9761 - val_loss: 2.4102 - val_acc: 0.6955\n",
      "Epoch 49/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0693 - acc: 0.9758 - val_loss: 1.9420 - val_acc: 0.7444\n",
      "Epoch 50/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0749 - acc: 0.9746 - val_loss: 2.5771 - val_acc: 0.7099\n",
      "Epoch 51/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0878 - acc: 0.9720 - val_loss: 2.2389 - val_acc: 0.7228\n",
      "Epoch 52/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0679 - acc: 0.9769 - val_loss: 1.9573 - val_acc: 0.7492\n",
      "Epoch 53/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0703 - acc: 0.9741 - val_loss: 2.3614 - val_acc: 0.7284\n",
      "Epoch 54/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0827 - acc: 0.9743 - val_loss: 1.6950 - val_acc: 0.7652\n",
      "Epoch 55/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0733 - acc: 0.9741 - val_loss: 2.5773 - val_acc: 0.6779\n",
      "Epoch 56/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0695 - acc: 0.9767 - val_loss: 2.5807 - val_acc: 0.7019\n",
      "Epoch 57/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0691 - acc: 0.9789 - val_loss: 2.9392 - val_acc: 0.6843\n",
      "Epoch 58/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0673 - acc: 0.9767 - val_loss: 2.3491 - val_acc: 0.7292\n",
      "Epoch 59/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0725 - acc: 0.9791 - val_loss: 2.4509 - val_acc: 0.7308\n",
      "Epoch 60/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0801 - acc: 0.9752 - val_loss: 2.9567 - val_acc: 0.6667\n",
      "Epoch 61/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0572 - acc: 0.9814 - val_loss: 1.8174 - val_acc: 0.7740\n",
      "Epoch 62/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0659 - acc: 0.9776 - val_loss: 2.3263 - val_acc: 0.7260\n",
      "Epoch 63/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0857 - acc: 0.9735 - val_loss: 3.2051 - val_acc: 0.6763\n",
      "Epoch 64/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0695 - acc: 0.9776 - val_loss: 3.0957 - val_acc: 0.6859\n",
      "Epoch 65/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0607 - acc: 0.9799 - val_loss: 2.5316 - val_acc: 0.7179\n",
      "Epoch 66/1000\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0655 - acc: 0.9799 - val_loss: 2.0551 - val_acc: 0.7436\n",
      "Epoch 67/1000\n",
      "2682/2682 [==============================] - 1s 298us/step - loss: 0.0695 - acc: 0.9776 - val_loss: 2.3493 - val_acc: 0.7348\n",
      "Epoch 68/1000\n",
      "2682/2682 [==============================] - 1s 301us/step - loss: 0.0656 - acc: 0.9812 - val_loss: 2.6174 - val_acc: 0.7308\n",
      "Epoch 69/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0707 - acc: 0.9765 - val_loss: 2.3108 - val_acc: 0.7308\n",
      "Epoch 70/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0653 - acc: 0.9801 - val_loss: 3.0117 - val_acc: 0.6875\n",
      "Epoch 71/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0667 - acc: 0.9825 - val_loss: 2.3916 - val_acc: 0.7356\n",
      "Epoch 72/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0755 - acc: 0.9776 - val_loss: 2.6542 - val_acc: 0.7196\n",
      "Epoch 73/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0684 - acc: 0.9773 - val_loss: 2.0482 - val_acc: 0.7564\n",
      "Epoch 74/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0651 - acc: 0.9786 - val_loss: 2.0607 - val_acc: 0.7580\n",
      "Epoch 75/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0720 - acc: 0.9771 - val_loss: 2.5638 - val_acc: 0.7260\n",
      "Epoch 76/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0684 - acc: 0.9782 - val_loss: 1.9603 - val_acc: 0.7596\n",
      "Epoch 77/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0624 - acc: 0.9795 - val_loss: 2.2846 - val_acc: 0.7492\n",
      "Epoch 78/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0683 - acc: 0.9776 - val_loss: 2.8808 - val_acc: 0.7067\n",
      "Epoch 79/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0636 - acc: 0.9787 - val_loss: 2.3966 - val_acc: 0.7308\n",
      "Epoch 80/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0705 - acc: 0.9776 - val_loss: 2.7869 - val_acc: 0.7067\n",
      "Epoch 81/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0503 - acc: 0.9814 - val_loss: 3.4877 - val_acc: 0.6731\n",
      "Epoch 82/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0585 - acc: 0.9799 - val_loss: 2.7933 - val_acc: 0.7244\n",
      "Epoch 83/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0577 - acc: 0.9821 - val_loss: 3.0925 - val_acc: 0.6843\n",
      "Epoch 84/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0582 - acc: 0.9825 - val_loss: 2.8708 - val_acc: 0.6955\n",
      "Epoch 85/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0483 - acc: 0.9836 - val_loss: 2.3611 - val_acc: 0.7452\n",
      "Epoch 86/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0641 - acc: 0.9793 - val_loss: 2.4406 - val_acc: 0.7276\n",
      "Epoch 87/1000\n",
      "2682/2682 [==============================] - 1s 302us/step - loss: 0.0610 - acc: 0.9799 - val_loss: 2.9586 - val_acc: 0.6955\n",
      "Epoch 88/1000\n",
      "2682/2682 [==============================] - 1s 310us/step - loss: 0.0525 - acc: 0.9817 - val_loss: 3.0537 - val_acc: 0.6859\n",
      "Epoch 89/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0559 - acc: 0.9828 - val_loss: 3.8153 - val_acc: 0.6522\n",
      "Epoch 90/1000\n",
      "2682/2682 [==============================] - 1s 302us/step - loss: 0.0564 - acc: 0.9802 - val_loss: 3.2452 - val_acc: 0.6827\n",
      "Epoch 91/1000\n",
      "2682/2682 [==============================] - 1s 318us/step - loss: 0.0517 - acc: 0.9832 - val_loss: 2.8302 - val_acc: 0.7147\n",
      "Epoch 92/1000\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.0609 - acc: 0.9828 - val_loss: 2.4420 - val_acc: 0.7284\n",
      "Epoch 93/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0519 - acc: 0.9823 - val_loss: 2.1080 - val_acc: 0.7548\n",
      "Epoch 94/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0545 - acc: 0.9802 - val_loss: 2.6165 - val_acc: 0.7196\n",
      "Epoch 95/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0548 - acc: 0.9834 - val_loss: 2.4636 - val_acc: 0.7436\n",
      "Epoch 96/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0578 - acc: 0.9832 - val_loss: 3.4310 - val_acc: 0.6683\n",
      "Epoch 97/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0553 - acc: 0.9849 - val_loss: 3.1338 - val_acc: 0.6859\n",
      "Epoch 98/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0512 - acc: 0.9806 - val_loss: 2.7752 - val_acc: 0.7236\n",
      "Epoch 99/1000\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.0594 - acc: 0.9812 - val_loss: 2.7041 - val_acc: 0.7356\n",
      "Epoch 100/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0569 - acc: 0.9806 - val_loss: 2.1830 - val_acc: 0.7628\n",
      "Epoch 101/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0568 - acc: 0.9806 - val_loss: 2.4551 - val_acc: 0.7452\n",
      "Epoch 102/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0437 - acc: 0.9847 - val_loss: 4.1457 - val_acc: 0.6474\n",
      "Epoch 103/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0480 - acc: 0.9823 - val_loss: 3.0354 - val_acc: 0.7147\n",
      "Epoch 104/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0482 - acc: 0.9832 - val_loss: 2.5680 - val_acc: 0.7388\n",
      "Epoch 105/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0484 - acc: 0.9842 - val_loss: 2.6392 - val_acc: 0.7388\n",
      "Epoch 106/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0573 - acc: 0.9810 - val_loss: 2.8092 - val_acc: 0.7308\n",
      "Epoch 107/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0476 - acc: 0.9817 - val_loss: 3.2885 - val_acc: 0.6787\n",
      "Epoch 108/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0507 - acc: 0.9855 - val_loss: 2.5497 - val_acc: 0.7404\n",
      "Epoch 109/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0516 - acc: 0.9836 - val_loss: 3.0399 - val_acc: 0.6955\n",
      "Epoch 110/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0611 - acc: 0.9843 - val_loss: 1.9876 - val_acc: 0.7724\n",
      "Epoch 111/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0473 - acc: 0.9866 - val_loss: 3.2285 - val_acc: 0.6779\n",
      "Epoch 112/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0587 - acc: 0.9834 - val_loss: 2.3753 - val_acc: 0.7452\n",
      "Epoch 113/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0480 - acc: 0.9866 - val_loss: 3.0155 - val_acc: 0.7244\n",
      "Epoch 114/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0450 - acc: 0.9840 - val_loss: 2.7641 - val_acc: 0.7324\n",
      "Epoch 115/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0520 - acc: 0.9830 - val_loss: 3.2149 - val_acc: 0.6923\n",
      "Epoch 116/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0499 - acc: 0.9862 - val_loss: 3.6686 - val_acc: 0.6763\n",
      "Epoch 117/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0551 - acc: 0.9823 - val_loss: 2.1385 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0592 - acc: 0.9812 - val_loss: 2.9354 - val_acc: 0.7212\n",
      "Epoch 119/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0435 - acc: 0.9851 - val_loss: 3.0789 - val_acc: 0.7131\n",
      "Epoch 120/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0452 - acc: 0.9851 - val_loss: 2.3150 - val_acc: 0.7596\n",
      "Epoch 121/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0415 - acc: 0.9866 - val_loss: 3.1616 - val_acc: 0.7051\n",
      "Epoch 122/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0552 - acc: 0.9827 - val_loss: 2.6331 - val_acc: 0.7388\n",
      "Epoch 123/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0515 - acc: 0.9847 - val_loss: 3.1513 - val_acc: 0.7099\n",
      "Epoch 124/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0439 - acc: 0.9840 - val_loss: 2.9922 - val_acc: 0.7212\n",
      "Epoch 125/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0417 - acc: 0.9864 - val_loss: 2.4679 - val_acc: 0.7484\n",
      "Epoch 126/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0442 - acc: 0.9843 - val_loss: 2.9409 - val_acc: 0.7292\n",
      "Epoch 127/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0434 - acc: 0.9866 - val_loss: 3.1265 - val_acc: 0.7204\n",
      "Epoch 128/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0483 - acc: 0.9858 - val_loss: 3.1242 - val_acc: 0.7131\n",
      "Epoch 129/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0611 - acc: 0.9858 - val_loss: 3.0570 - val_acc: 0.7260\n",
      "Epoch 130/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0503 - acc: 0.9866 - val_loss: 2.9059 - val_acc: 0.7324\n",
      "Epoch 131/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0509 - acc: 0.9862 - val_loss: 2.6804 - val_acc: 0.7388\n",
      "Epoch 132/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0563 - acc: 0.9799 - val_loss: 2.8405 - val_acc: 0.7244\n",
      "Epoch 133/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0431 - acc: 0.9892 - val_loss: 2.8315 - val_acc: 0.7452\n",
      "Epoch 134/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0495 - acc: 0.9843 - val_loss: 2.9956 - val_acc: 0.7276\n",
      "Epoch 135/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0444 - acc: 0.9877 - val_loss: 3.5913 - val_acc: 0.6763\n",
      "Epoch 136/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0509 - acc: 0.9866 - val_loss: 3.2039 - val_acc: 0.7147\n",
      "Epoch 137/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0501 - acc: 0.9834 - val_loss: 3.2416 - val_acc: 0.7212\n",
      "Epoch 138/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0560 - acc: 0.9832 - val_loss: 3.0841 - val_acc: 0.7308\n",
      "Epoch 139/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0435 - acc: 0.9866 - val_loss: 3.3064 - val_acc: 0.7115\n",
      "Epoch 140/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0412 - acc: 0.9870 - val_loss: 3.0379 - val_acc: 0.7308\n",
      "Epoch 141/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0488 - acc: 0.9821 - val_loss: 2.4992 - val_acc: 0.7724\n",
      "Epoch 142/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0422 - acc: 0.9866 - val_loss: 2.9244 - val_acc: 0.7268\n",
      "Epoch 143/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0447 - acc: 0.9855 - val_loss: 3.4586 - val_acc: 0.6939\n",
      "Epoch 144/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0395 - acc: 0.9858 - val_loss: 2.3128 - val_acc: 0.7644\n",
      "Epoch 145/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0462 - acc: 0.9847 - val_loss: 3.0328 - val_acc: 0.7228\n",
      "Epoch 146/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0444 - acc: 0.9840 - val_loss: 2.2554 - val_acc: 0.7756\n",
      "Epoch 147/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0459 - acc: 0.9847 - val_loss: 2.1812 - val_acc: 0.7740\n",
      "Epoch 148/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0431 - acc: 0.9871 - val_loss: 3.3170 - val_acc: 0.6987\n",
      "Epoch 149/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0406 - acc: 0.9870 - val_loss: 3.0909 - val_acc: 0.7276\n",
      "Epoch 150/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0424 - acc: 0.9853 - val_loss: 2.9011 - val_acc: 0.7276\n",
      "Epoch 151/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0431 - acc: 0.9881 - val_loss: 2.9165 - val_acc: 0.7212\n",
      "Epoch 152/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0377 - acc: 0.9877 - val_loss: 2.9183 - val_acc: 0.7356\n",
      "Epoch 153/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0424 - acc: 0.9877 - val_loss: 2.7515 - val_acc: 0.7452\n",
      "Epoch 154/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0414 - acc: 0.9866 - val_loss: 3.1149 - val_acc: 0.7276\n",
      "Epoch 155/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0501 - acc: 0.9851 - val_loss: 3.4920 - val_acc: 0.6827\n",
      "Epoch 156/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0469 - acc: 0.9881 - val_loss: 3.0899 - val_acc: 0.7260\n",
      "Epoch 157/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0488 - acc: 0.9866 - val_loss: 3.4331 - val_acc: 0.7035\n",
      "Epoch 158/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0558 - acc: 0.9851 - val_loss: 2.9459 - val_acc: 0.7340\n",
      "Epoch 159/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0385 - acc: 0.9866 - val_loss: 3.7115 - val_acc: 0.6779\n",
      "Epoch 160/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0526 - acc: 0.9881 - val_loss: 2.9731 - val_acc: 0.7300\n",
      "Epoch 161/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0398 - acc: 0.9870 - val_loss: 3.5079 - val_acc: 0.7083\n",
      "Epoch 162/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0493 - acc: 0.9873 - val_loss: 3.4501 - val_acc: 0.7067\n",
      "Epoch 163/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0386 - acc: 0.9881 - val_loss: 3.9559 - val_acc: 0.6715\n",
      "Epoch 164/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0465 - acc: 0.9832 - val_loss: 2.8694 - val_acc: 0.7388\n",
      "Epoch 165/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0470 - acc: 0.9877 - val_loss: 2.7798 - val_acc: 0.7436\n",
      "Epoch 166/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0361 - acc: 0.9862 - val_loss: 3.0418 - val_acc: 0.7340\n",
      "Epoch 167/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0423 - acc: 0.9851 - val_loss: 4.0868 - val_acc: 0.6699\n",
      "Epoch 168/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0340 - acc: 0.9892 - val_loss: 2.9183 - val_acc: 0.7516\n",
      "Epoch 169/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0354 - acc: 0.9903 - val_loss: 2.8069 - val_acc: 0.7404\n",
      "Epoch 170/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0428 - acc: 0.9870 - val_loss: 2.6888 - val_acc: 0.7516\n",
      "Epoch 171/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0352 - acc: 0.9899 - val_loss: 3.2168 - val_acc: 0.7308\n",
      "Epoch 172/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0384 - acc: 0.9858 - val_loss: 3.2408 - val_acc: 0.7196\n",
      "Epoch 173/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0355 - acc: 0.9907 - val_loss: 3.3969 - val_acc: 0.7107\n",
      "Epoch 174/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0283 - acc: 0.9888 - val_loss: 2.8251 - val_acc: 0.7484\n",
      "Epoch 175/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0452 - acc: 0.9871 - val_loss: 3.4260 - val_acc: 0.6971\n",
      "Epoch 176/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0376 - acc: 0.9873 - val_loss: 2.6591 - val_acc: 0.7532\n",
      "Epoch 177/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0463 - acc: 0.9873 - val_loss: 3.5391 - val_acc: 0.7067\n",
      "Epoch 178/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0424 - acc: 0.9866 - val_loss: 3.6214 - val_acc: 0.6939\n",
      "Epoch 179/1000\n",
      "2682/2682 [==============================] - 1s 300us/step - loss: 0.0283 - acc: 0.9907 - val_loss: 3.3013 - val_acc: 0.7212\n",
      "Epoch 180/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0497 - acc: 0.9881 - val_loss: 3.4553 - val_acc: 0.7115\n",
      "Epoch 181/1000\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0424 - acc: 0.9888 - val_loss: 2.9761 - val_acc: 0.7372\n",
      "Epoch 182/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0407 - acc: 0.9860 - val_loss: 2.7239 - val_acc: 0.7452\n",
      "Epoch 183/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0339 - acc: 0.9901 - val_loss: 3.4063 - val_acc: 0.7131\n",
      "Epoch 184/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0347 - acc: 0.9899 - val_loss: 3.2317 - val_acc: 0.7292\n",
      "Epoch 185/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0367 - acc: 0.9896 - val_loss: 2.9925 - val_acc: 0.7404\n",
      "Epoch 186/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0343 - acc: 0.9884 - val_loss: 3.5837 - val_acc: 0.7099\n",
      "Epoch 187/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0340 - acc: 0.9911 - val_loss: 2.9309 - val_acc: 0.7452\n",
      "Epoch 188/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0271 - acc: 0.9903 - val_loss: 2.8049 - val_acc: 0.7524\n",
      "Epoch 189/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0442 - acc: 0.9870 - val_loss: 3.1931 - val_acc: 0.7292\n",
      "Epoch 190/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0319 - acc: 0.9903 - val_loss: 3.4497 - val_acc: 0.7179\n",
      "Epoch 191/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0401 - acc: 0.9873 - val_loss: 2.8171 - val_acc: 0.7420\n",
      "Epoch 192/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0342 - acc: 0.9892 - val_loss: 3.1686 - val_acc: 0.7324\n",
      "Epoch 193/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0330 - acc: 0.9909 - val_loss: 3.8245 - val_acc: 0.6827\n",
      "Epoch 194/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0346 - acc: 0.9888 - val_loss: 3.6048 - val_acc: 0.6859\n",
      "Epoch 195/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0424 - acc: 0.9871 - val_loss: 3.4004 - val_acc: 0.7308\n",
      "Epoch 196/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0493 - acc: 0.9866 - val_loss: 3.8147 - val_acc: 0.6859\n",
      "Epoch 197/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0410 - acc: 0.9888 - val_loss: 3.4061 - val_acc: 0.7131\n",
      "Epoch 198/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0319 - acc: 0.9914 - val_loss: 3.6651 - val_acc: 0.7019\n",
      "Epoch 199/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0291 - acc: 0.9907 - val_loss: 3.7224 - val_acc: 0.6875\n",
      "Epoch 200/1000\n",
      "2682/2682 [==============================] - 1s 303us/step - loss: 0.0392 - acc: 0.9888 - val_loss: 3.4131 - val_acc: 0.7115\n",
      "Epoch 201/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0350 - acc: 0.9896 - val_loss: 3.8515 - val_acc: 0.6795\n",
      "Epoch 202/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0290 - acc: 0.9897 - val_loss: 2.8610 - val_acc: 0.7468\n",
      "Epoch 203/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0430 - acc: 0.9881 - val_loss: 3.7776 - val_acc: 0.6875\n",
      "Epoch 204/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0355 - acc: 0.9881 - val_loss: 3.0914 - val_acc: 0.7388\n",
      "Epoch 205/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0431 - acc: 0.9858 - val_loss: 2.3374 - val_acc: 0.7772\n",
      "Epoch 206/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0368 - acc: 0.9873 - val_loss: 2.9859 - val_acc: 0.7388\n",
      "Epoch 207/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0290 - acc: 0.9909 - val_loss: 4.2014 - val_acc: 0.6715\n",
      "Epoch 208/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0381 - acc: 0.9873 - val_loss: 3.6404 - val_acc: 0.7003\n",
      "Epoch 209/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0273 - acc: 0.9892 - val_loss: 3.3040 - val_acc: 0.7356\n",
      "Epoch 210/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0391 - acc: 0.9892 - val_loss: 3.2265 - val_acc: 0.7316\n",
      "Epoch 211/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0308 - acc: 0.9881 - val_loss: 4.1062 - val_acc: 0.6699\n",
      "Epoch 212/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0305 - acc: 0.9918 - val_loss: 3.5917 - val_acc: 0.6955\n",
      "Epoch 213/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0434 - acc: 0.9879 - val_loss: 3.9045 - val_acc: 0.6827\n",
      "Epoch 214/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0261 - acc: 0.9907 - val_loss: 3.7370 - val_acc: 0.6907\n",
      "Epoch 215/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0317 - acc: 0.9896 - val_loss: 2.5794 - val_acc: 0.7644\n",
      "Epoch 216/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0329 - acc: 0.9888 - val_loss: 3.2896 - val_acc: 0.7228\n",
      "Epoch 217/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0400 - acc: 0.9890 - val_loss: 3.4328 - val_acc: 0.7179\n",
      "Epoch 218/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0350 - acc: 0.9903 - val_loss: 2.9993 - val_acc: 0.7452\n",
      "Epoch 219/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0374 - acc: 0.9877 - val_loss: 3.2624 - val_acc: 0.7308\n",
      "Epoch 220/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0483 - acc: 0.9888 - val_loss: 3.7290 - val_acc: 0.6843\n",
      "Epoch 221/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0359 - acc: 0.9896 - val_loss: 3.6345 - val_acc: 0.7003\n",
      "Epoch 222/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0248 - acc: 0.9903 - val_loss: 3.3291 - val_acc: 0.7308\n",
      "Epoch 223/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0300 - acc: 0.9918 - val_loss: 3.6943 - val_acc: 0.7003\n",
      "Epoch 224/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0437 - acc: 0.9903 - val_loss: 3.4221 - val_acc: 0.7179\n",
      "Epoch 225/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0248 - acc: 0.9892 - val_loss: 3.7752 - val_acc: 0.6939\n",
      "Epoch 226/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0354 - acc: 0.9907 - val_loss: 3.3582 - val_acc: 0.7244\n",
      "Epoch 227/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0403 - acc: 0.9881 - val_loss: 3.2141 - val_acc: 0.7356\n",
      "Epoch 228/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0304 - acc: 0.9903 - val_loss: 3.8036 - val_acc: 0.6843\n",
      "Epoch 229/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0347 - acc: 0.9884 - val_loss: 3.8557 - val_acc: 0.6811\n",
      "Epoch 230/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0303 - acc: 0.9896 - val_loss: 3.2908 - val_acc: 0.7260\n",
      "Epoch 231/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0294 - acc: 0.9907 - val_loss: 3.4882 - val_acc: 0.7131\n",
      "Epoch 232/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 3.1660 - val_acc: 0.7420\n",
      "Epoch 233/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0305 - acc: 0.9903 - val_loss: 3.3487 - val_acc: 0.7308\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0339 - acc: 0.9892 - val_loss: 3.6444 - val_acc: 0.7035\n",
      "Epoch 235/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0311 - acc: 0.9916 - val_loss: 3.4399 - val_acc: 0.7276\n",
      "Epoch 236/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0306 - acc: 0.9914 - val_loss: 3.9696 - val_acc: 0.6875\n",
      "Epoch 237/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0291 - acc: 0.9907 - val_loss: 2.5321 - val_acc: 0.7788\n",
      "Epoch 238/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0283 - acc: 0.9918 - val_loss: 3.1147 - val_acc: 0.7436\n",
      "Epoch 239/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0206 - acc: 0.9937 - val_loss: 3.3799 - val_acc: 0.7276\n",
      "Epoch 240/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0342 - acc: 0.9899 - val_loss: 3.3678 - val_acc: 0.7276\n",
      "Epoch 241/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0378 - acc: 0.9907 - val_loss: 3.7918 - val_acc: 0.6939\n",
      "Epoch 242/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0273 - acc: 0.9886 - val_loss: 3.2499 - val_acc: 0.7340\n",
      "Epoch 243/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0373 - acc: 0.9892 - val_loss: 4.4433 - val_acc: 0.6683\n",
      "Epoch 244/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0251 - acc: 0.9937 - val_loss: 3.1368 - val_acc: 0.7468\n",
      "Epoch 245/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0263 - acc: 0.9925 - val_loss: 3.2463 - val_acc: 0.7436\n",
      "Epoch 246/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0339 - acc: 0.9911 - val_loss: 3.2060 - val_acc: 0.7404\n",
      "Epoch 247/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0281 - acc: 0.9925 - val_loss: 3.8724 - val_acc: 0.6875\n",
      "Epoch 248/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0408 - acc: 0.9894 - val_loss: 3.2995 - val_acc: 0.7316\n",
      "Epoch 249/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0264 - acc: 0.9914 - val_loss: 2.8421 - val_acc: 0.7500\n",
      "Epoch 250/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0320 - acc: 0.9909 - val_loss: 3.3701 - val_acc: 0.7324\n",
      "Epoch 251/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0258 - acc: 0.9922 - val_loss: 2.8493 - val_acc: 0.7612\n",
      "Epoch 252/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0359 - acc: 0.9899 - val_loss: 3.2387 - val_acc: 0.7356\n",
      "Epoch 253/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0416 - acc: 0.9884 - val_loss: 3.4279 - val_acc: 0.7212\n",
      "Epoch 254/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 3.1437 - val_acc: 0.7340\n",
      "Epoch 255/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0308 - acc: 0.9929 - val_loss: 3.6650 - val_acc: 0.7115\n",
      "Epoch 256/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0359 - acc: 0.9896 - val_loss: 3.5338 - val_acc: 0.7115\n",
      "Epoch 257/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0239 - acc: 0.9911 - val_loss: 3.4971 - val_acc: 0.7228\n",
      "Epoch 258/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0362 - acc: 0.9903 - val_loss: 3.1526 - val_acc: 0.7420\n",
      "Epoch 259/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0288 - acc: 0.9899 - val_loss: 3.1354 - val_acc: 0.7388\n",
      "Epoch 260/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0242 - acc: 0.9918 - val_loss: 3.1113 - val_acc: 0.7372\n",
      "Epoch 261/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0262 - acc: 0.9937 - val_loss: 3.0575 - val_acc: 0.7452\n",
      "Epoch 262/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0382 - acc: 0.9903 - val_loss: 4.0123 - val_acc: 0.6843\n",
      "Epoch 263/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0257 - acc: 0.9911 - val_loss: 3.2385 - val_acc: 0.7356\n",
      "Epoch 264/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0289 - acc: 0.9925 - val_loss: 3.7440 - val_acc: 0.6923\n",
      "Epoch 265/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 3.5861 - val_acc: 0.7147\n",
      "Epoch 266/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0270 - acc: 0.9933 - val_loss: 3.5135 - val_acc: 0.7196\n",
      "Epoch 267/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0309 - acc: 0.9925 - val_loss: 3.6840 - val_acc: 0.7035\n",
      "Epoch 268/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0346 - acc: 0.9914 - val_loss: 3.4411 - val_acc: 0.7372\n",
      "Epoch 269/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0229 - acc: 0.9922 - val_loss: 4.0118 - val_acc: 0.6859\n",
      "Epoch 270/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0269 - acc: 0.9925 - val_loss: 3.2756 - val_acc: 0.7340\n",
      "Epoch 271/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0289 - acc: 0.9922 - val_loss: 3.0080 - val_acc: 0.7468\n",
      "Epoch 272/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0241 - acc: 0.9924 - val_loss: 2.8726 - val_acc: 0.7532\n",
      "Epoch 273/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 3.3415 - val_acc: 0.7340\n",
      "Epoch 274/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0316 - acc: 0.9933 - val_loss: 4.0032 - val_acc: 0.6891\n",
      "Epoch 275/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0255 - acc: 0.9914 - val_loss: 3.4711 - val_acc: 0.7244\n",
      "Epoch 276/1000\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.0270 - acc: 0.9914 - val_loss: 3.3443 - val_acc: 0.7356\n",
      "Epoch 277/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0254 - acc: 0.9922 - val_loss: 3.4612 - val_acc: 0.7372\n",
      "Epoch 278/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0214 - acc: 0.9948 - val_loss: 3.5340 - val_acc: 0.7260\n",
      "Epoch 279/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0424 - acc: 0.9888 - val_loss: 3.8110 - val_acc: 0.6955\n",
      "Epoch 280/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0261 - acc: 0.9925 - val_loss: 3.7069 - val_acc: 0.7083\n",
      "Epoch 281/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0216 - acc: 0.9931 - val_loss: 3.3359 - val_acc: 0.7340\n",
      "Epoch 282/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0234 - acc: 0.9918 - val_loss: 3.6051 - val_acc: 0.7147\n",
      "Epoch 283/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0282 - acc: 0.9944 - val_loss: 3.6465 - val_acc: 0.7163\n",
      "Epoch 284/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0186 - acc: 0.9929 - val_loss: 3.2963 - val_acc: 0.7404\n",
      "Epoch 285/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0247 - acc: 0.9940 - val_loss: 4.3932 - val_acc: 0.6683\n",
      "Epoch 286/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0255 - acc: 0.9937 - val_loss: 4.3931 - val_acc: 0.6683\n",
      "Epoch 287/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0284 - acc: 0.9933 - val_loss: 3.5650 - val_acc: 0.7179\n",
      "Epoch 288/1000\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.0335 - acc: 0.9918 - val_loss: 3.1328 - val_acc: 0.7452\n",
      "Epoch 289/1000\n",
      "2682/2682 [==============================] - 1s 312us/step - loss: 0.0294 - acc: 0.9916 - val_loss: 3.6649 - val_acc: 0.7083\n",
      "Epoch 290/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0172 - acc: 0.9937 - val_loss: 3.4821 - val_acc: 0.7324\n",
      "Epoch 291/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 3.9115 - val_acc: 0.7051\n",
      "Epoch 292/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0277 - acc: 0.9929 - val_loss: 2.8461 - val_acc: 0.7564\n",
      "Epoch 293/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0170 - acc: 0.9963 - val_loss: 3.6315 - val_acc: 0.7228\n",
      "Epoch 294/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0248 - acc: 0.9918 - val_loss: 3.3158 - val_acc: 0.7356\n",
      "Epoch 295/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0148 - acc: 0.9937 - val_loss: 3.6299 - val_acc: 0.7212\n",
      "Epoch 296/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0273 - acc: 0.9922 - val_loss: 4.1088 - val_acc: 0.6875\n",
      "Epoch 297/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0275 - acc: 0.9935 - val_loss: 4.5004 - val_acc: 0.6699\n",
      "Epoch 298/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0282 - acc: 0.9925 - val_loss: 4.0776 - val_acc: 0.6843\n",
      "Epoch 299/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0292 - acc: 0.9922 - val_loss: 4.0277 - val_acc: 0.6971\n",
      "Epoch 300/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0211 - acc: 0.9933 - val_loss: 3.7757 - val_acc: 0.7051\n",
      "Epoch 301/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0183 - acc: 0.9948 - val_loss: 3.7413 - val_acc: 0.7019\n",
      "Epoch 302/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0382 - acc: 0.9911 - val_loss: 3.3828 - val_acc: 0.7388\n",
      "Epoch 303/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0191 - acc: 0.9942 - val_loss: 2.9021 - val_acc: 0.7548\n",
      "Epoch 304/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0295 - acc: 0.9927 - val_loss: 3.6952 - val_acc: 0.7115\n",
      "Epoch 305/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0225 - acc: 0.9944 - val_loss: 3.6933 - val_acc: 0.7091\n",
      "Epoch 306/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0415 - acc: 0.9918 - val_loss: 3.6433 - val_acc: 0.7212\n",
      "Epoch 307/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0302 - acc: 0.9916 - val_loss: 3.6666 - val_acc: 0.7035\n",
      "Epoch 308/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0182 - acc: 0.9940 - val_loss: 3.9153 - val_acc: 0.7003\n",
      "Epoch 309/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0329 - acc: 0.9903 - val_loss: 4.1636 - val_acc: 0.6795\n",
      "Epoch 310/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0206 - acc: 0.9944 - val_loss: 3.6009 - val_acc: 0.7179\n",
      "Epoch 311/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0214 - acc: 0.9918 - val_loss: 3.7194 - val_acc: 0.7115\n",
      "Epoch 312/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0161 - acc: 0.9944 - val_loss: 3.6009 - val_acc: 0.7308\n",
      "Epoch 313/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0354 - acc: 0.9896 - val_loss: 3.7791 - val_acc: 0.7131\n",
      "Epoch 314/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0186 - acc: 0.9922 - val_loss: 3.7414 - val_acc: 0.7067\n",
      "Epoch 315/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0177 - acc: 0.9933 - val_loss: 3.6490 - val_acc: 0.7212\n",
      "Epoch 316/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0194 - acc: 0.9929 - val_loss: 4.1247 - val_acc: 0.6827\n",
      "Epoch 317/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 3.4852 - val_acc: 0.7308\n",
      "Epoch 318/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 3.1060 - val_acc: 0.7500\n",
      "Epoch 319/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0211 - acc: 0.9937 - val_loss: 3.2069 - val_acc: 0.7436\n",
      "Epoch 320/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0319 - acc: 0.9918 - val_loss: 3.7886 - val_acc: 0.7067\n",
      "Epoch 321/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0288 - acc: 0.9925 - val_loss: 3.5914 - val_acc: 0.7179\n",
      "Epoch 322/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0296 - acc: 0.9937 - val_loss: 3.5889 - val_acc: 0.7196\n",
      "Epoch 323/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0207 - acc: 0.9918 - val_loss: 2.9339 - val_acc: 0.7468\n",
      "Epoch 324/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0227 - acc: 0.9944 - val_loss: 3.6436 - val_acc: 0.7147\n",
      "Epoch 325/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0252 - acc: 0.9922 - val_loss: 3.3027 - val_acc: 0.7324\n",
      "Epoch 326/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0172 - acc: 0.9948 - val_loss: 3.5077 - val_acc: 0.7340\n",
      "Epoch 327/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0229 - acc: 0.9924 - val_loss: 3.3441 - val_acc: 0.7276\n",
      "Epoch 328/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0168 - acc: 0.9933 - val_loss: 3.6641 - val_acc: 0.7083\n",
      "Epoch 329/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0239 - acc: 0.9948 - val_loss: 3.4936 - val_acc: 0.7372\n",
      "Epoch 330/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 3.4338 - val_acc: 0.7436\n",
      "Epoch 331/1000\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.0193 - acc: 0.9940 - val_loss: 3.5634 - val_acc: 0.7276\n",
      "Epoch 332/1000\n",
      "2682/2682 [==============================] - 1s 296us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 3.8127 - val_acc: 0.7067\n",
      "Epoch 333/1000\n",
      "2682/2682 [==============================] - 1s 320us/step - loss: 0.0328 - acc: 0.9940 - val_loss: 3.8203 - val_acc: 0.7083\n",
      "Epoch 334/1000\n",
      "2682/2682 [==============================] - 1s 314us/step - loss: 0.0215 - acc: 0.9937 - val_loss: 3.5779 - val_acc: 0.7212\n",
      "Epoch 335/1000\n",
      "2682/2682 [==============================] - 1s 309us/step - loss: 0.0144 - acc: 0.9944 - val_loss: 4.4014 - val_acc: 0.6731\n",
      "Epoch 336/1000\n",
      "2682/2682 [==============================] - 1s 317us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 4.1113 - val_acc: 0.6875\n",
      "Epoch 337/1000\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 3.6830 - val_acc: 0.7196\n",
      "Epoch 338/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0279 - acc: 0.9940 - val_loss: 3.7122 - val_acc: 0.7179\n",
      "Epoch 339/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0197 - acc: 0.9929 - val_loss: 3.4954 - val_acc: 0.7340\n",
      "Epoch 340/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0217 - acc: 0.9944 - val_loss: 3.4139 - val_acc: 0.7372\n",
      "Epoch 341/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0340 - acc: 0.9914 - val_loss: 3.4096 - val_acc: 0.7292\n",
      "Epoch 342/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0265 - acc: 0.9933 - val_loss: 3.4505 - val_acc: 0.7372\n",
      "Epoch 343/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0200 - acc: 0.9940 - val_loss: 3.5172 - val_acc: 0.7212\n",
      "Epoch 344/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0268 - acc: 0.9933 - val_loss: 4.0104 - val_acc: 0.7019\n",
      "Epoch 345/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0468 - acc: 0.9922 - val_loss: 3.4961 - val_acc: 0.7324\n",
      "Epoch 346/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0290 - acc: 0.9924 - val_loss: 3.7615 - val_acc: 0.7003\n",
      "Epoch 347/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0244 - acc: 0.9948 - val_loss: 3.7223 - val_acc: 0.7099\n",
      "Epoch 348/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0252 - acc: 0.9903 - val_loss: 3.6264 - val_acc: 0.7147\n",
      "Epoch 349/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0135 - acc: 0.9952 - val_loss: 3.6115 - val_acc: 0.7292\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0312 - acc: 0.9922 - val_loss: 3.4384 - val_acc: 0.7356\n",
      "Epoch 351/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0148 - acc: 0.9974 - val_loss: 3.6491 - val_acc: 0.7163\n",
      "Epoch 352/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0125 - acc: 0.9948 - val_loss: 4.1341 - val_acc: 0.6875\n",
      "Epoch 353/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 2.9574 - val_acc: 0.7500\n",
      "Epoch 354/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0211 - acc: 0.9959 - val_loss: 4.0095 - val_acc: 0.7035\n",
      "Epoch 355/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0213 - acc: 0.9955 - val_loss: 3.4070 - val_acc: 0.7420\n",
      "Epoch 356/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 3.5279 - val_acc: 0.7356\n",
      "Epoch 357/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0152 - acc: 0.9952 - val_loss: 4.3282 - val_acc: 0.6795\n",
      "Epoch 358/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 4.3691 - val_acc: 0.6811\n",
      "Epoch 359/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0423 - acc: 0.9911 - val_loss: 3.2500 - val_acc: 0.7372\n",
      "Epoch 360/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0163 - acc: 0.9955 - val_loss: 3.7895 - val_acc: 0.7147\n",
      "Epoch 361/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0175 - acc: 0.9955 - val_loss: 4.1052 - val_acc: 0.6923\n",
      "Epoch 362/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 3.6893 - val_acc: 0.7292\n",
      "Epoch 363/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0269 - acc: 0.9925 - val_loss: 3.7533 - val_acc: 0.7115\n",
      "Epoch 364/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0221 - acc: 0.9944 - val_loss: 3.7121 - val_acc: 0.7196\n",
      "Epoch 365/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0225 - acc: 0.9948 - val_loss: 3.9427 - val_acc: 0.7019\n",
      "Epoch 366/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0251 - acc: 0.9929 - val_loss: 3.8827 - val_acc: 0.6939\n",
      "Epoch 367/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0202 - acc: 0.9948 - val_loss: 3.1871 - val_acc: 0.7420\n",
      "Epoch 368/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 3.8292 - val_acc: 0.7083\n",
      "Epoch 369/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0288 - acc: 0.9937 - val_loss: 3.7471 - val_acc: 0.7131\n",
      "Epoch 370/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 3.7255 - val_acc: 0.7228\n",
      "Epoch 371/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0218 - acc: 0.9959 - val_loss: 3.8567 - val_acc: 0.7067\n",
      "Epoch 372/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0213 - acc: 0.9948 - val_loss: 3.5190 - val_acc: 0.7388\n",
      "Epoch 373/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0131 - acc: 0.9978 - val_loss: 3.5687 - val_acc: 0.7340\n",
      "Epoch 374/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0355 - acc: 0.9937 - val_loss: 4.0001 - val_acc: 0.6987\n",
      "Epoch 375/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0257 - acc: 0.9933 - val_loss: 3.6825 - val_acc: 0.7115\n",
      "Epoch 376/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 4.0242 - val_acc: 0.7003\n",
      "Epoch 377/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0356 - acc: 0.9918 - val_loss: 3.9401 - val_acc: 0.7099\n",
      "Epoch 378/1000\n",
      "2682/2682 [==============================] - 1s 313us/step - loss: 0.0296 - acc: 0.9929 - val_loss: 3.9684 - val_acc: 0.6971\n",
      "Epoch 379/1000\n",
      "2682/2682 [==============================] - 1s 300us/step - loss: 0.0089 - acc: 0.9955 - val_loss: 3.7537 - val_acc: 0.7196\n",
      "Epoch 380/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0148 - acc: 0.9944 - val_loss: 3.3485 - val_acc: 0.7436\n",
      "Epoch 381/1000\n",
      "2682/2682 [==============================] - 1s 300us/step - loss: 0.0254 - acc: 0.9948 - val_loss: 3.9453 - val_acc: 0.7051\n",
      "Epoch 382/1000\n",
      "2682/2682 [==============================] - 1s 295us/step - loss: 0.0223 - acc: 0.9948 - val_loss: 3.6959 - val_acc: 0.7179\n",
      "Epoch 383/1000\n",
      "2682/2682 [==============================] - 1s 345us/step - loss: 0.0179 - acc: 0.9963 - val_loss: 4.0429 - val_acc: 0.6923\n",
      "Epoch 384/1000\n",
      "2682/2682 [==============================] - 1s 327us/step - loss: 0.0263 - acc: 0.9929 - val_loss: 3.5570 - val_acc: 0.7228\n",
      "Epoch 385/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0151 - acc: 0.9940 - val_loss: 3.7160 - val_acc: 0.7212\n",
      "Epoch 386/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0231 - acc: 0.9929 - val_loss: 4.1657 - val_acc: 0.6891\n",
      "Epoch 387/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 3.4366 - val_acc: 0.7388\n",
      "Epoch 388/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0269 - acc: 0.9933 - val_loss: 3.6037 - val_acc: 0.7228\n",
      "Epoch 389/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 3.6797 - val_acc: 0.7228\n",
      "Epoch 390/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 3.4581 - val_acc: 0.7356\n",
      "Epoch 391/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 3.7706 - val_acc: 0.7276\n",
      "Epoch 392/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0255 - acc: 0.9940 - val_loss: 3.3060 - val_acc: 0.7388\n",
      "Epoch 393/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0226 - acc: 0.9963 - val_loss: 3.7792 - val_acc: 0.7196\n",
      "Epoch 394/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0354 - acc: 0.9911 - val_loss: 3.5778 - val_acc: 0.7340\n",
      "Epoch 395/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0199 - acc: 0.9937 - val_loss: 3.8246 - val_acc: 0.7123\n",
      "Epoch 396/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0237 - acc: 0.9933 - val_loss: 3.7854 - val_acc: 0.7147\n",
      "Epoch 397/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0254 - acc: 0.9940 - val_loss: 3.6957 - val_acc: 0.7228\n",
      "Epoch 398/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0116 - acc: 0.9948 - val_loss: 3.8851 - val_acc: 0.7099\n",
      "Epoch 399/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0249 - acc: 0.9940 - val_loss: 3.6211 - val_acc: 0.7260\n",
      "Epoch 400/1000\n",
      "2682/2682 [==============================] - 1s 312us/step - loss: 0.0113 - acc: 0.9966 - val_loss: 3.7877 - val_acc: 0.7147\n",
      "Epoch 401/1000\n",
      "2682/2682 [==============================] - 1s 307us/step - loss: 0.0221 - acc: 0.9937 - val_loss: 3.5274 - val_acc: 0.7388\n",
      "Epoch 402/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0295 - acc: 0.9955 - val_loss: 3.7538 - val_acc: 0.7163\n",
      "Epoch 403/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0130 - acc: 0.9966 - val_loss: 4.1662 - val_acc: 0.6907\n",
      "Epoch 404/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0403 - acc: 0.9937 - val_loss: 3.7944 - val_acc: 0.7099\n",
      "Epoch 405/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0230 - acc: 0.9937 - val_loss: 3.8696 - val_acc: 0.7083\n",
      "Epoch 406/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0224 - acc: 0.9955 - val_loss: 3.6634 - val_acc: 0.7340\n",
      "Epoch 407/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 3.6363 - val_acc: 0.7276\n",
      "Epoch 408/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0329 - acc: 0.9937 - val_loss: 3.5830 - val_acc: 0.7308\n",
      "Epoch 409/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 3.9361 - val_acc: 0.6987\n",
      "Epoch 410/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0156 - acc: 0.9948 - val_loss: 4.0544 - val_acc: 0.6907\n",
      "Epoch 411/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0210 - acc: 0.9944 - val_loss: 4.0987 - val_acc: 0.6971\n",
      "Epoch 412/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 3.7586 - val_acc: 0.7228\n",
      "Epoch 413/1000\n",
      "2682/2682 [==============================] - 1s 298us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 4.0837 - val_acc: 0.6955\n",
      "Epoch 414/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0255 - acc: 0.9933 - val_loss: 3.4504 - val_acc: 0.7388\n",
      "Epoch 415/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 3.5322 - val_acc: 0.7324\n",
      "Epoch 416/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 3.6118 - val_acc: 0.7340\n",
      "Epoch 417/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 3.6898 - val_acc: 0.7324\n",
      "Epoch 418/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0196 - acc: 0.9955 - val_loss: 4.1991 - val_acc: 0.6907\n",
      "Epoch 419/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0239 - acc: 0.9948 - val_loss: 3.6046 - val_acc: 0.7292\n",
      "Epoch 420/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0134 - acc: 0.9948 - val_loss: 4.2746 - val_acc: 0.6859\n",
      "Epoch 421/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 3.4082 - val_acc: 0.7388\n",
      "Epoch 422/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 3.5844 - val_acc: 0.7356\n",
      "Epoch 423/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0151 - acc: 0.9966 - val_loss: 3.7598 - val_acc: 0.7196\n",
      "Epoch 424/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0196 - acc: 0.9955 - val_loss: 3.4346 - val_acc: 0.7356\n",
      "Epoch 425/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0191 - acc: 0.9929 - val_loss: 3.8748 - val_acc: 0.7083\n",
      "Epoch 426/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 3.9923 - val_acc: 0.6923\n",
      "Epoch 427/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0294 - acc: 0.9944 - val_loss: 3.6776 - val_acc: 0.7244\n",
      "Epoch 428/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 3.7245 - val_acc: 0.7196\n",
      "Epoch 429/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0210 - acc: 0.9959 - val_loss: 3.7473 - val_acc: 0.7163\n",
      "Epoch 430/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0194 - acc: 0.9944 - val_loss: 3.6748 - val_acc: 0.7244\n",
      "Epoch 431/1000\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.0082 - acc: 0.9959 - val_loss: 3.7016 - val_acc: 0.7276\n",
      "Epoch 432/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0200 - acc: 0.9952 - val_loss: 3.3955 - val_acc: 0.7436\n",
      "Epoch 433/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0127 - acc: 0.9963 - val_loss: 3.6973 - val_acc: 0.7260\n",
      "Epoch 434/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0228 - acc: 0.9955 - val_loss: 3.9476 - val_acc: 0.7051\n",
      "Epoch 435/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0195 - acc: 0.9948 - val_loss: 3.6839 - val_acc: 0.7244\n",
      "Epoch 436/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0179 - acc: 0.9948 - val_loss: 3.9346 - val_acc: 0.7099\n",
      "Epoch 437/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0106 - acc: 0.9974 - val_loss: 3.8318 - val_acc: 0.7115\n",
      "Epoch 438/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 4.0307 - val_acc: 0.7019\n",
      "Epoch 439/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0258 - acc: 0.9940 - val_loss: 3.8886 - val_acc: 0.7035\n",
      "Epoch 440/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0248 - acc: 0.9959 - val_loss: 4.4544 - val_acc: 0.6731\n",
      "Epoch 441/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 3.8774 - val_acc: 0.7099\n",
      "Epoch 442/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 3.6794 - val_acc: 0.7292\n",
      "Epoch 443/1000\n",
      "2682/2682 [==============================] - 1s 293us/step - loss: 0.0167 - acc: 0.9970 - val_loss: 3.5716 - val_acc: 0.7308\n",
      "Epoch 444/1000\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.0183 - acc: 0.9955 - val_loss: 3.9080 - val_acc: 0.7051\n",
      "Epoch 445/1000\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.0075 - acc: 0.9966 - val_loss: 3.7206 - val_acc: 0.7212\n",
      "Epoch 446/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0141 - acc: 0.9963 - val_loss: 3.9145 - val_acc: 0.7107\n",
      "Epoch 447/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0128 - acc: 0.9966 - val_loss: 3.6064 - val_acc: 0.7340\n",
      "Epoch 448/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0188 - acc: 0.9955 - val_loss: 4.0759 - val_acc: 0.6923\n",
      "Epoch 449/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0102 - acc: 0.9944 - val_loss: 3.4574 - val_acc: 0.7420\n",
      "Epoch 450/1000\n",
      "2682/2682 [==============================] - 1s 302us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 4.0370 - val_acc: 0.6987\n",
      "Epoch 451/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0146 - acc: 0.9948 - val_loss: 4.3250 - val_acc: 0.6811\n",
      "Epoch 452/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0110 - acc: 0.9974 - val_loss: 3.3512 - val_acc: 0.7484\n",
      "Epoch 453/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 4.0183 - val_acc: 0.7051\n",
      "Epoch 454/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0194 - acc: 0.9952 - val_loss: 3.8252 - val_acc: 0.7163\n",
      "Epoch 455/1000\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.0261 - acc: 0.9948 - val_loss: 3.8180 - val_acc: 0.7179\n",
      "Epoch 456/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0167 - acc: 0.9948 - val_loss: 3.7571 - val_acc: 0.7196\n",
      "Epoch 457/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0181 - acc: 0.9948 - val_loss: 3.5246 - val_acc: 0.7308\n",
      "Epoch 458/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 4.1124 - val_acc: 0.6963\n",
      "Epoch 459/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0138 - acc: 0.9944 - val_loss: 3.7579 - val_acc: 0.7228\n",
      "Epoch 460/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0212 - acc: 0.9963 - val_loss: 3.8174 - val_acc: 0.7196\n",
      "Epoch 461/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0150 - acc: 0.9959 - val_loss: 3.7344 - val_acc: 0.7228\n",
      "Epoch 462/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 3.5112 - val_acc: 0.7468\n",
      "Epoch 463/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0302 - acc: 0.9929 - val_loss: 4.0504 - val_acc: 0.7099\n",
      "Epoch 464/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0180 - acc: 0.9955 - val_loss: 3.8418 - val_acc: 0.7131\n",
      "Epoch 465/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0217 - acc: 0.9940 - val_loss: 4.0093 - val_acc: 0.6971\n",
      "Epoch 466/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0190 - acc: 0.9970 - val_loss: 3.7554 - val_acc: 0.7244\n",
      "Epoch 467/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0126 - acc: 0.9978 - val_loss: 4.1920 - val_acc: 0.6971\n",
      "Epoch 468/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 3.5873 - val_acc: 0.7372\n",
      "Epoch 469/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0215 - acc: 0.9944 - val_loss: 4.1891 - val_acc: 0.6939\n",
      "Epoch 470/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0103 - acc: 0.9966 - val_loss: 3.5730 - val_acc: 0.7372\n",
      "Epoch 471/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0128 - acc: 0.9955 - val_loss: 3.7227 - val_acc: 0.7212\n",
      "Epoch 472/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0083 - acc: 0.9963 - val_loss: 4.1601 - val_acc: 0.6987\n",
      "Epoch 473/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0212 - acc: 0.9952 - val_loss: 3.4638 - val_acc: 0.7396\n",
      "Epoch 474/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 4.1164 - val_acc: 0.7035\n",
      "Epoch 475/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 4.0676 - val_acc: 0.7099\n",
      "Epoch 476/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 4.3264 - val_acc: 0.6907\n",
      "Epoch 477/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 4.0508 - val_acc: 0.7051\n",
      "Epoch 478/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0192 - acc: 0.9955 - val_loss: 3.5673 - val_acc: 0.7420\n",
      "Epoch 479/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0193 - acc: 0.9948 - val_loss: 3.9363 - val_acc: 0.7067\n",
      "Epoch 480/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0233 - acc: 0.9933 - val_loss: 3.9025 - val_acc: 0.7107\n",
      "Epoch 481/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0127 - acc: 0.9963 - val_loss: 4.4655 - val_acc: 0.6731\n",
      "Epoch 482/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 3.7154 - val_acc: 0.7276\n",
      "Epoch 483/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0209 - acc: 0.9937 - val_loss: 3.8103 - val_acc: 0.7179\n",
      "Epoch 484/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0223 - acc: 0.9959 - val_loss: 3.9928 - val_acc: 0.7051\n",
      "Epoch 485/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0208 - acc: 0.9933 - val_loss: 3.6415 - val_acc: 0.7356\n",
      "Epoch 486/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0209 - acc: 0.9955 - val_loss: 4.0829 - val_acc: 0.7035\n",
      "Epoch 487/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 3.6184 - val_acc: 0.7420\n",
      "Epoch 488/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 4.2661 - val_acc: 0.6955\n",
      "Epoch 489/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0175 - acc: 0.9966 - val_loss: 3.7951 - val_acc: 0.7276\n",
      "Epoch 490/1000\n",
      "2682/2682 [==============================] - 1s 295us/step - loss: 0.0145 - acc: 0.9978 - val_loss: 3.5551 - val_acc: 0.7420\n",
      "Epoch 491/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0165 - acc: 0.9944 - val_loss: 3.8517 - val_acc: 0.7212\n",
      "Epoch 492/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0127 - acc: 0.9974 - val_loss: 3.9932 - val_acc: 0.7115\n",
      "Epoch 493/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0236 - acc: 0.9955 - val_loss: 3.5628 - val_acc: 0.7388\n",
      "Epoch 494/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0156 - acc: 0.9955 - val_loss: 3.7407 - val_acc: 0.7228\n",
      "Epoch 495/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0092 - acc: 0.9966 - val_loss: 3.8584 - val_acc: 0.7163\n",
      "Epoch 496/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0175 - acc: 0.9955 - val_loss: 3.7759 - val_acc: 0.7179\n",
      "Epoch 497/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 3.8488 - val_acc: 0.7179\n",
      "Epoch 498/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0131 - acc: 0.9981 - val_loss: 3.8042 - val_acc: 0.7212\n",
      "Epoch 499/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0145 - acc: 0.9944 - val_loss: 3.8451 - val_acc: 0.7099\n",
      "Epoch 500/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0074 - acc: 0.9970 - val_loss: 4.0193 - val_acc: 0.7083\n",
      "Epoch 501/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0135 - acc: 0.9948 - val_loss: 3.6398 - val_acc: 0.7324\n",
      "Epoch 502/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0228 - acc: 0.9959 - val_loss: 3.6157 - val_acc: 0.7372\n",
      "Epoch 503/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 3.9083 - val_acc: 0.7163\n",
      "Epoch 504/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0247 - acc: 0.9933 - val_loss: 3.4949 - val_acc: 0.7468\n",
      "Epoch 505/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 4.2049 - val_acc: 0.7019\n",
      "Epoch 506/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 3.9020 - val_acc: 0.7212\n",
      "Epoch 507/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 3.4902 - val_acc: 0.7388\n",
      "Epoch 508/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0110 - acc: 0.9959 - val_loss: 4.0899 - val_acc: 0.7019\n",
      "Epoch 509/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 4.2385 - val_acc: 0.6971\n",
      "Epoch 510/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0118 - acc: 0.9963 - val_loss: 3.8582 - val_acc: 0.7212\n",
      "Epoch 511/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0105 - acc: 0.9959 - val_loss: 3.9460 - val_acc: 0.7179\n",
      "Epoch 512/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0186 - acc: 0.9952 - val_loss: 3.9432 - val_acc: 0.7147\n",
      "Epoch 513/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0233 - acc: 0.9944 - val_loss: 3.9847 - val_acc: 0.7067\n",
      "Epoch 514/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0104 - acc: 0.9966 - val_loss: 4.2208 - val_acc: 0.6971\n",
      "Epoch 515/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 3.6088 - val_acc: 0.7372\n",
      "Epoch 516/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0146 - acc: 0.9955 - val_loss: 4.0734 - val_acc: 0.6987\n",
      "Epoch 517/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0163 - acc: 0.9966 - val_loss: 3.6676 - val_acc: 0.7324\n",
      "Epoch 518/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 4.0804 - val_acc: 0.7003\n",
      "Epoch 519/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 3.7597 - val_acc: 0.7212\n",
      "Epoch 520/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0064 - acc: 0.9978 - val_loss: 3.9141 - val_acc: 0.7163\n",
      "Epoch 521/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0101 - acc: 0.9963 - val_loss: 4.0604 - val_acc: 0.6987\n",
      "Epoch 522/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 3.8294 - val_acc: 0.7163\n",
      "Epoch 523/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0258 - acc: 0.9963 - val_loss: 3.8797 - val_acc: 0.7212\n",
      "Epoch 524/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0194 - acc: 0.9970 - val_loss: 3.9990 - val_acc: 0.7067\n",
      "Epoch 525/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0133 - acc: 0.9974 - val_loss: 3.5436 - val_acc: 0.7436\n",
      "Epoch 526/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0161 - acc: 0.9970 - val_loss: 3.7477 - val_acc: 0.7356\n",
      "Epoch 527/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0239 - acc: 0.9952 - val_loss: 3.7774 - val_acc: 0.7340\n",
      "Epoch 528/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 3.7743 - val_acc: 0.7340\n",
      "Epoch 529/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0175 - acc: 0.9940 - val_loss: 4.5270 - val_acc: 0.6779\n",
      "Epoch 530/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0094 - acc: 0.9963 - val_loss: 3.7220 - val_acc: 0.7276\n",
      "Epoch 531/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 3.7907 - val_acc: 0.7276\n",
      "Epoch 532/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0163 - acc: 0.9955 - val_loss: 3.8939 - val_acc: 0.7163\n",
      "Epoch 533/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 4.1331 - val_acc: 0.7019\n",
      "Epoch 534/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 4.2175 - val_acc: 0.6923\n",
      "Epoch 535/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0176 - acc: 0.9948 - val_loss: 4.3617 - val_acc: 0.6843\n",
      "Epoch 536/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0131 - acc: 0.9978 - val_loss: 3.7273 - val_acc: 0.7292\n",
      "Epoch 537/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0181 - acc: 0.9963 - val_loss: 3.8837 - val_acc: 0.7212\n",
      "Epoch 538/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0239 - acc: 0.9966 - val_loss: 3.3793 - val_acc: 0.7516\n",
      "Epoch 539/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0111 - acc: 0.9963 - val_loss: 3.7556 - val_acc: 0.7340\n",
      "Epoch 540/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0098 - acc: 0.9985 - val_loss: 3.7579 - val_acc: 0.7308\n",
      "Epoch 541/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0271 - acc: 0.9948 - val_loss: 3.8021 - val_acc: 0.7244\n",
      "Epoch 542/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 4.2884 - val_acc: 0.6939\n",
      "Epoch 543/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 3.7646 - val_acc: 0.7292\n",
      "Epoch 544/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0164 - acc: 0.9966 - val_loss: 3.8764 - val_acc: 0.7131\n",
      "Epoch 545/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 3.9480 - val_acc: 0.7083\n",
      "Epoch 546/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 3.4970 - val_acc: 0.7404\n",
      "Epoch 547/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0097 - acc: 0.9963 - val_loss: 3.7113 - val_acc: 0.7332\n",
      "Epoch 548/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0146 - acc: 0.9974 - val_loss: 3.9604 - val_acc: 0.7115\n",
      "Epoch 549/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0126 - acc: 0.9974 - val_loss: 3.7578 - val_acc: 0.7292\n",
      "Epoch 550/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0123 - acc: 0.9974 - val_loss: 4.3377 - val_acc: 0.6939\n",
      "Epoch 551/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0089 - acc: 0.9981 - val_loss: 3.6549 - val_acc: 0.7420\n",
      "Epoch 552/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 3.9461 - val_acc: 0.7131\n",
      "Epoch 553/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 4.3416 - val_acc: 0.6891\n",
      "Epoch 554/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0214 - acc: 0.9963 - val_loss: 3.6482 - val_acc: 0.7324\n",
      "Epoch 555/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 4.0875 - val_acc: 0.7067\n",
      "Epoch 556/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0147 - acc: 0.9974 - val_loss: 3.9093 - val_acc: 0.7244\n",
      "Epoch 557/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0361 - acc: 0.9940 - val_loss: 3.9269 - val_acc: 0.7067\n",
      "Epoch 558/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0074 - acc: 0.9985 - val_loss: 3.6929 - val_acc: 0.7340\n",
      "Epoch 559/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0036 - acc: 0.9985 - val_loss: 3.8903 - val_acc: 0.7212\n",
      "Epoch 560/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0229 - acc: 0.9952 - val_loss: 3.6795 - val_acc: 0.7452\n",
      "Epoch 561/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0153 - acc: 0.9978 - val_loss: 3.9719 - val_acc: 0.7131\n",
      "Epoch 562/1000\n",
      "2682/2682 [==============================] - 1s 318us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 4.2611 - val_acc: 0.6955\n",
      "Epoch 563/1000\n",
      "2682/2682 [==============================] - 1s 302us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 3.4874 - val_acc: 0.7388\n",
      "Epoch 564/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 4.3407 - val_acc: 0.6891\n",
      "Epoch 565/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0193 - acc: 0.9944 - val_loss: 3.8175 - val_acc: 0.7228\n",
      "Epoch 566/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0085 - acc: 0.9985 - val_loss: 3.8003 - val_acc: 0.7276\n",
      "Epoch 567/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0163 - acc: 0.9978 - val_loss: 4.2392 - val_acc: 0.6987\n",
      "Epoch 568/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0117 - acc: 0.9952 - val_loss: 3.8401 - val_acc: 0.7260\n",
      "Epoch 569/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 3.5788 - val_acc: 0.7420\n",
      "Epoch 570/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0090 - acc: 0.9966 - val_loss: 4.0006 - val_acc: 0.7099\n",
      "Epoch 571/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0088 - acc: 0.9978 - val_loss: 4.1059 - val_acc: 0.7019\n",
      "Epoch 572/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0234 - acc: 0.9948 - val_loss: 3.8796 - val_acc: 0.7308\n",
      "Epoch 573/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0145 - acc: 0.9959 - val_loss: 3.6847 - val_acc: 0.7324\n",
      "Epoch 574/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0090 - acc: 0.9966 - val_loss: 3.7990 - val_acc: 0.7356\n",
      "Epoch 575/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 3.8810 - val_acc: 0.7179\n",
      "Epoch 576/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 3.9062 - val_acc: 0.7244\n",
      "Epoch 577/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 3.9486 - val_acc: 0.7099\n",
      "Epoch 578/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 3.7199 - val_acc: 0.7292\n",
      "Epoch 579/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0120 - acc: 0.9978 - val_loss: 3.7266 - val_acc: 0.7372\n",
      "Epoch 580/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0136 - acc: 0.9959 - val_loss: 3.8259 - val_acc: 0.7292\n",
      "Epoch 581/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0162 - acc: 0.9963 - val_loss: 4.0028 - val_acc: 0.7051\n",
      "Epoch 582/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0105 - acc: 0.9974 - val_loss: 3.3732 - val_acc: 0.7452\n",
      "Epoch 583/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0107 - acc: 0.9978 - val_loss: 3.8509 - val_acc: 0.7228\n",
      "Epoch 584/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 3.5959 - val_acc: 0.7308\n",
      "Epoch 585/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0104 - acc: 0.9974 - val_loss: 3.5159 - val_acc: 0.7420\n",
      "Epoch 586/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 4.1503 - val_acc: 0.7019\n",
      "Epoch 587/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0139 - acc: 0.9978 - val_loss: 3.9715 - val_acc: 0.7131\n",
      "Epoch 588/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0133 - acc: 0.9966 - val_loss: 4.0741 - val_acc: 0.7003\n",
      "Epoch 589/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 4.3264 - val_acc: 0.6923\n",
      "Epoch 590/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 3.8935 - val_acc: 0.7163\n",
      "Epoch 591/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 3.8264 - val_acc: 0.7292\n",
      "Epoch 592/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0072 - acc: 0.9966 - val_loss: 3.5818 - val_acc: 0.7404\n",
      "Epoch 593/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 3.8312 - val_acc: 0.7340\n",
      "Epoch 594/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0038 - acc: 0.9985 - val_loss: 3.8038 - val_acc: 0.7292\n",
      "Epoch 595/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0216 - acc: 0.9966 - val_loss: 4.3297 - val_acc: 0.6923\n",
      "Epoch 596/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0099 - acc: 0.9978 - val_loss: 3.8363 - val_acc: 0.7292\n",
      "Epoch 597/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0111 - acc: 0.9955 - val_loss: 3.8657 - val_acc: 0.7292\n",
      "Epoch 598/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0090 - acc: 0.9966 - val_loss: 3.8007 - val_acc: 0.7276\n",
      "Epoch 599/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0119 - acc: 0.9985 - val_loss: 4.0423 - val_acc: 0.7067\n",
      "Epoch 600/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 3.6572 - val_acc: 0.7388\n",
      "Epoch 601/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0103 - acc: 0.9978 - val_loss: 3.9569 - val_acc: 0.7147\n",
      "Epoch 602/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 3.7428 - val_acc: 0.7436\n",
      "Epoch 603/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0235 - acc: 0.9952 - val_loss: 3.8332 - val_acc: 0.7212\n",
      "Epoch 604/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 3.8392 - val_acc: 0.7292\n",
      "Epoch 605/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0074 - acc: 0.9978 - val_loss: 4.1884 - val_acc: 0.6987\n",
      "Epoch 606/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0132 - acc: 0.9963 - val_loss: 3.4806 - val_acc: 0.7404\n",
      "Epoch 607/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0086 - acc: 0.9981 - val_loss: 3.8574 - val_acc: 0.7244\n",
      "Epoch 608/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 3.7659 - val_acc: 0.7308\n",
      "Epoch 609/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 4.0878 - val_acc: 0.7035\n",
      "Epoch 610/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 3.9950 - val_acc: 0.7115\n",
      "Epoch 611/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0117 - acc: 0.9974 - val_loss: 4.1739 - val_acc: 0.6971\n",
      "Epoch 612/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0148 - acc: 0.9974 - val_loss: 3.8427 - val_acc: 0.7244\n",
      "Epoch 613/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0082 - acc: 0.9989 - val_loss: 3.8768 - val_acc: 0.7196\n",
      "Epoch 614/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0069 - acc: 0.9974 - val_loss: 3.3143 - val_acc: 0.7516\n",
      "Epoch 615/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0102 - acc: 0.9966 - val_loss: 3.8663 - val_acc: 0.7244\n",
      "Epoch 616/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0161 - acc: 0.9966 - val_loss: 4.1753 - val_acc: 0.6987\n",
      "Epoch 617/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0085 - acc: 0.9974 - val_loss: 3.3493 - val_acc: 0.7484\n",
      "Epoch 618/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0155 - acc: 0.9978 - val_loss: 3.9355 - val_acc: 0.7212\n",
      "Epoch 619/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0280 - acc: 0.9940 - val_loss: 3.9190 - val_acc: 0.7163\n",
      "Epoch 620/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0086 - acc: 0.9981 - val_loss: 4.1489 - val_acc: 0.6971\n",
      "Epoch 621/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0158 - acc: 0.9963 - val_loss: 3.7305 - val_acc: 0.7404\n",
      "Epoch 622/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0084 - acc: 0.9978 - val_loss: 4.0726 - val_acc: 0.7083\n",
      "Epoch 623/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 4.0977 - val_acc: 0.7019\n",
      "Epoch 624/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0213 - acc: 0.9966 - val_loss: 3.9508 - val_acc: 0.7147\n",
      "Epoch 625/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 3.8948 - val_acc: 0.7212\n",
      "Epoch 626/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0136 - acc: 0.9959 - val_loss: 4.1060 - val_acc: 0.7067\n",
      "Epoch 627/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0074 - acc: 0.9978 - val_loss: 3.7537 - val_acc: 0.7324\n",
      "Epoch 628/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 3.9072 - val_acc: 0.7228\n",
      "Epoch 629/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 3.7022 - val_acc: 0.7420\n",
      "Epoch 630/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0109 - acc: 0.9989 - val_loss: 3.8919 - val_acc: 0.7228\n",
      "Epoch 631/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0105 - acc: 0.9974 - val_loss: 3.6250 - val_acc: 0.7420\n",
      "Epoch 632/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0134 - acc: 0.9955 - val_loss: 3.8056 - val_acc: 0.7212\n",
      "Epoch 633/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 3.7643 - val_acc: 0.7276\n",
      "Epoch 634/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 3.8369 - val_acc: 0.7179\n",
      "Epoch 635/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 3.7705 - val_acc: 0.7244\n",
      "Epoch 636/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0135 - acc: 0.9952 - val_loss: 3.7619 - val_acc: 0.7276\n",
      "Epoch 637/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.1023 - val_acc: 0.7083\n",
      "Epoch 638/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 3.7842 - val_acc: 0.7324\n",
      "Epoch 639/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0058 - acc: 0.9985 - val_loss: 4.0475 - val_acc: 0.7083\n",
      "Epoch 640/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0057 - acc: 0.9978 - val_loss: 4.0987 - val_acc: 0.6971\n",
      "Epoch 641/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 3.9793 - val_acc: 0.7163\n",
      "Epoch 642/1000\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.0089 - acc: 0.9981 - val_loss: 4.2265 - val_acc: 0.7035\n",
      "Epoch 643/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 3.8924 - val_acc: 0.7228\n",
      "Epoch 644/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0122 - acc: 0.9966 - val_loss: 3.6471 - val_acc: 0.7372\n",
      "Epoch 645/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 3.9745 - val_acc: 0.7131\n",
      "Epoch 646/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0093 - acc: 0.9966 - val_loss: 3.7993 - val_acc: 0.7228\n",
      "Epoch 647/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 4.1615 - val_acc: 0.7003\n",
      "Epoch 648/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 3.7482 - val_acc: 0.7356\n",
      "Epoch 649/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 3.5344 - val_acc: 0.7356\n",
      "Epoch 650/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 3.8036 - val_acc: 0.7276\n",
      "Epoch 651/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 4.1198 - val_acc: 0.7067\n",
      "Epoch 652/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0103 - acc: 0.9981 - val_loss: 4.1989 - val_acc: 0.7019\n",
      "Epoch 653/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0039 - acc: 0.9985 - val_loss: 4.0296 - val_acc: 0.7115\n",
      "Epoch 654/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 3.8663 - val_acc: 0.7228\n",
      "Epoch 655/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0080 - acc: 0.9981 - val_loss: 3.9575 - val_acc: 0.7179\n",
      "Epoch 656/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0080 - acc: 0.9989 - val_loss: 3.8992 - val_acc: 0.7276\n",
      "Epoch 657/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0094 - acc: 0.9963 - val_loss: 4.1030 - val_acc: 0.7115\n",
      "Epoch 658/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0081 - acc: 0.9981 - val_loss: 4.5455 - val_acc: 0.6827\n",
      "Epoch 659/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 4.0036 - val_acc: 0.7083\n",
      "Epoch 660/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 3.4451 - val_acc: 0.7468\n",
      "Epoch 661/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0197 - acc: 0.9974 - val_loss: 4.2331 - val_acc: 0.6939\n",
      "Epoch 662/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0226 - acc: 0.9959 - val_loss: 3.8960 - val_acc: 0.7212\n",
      "Epoch 663/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 3.8081 - val_acc: 0.7260\n",
      "Epoch 664/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0110 - acc: 0.9974 - val_loss: 4.0199 - val_acc: 0.7147\n",
      "Epoch 665/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 3.8233 - val_acc: 0.7196\n",
      "Epoch 666/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0159 - acc: 0.9974 - val_loss: 3.8514 - val_acc: 0.7212\n",
      "Epoch 667/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0064 - acc: 0.9985 - val_loss: 3.6946 - val_acc: 0.7372\n",
      "Epoch 668/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 3.8406 - val_acc: 0.7340\n",
      "Epoch 669/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0181 - acc: 0.9963 - val_loss: 4.2463 - val_acc: 0.6955\n",
      "Epoch 670/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0130 - acc: 0.9978 - val_loss: 4.2577 - val_acc: 0.7003\n",
      "Epoch 671/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 3.8128 - val_acc: 0.7340\n",
      "Epoch 672/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 3.9129 - val_acc: 0.7163\n",
      "Epoch 673/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 3.9058 - val_acc: 0.7196\n",
      "Epoch 674/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0081 - acc: 0.9978 - val_loss: 3.7881 - val_acc: 0.7340\n",
      "Epoch 675/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0091 - acc: 0.9993 - val_loss: 3.8062 - val_acc: 0.7356\n",
      "Epoch 676/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0048 - acc: 0.9981 - val_loss: 4.1140 - val_acc: 0.7051\n",
      "Epoch 677/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0106 - acc: 0.9966 - val_loss: 4.1299 - val_acc: 0.7051\n",
      "Epoch 678/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0041 - acc: 0.9981 - val_loss: 3.8576 - val_acc: 0.7308\n",
      "Epoch 679/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0112 - acc: 0.9985 - val_loss: 4.2491 - val_acc: 0.6939\n",
      "Epoch 680/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0078 - acc: 0.9989 - val_loss: 4.2576 - val_acc: 0.7019\n",
      "Epoch 681/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 3.7345 - val_acc: 0.7372\n",
      "Epoch 682/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 3.8290 - val_acc: 0.7260\n",
      "Epoch 683/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 3.4132 - val_acc: 0.7484\n",
      "Epoch 684/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0095 - acc: 0.9981 - val_loss: 4.5159 - val_acc: 0.6859\n",
      "Epoch 685/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0093 - acc: 0.9966 - val_loss: 4.2482 - val_acc: 0.6955\n",
      "Epoch 686/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 4.0478 - val_acc: 0.7099\n",
      "Epoch 687/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 4.0956 - val_acc: 0.7051\n",
      "Epoch 688/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0103 - acc: 0.9981 - val_loss: 4.1841 - val_acc: 0.7035\n",
      "Epoch 689/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 4.0717 - val_acc: 0.7147\n",
      "Epoch 690/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0174 - acc: 0.9970 - val_loss: 3.8757 - val_acc: 0.7308\n",
      "Epoch 691/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0159 - acc: 0.9955 - val_loss: 4.3337 - val_acc: 0.6987\n",
      "Epoch 692/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0100 - acc: 0.9978 - val_loss: 4.1029 - val_acc: 0.7115\n",
      "Epoch 693/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0077 - acc: 0.9985 - val_loss: 3.6384 - val_acc: 0.7388\n",
      "Epoch 694/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0092 - acc: 0.9985 - val_loss: 4.1967 - val_acc: 0.6971\n",
      "Epoch 695/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 3.9827 - val_acc: 0.7228\n",
      "Epoch 696/1000\n",
      "2682/2682 [==============================] - 1s 260us/step - loss: 0.0104 - acc: 0.9981 - val_loss: 4.2190 - val_acc: 0.7019\n",
      "Epoch 697/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 3.9470 - val_acc: 0.7212\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0058 - acc: 0.9981 - val_loss: 4.2350 - val_acc: 0.7051\n",
      "Epoch 699/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0121 - acc: 0.9974 - val_loss: 4.1983 - val_acc: 0.7035\n",
      "Epoch 700/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0065 - acc: 0.9989 - val_loss: 4.1058 - val_acc: 0.7051\n",
      "Epoch 701/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0036 - acc: 0.9978 - val_loss: 4.5693 - val_acc: 0.6907\n",
      "Epoch 702/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 3.9426 - val_acc: 0.7244\n",
      "Epoch 703/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0052 - acc: 0.9985 - val_loss: 4.0954 - val_acc: 0.7115\n",
      "Epoch 704/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0094 - acc: 0.9981 - val_loss: 4.6621 - val_acc: 0.6779\n",
      "Epoch 705/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 3.7097 - val_acc: 0.7308\n",
      "Epoch 706/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0089 - acc: 0.9978 - val_loss: 3.7141 - val_acc: 0.7356\n",
      "Epoch 707/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0151 - acc: 0.9978 - val_loss: 4.1689 - val_acc: 0.7051\n",
      "Epoch 708/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 4.0409 - val_acc: 0.7163\n",
      "Epoch 709/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0079 - acc: 0.9985 - val_loss: 3.8488 - val_acc: 0.7212\n",
      "Epoch 710/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0089 - acc: 0.9979 - val_loss: 3.9315 - val_acc: 0.7131\n",
      "Epoch 711/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0040 - acc: 0.9981 - val_loss: 4.0763 - val_acc: 0.7067\n",
      "Epoch 712/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 3.8988 - val_acc: 0.7212\n",
      "Epoch 713/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0027 - acc: 0.9989 - val_loss: 4.2503 - val_acc: 0.7035\n",
      "Epoch 714/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0090 - acc: 0.9978 - val_loss: 3.7325 - val_acc: 0.7340\n",
      "Epoch 715/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 3.6401 - val_acc: 0.7340\n",
      "Epoch 716/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 4.0871 - val_acc: 0.7131\n",
      "Epoch 717/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0124 - acc: 0.9966 - val_loss: 4.0930 - val_acc: 0.7163\n",
      "Epoch 718/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0033 - acc: 0.9985 - val_loss: 4.0548 - val_acc: 0.7163\n",
      "Epoch 719/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0068 - acc: 0.9970 - val_loss: 3.9972 - val_acc: 0.7179\n",
      "Epoch 720/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 4.2796 - val_acc: 0.6971\n",
      "Epoch 721/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0163 - acc: 0.9981 - val_loss: 3.7998 - val_acc: 0.7356\n",
      "Epoch 722/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0119 - acc: 0.9981 - val_loss: 4.0051 - val_acc: 0.7196\n",
      "Epoch 723/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0092 - acc: 0.9979 - val_loss: 3.8187 - val_acc: 0.7292\n",
      "Epoch 724/1000\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 3.8258 - val_acc: 0.7260\n",
      "Epoch 725/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0110 - acc: 0.9959 - val_loss: 3.9176 - val_acc: 0.7163\n",
      "Epoch 726/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.6738 - val_acc: 0.7364\n",
      "Epoch 727/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 4.1343 - val_acc: 0.7051\n",
      "Epoch 728/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 3.6378 - val_acc: 0.7356\n",
      "Epoch 729/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0109 - acc: 0.9978 - val_loss: 4.2440 - val_acc: 0.6955\n",
      "Epoch 730/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 3.5811 - val_acc: 0.7468\n",
      "Epoch 731/1000\n",
      "2682/2682 [==============================] - 1s 266us/step - loss: 0.0096 - acc: 0.9978 - val_loss: 4.0908 - val_acc: 0.7115\n",
      "Epoch 732/1000\n",
      "2682/2682 [==============================] - 1s 265us/step - loss: 0.0144 - acc: 0.9974 - val_loss: 3.8181 - val_acc: 0.7260\n",
      "Epoch 733/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0085 - acc: 0.9989 - val_loss: 3.7795 - val_acc: 0.7372\n",
      "Epoch 734/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 3.9714 - val_acc: 0.7179\n",
      "Epoch 735/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 3.9801 - val_acc: 0.7196\n",
      "Epoch 736/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0052 - acc: 0.9985 - val_loss: 3.9952 - val_acc: 0.7196\n",
      "Epoch 737/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 4.3780 - val_acc: 0.6939\n",
      "Epoch 738/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0108 - acc: 0.9974 - val_loss: 3.7776 - val_acc: 0.7308\n",
      "Epoch 739/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 4.1728 - val_acc: 0.6987\n",
      "Epoch 740/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0088 - acc: 0.9978 - val_loss: 3.7852 - val_acc: 0.7292\n",
      "Epoch 741/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 4.0010 - val_acc: 0.7163\n",
      "Epoch 742/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 4.3304 - val_acc: 0.7003\n",
      "Epoch 743/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 3.8206 - val_acc: 0.7372\n",
      "Epoch 744/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 3.9038 - val_acc: 0.7260\n",
      "Epoch 745/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 3.9675 - val_acc: 0.7244\n",
      "Epoch 746/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0086 - acc: 0.9966 - val_loss: 4.0930 - val_acc: 0.7035\n",
      "Epoch 747/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 3.9978 - val_acc: 0.7163\n",
      "Epoch 748/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0122 - acc: 0.9981 - val_loss: 3.9257 - val_acc: 0.7244\n",
      "Epoch 749/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 3.8745 - val_acc: 0.7260\n",
      "Epoch 750/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0075 - acc: 0.9970 - val_loss: 4.1749 - val_acc: 0.6987\n",
      "Epoch 751/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0021 - acc: 0.9989 - val_loss: 3.5419 - val_acc: 0.7436\n",
      "Epoch 752/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 4.2158 - val_acc: 0.7051\n",
      "Epoch 753/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0120 - acc: 0.9974 - val_loss: 3.5805 - val_acc: 0.7468\n",
      "Epoch 754/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 4.3986 - val_acc: 0.6955\n",
      "Epoch 755/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0142 - acc: 0.9978 - val_loss: 3.7932 - val_acc: 0.7228\n",
      "Epoch 756/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0058 - acc: 0.9974 - val_loss: 3.9476 - val_acc: 0.7212\n",
      "Epoch 757/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 4.1479 - val_acc: 0.7107\n",
      "Epoch 758/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0076 - acc: 0.9989 - val_loss: 3.9061 - val_acc: 0.7212\n",
      "Epoch 759/1000\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 3.8705 - val_acc: 0.7260\n",
      "Epoch 760/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 3.9195 - val_acc: 0.7196\n",
      "Epoch 761/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0085 - acc: 0.9981 - val_loss: 3.6529 - val_acc: 0.7420\n",
      "Epoch 762/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 3.7606 - val_acc: 0.7388\n",
      "Epoch 763/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 3.5123 - val_acc: 0.7452\n",
      "Epoch 764/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 3.8422 - val_acc: 0.7308\n",
      "Epoch 765/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0024 - acc: 0.9989 - val_loss: 4.3160 - val_acc: 0.6939\n",
      "Epoch 766/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 3.7526 - val_acc: 0.7324\n",
      "Epoch 767/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0011 - acc: 0.9993 - val_loss: 4.1557 - val_acc: 0.7115\n",
      "Epoch 768/1000\n",
      "2682/2682 [==============================] - 1s 262us/step - loss: 0.0058 - acc: 0.9978 - val_loss: 4.2444 - val_acc: 0.7067\n",
      "Epoch 769/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0128 - acc: 0.9972 - val_loss: 4.0367 - val_acc: 0.7196\n",
      "Epoch 770/1000\n",
      "2682/2682 [==============================] - 1s 264us/step - loss: 0.0176 - acc: 0.9968 - val_loss: 3.8343 - val_acc: 0.7316\n",
      "Epoch 771/1000\n",
      "2682/2682 [==============================] - 1s 263us/step - loss: 0.0080 - acc: 0.9981 - val_loss: 4.2284 - val_acc: 0.7051\n",
      "Epoch 772/1000\n",
      "2682/2682 [==============================] - 1s 261us/step - loss: 0.0034 - acc: 0.9985 - val_loss: 4.2552 - val_acc: 0.7003\n",
      "Epoch 773/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0055 - acc: 0.9978 - val_loss: 4.1891 - val_acc: 0.7083\n",
      "Epoch 774/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0081 - acc: 0.9981 - val_loss: 4.1219 - val_acc: 0.7099\n",
      "Epoch 775/1000\n",
      "2682/2682 [==============================] - 1s 292us/step - loss: 0.0052 - acc: 0.9993 - val_loss: 4.0382 - val_acc: 0.7212\n",
      "Epoch 776/1000\n",
      "2682/2682 [==============================] - 1s 267us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 4.0391 - val_acc: 0.7131\n",
      "Epoch 777/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 3.7917 - val_acc: 0.7388\n",
      "Epoch 778/1000\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.0145 - acc: 0.9974 - val_loss: 3.8232 - val_acc: 0.7388\n",
      "Epoch 779/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0031 - acc: 0.9985 - val_loss: 3.8352 - val_acc: 0.7244\n",
      "Epoch 780/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 3.6499 - val_acc: 0.7420\n",
      "Epoch 781/1000\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 3.9345 - val_acc: 0.7212\n",
      "Epoch 782/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 4.6277 - val_acc: 0.6779\n",
      "Epoch 783/1000\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 3.9022 - val_acc: 0.7260\n",
      "Epoch 784/1000\n",
      "2682/2682 [==============================] - 1s 296us/step - loss: 0.0087 - acc: 0.9974 - val_loss: 3.8092 - val_acc: 0.7300\n",
      "Epoch 785/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0086 - acc: 0.9985 - val_loss: 3.9010 - val_acc: 0.7292\n",
      "Epoch 786/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 4.0508 - val_acc: 0.7131\n",
      "Epoch 787/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0086 - acc: 0.9978 - val_loss: 4.1355 - val_acc: 0.7035\n",
      "Epoch 788/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 3.8772 - val_acc: 0.7308\n",
      "Epoch 789/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0080 - acc: 0.9985 - val_loss: 4.1634 - val_acc: 0.7003\n",
      "Epoch 790/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0025 - acc: 0.9989 - val_loss: 4.0953 - val_acc: 0.7099\n",
      "Epoch 791/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 3.8889 - val_acc: 0.7292\n",
      "Epoch 792/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 4.4617 - val_acc: 0.6939\n",
      "Epoch 793/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 3.7679 - val_acc: 0.7292\n",
      "Epoch 794/1000\n",
      "2682/2682 [==============================] - 1s 297us/step - loss: 0.0077 - acc: 0.9979 - val_loss: 3.5579 - val_acc: 0.7468\n",
      "Epoch 795/1000\n",
      "2682/2682 [==============================] - 1s 363us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 4.1329 - val_acc: 0.7123\n",
      "Epoch 796/1000\n",
      "2682/2682 [==============================] - 1s 350us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 3.7590 - val_acc: 0.7420\n",
      "Epoch 797/1000\n",
      "2682/2682 [==============================] - 1s 352us/step - loss: 0.0169 - acc: 0.9970 - val_loss: 3.8769 - val_acc: 0.7212\n",
      "Epoch 798/1000\n",
      "2682/2682 [==============================] - 1s 348us/step - loss: 0.0081 - acc: 0.9989 - val_loss: 4.3981 - val_acc: 0.6971\n",
      "Epoch 799/1000\n",
      "2682/2682 [==============================] - 1s 347us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 4.8494 - val_acc: 0.6715\n",
      "Epoch 800/1000\n",
      "2682/2682 [==============================] - 1s 350us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 3.8343 - val_acc: 0.7292\n",
      "Epoch 801/1000\n",
      "2682/2682 [==============================] - 1s 351us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 3.8581 - val_acc: 0.7244\n",
      "Epoch 802/1000\n",
      "2682/2682 [==============================] - 1s 295us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 3.7226 - val_acc: 0.7436\n",
      "Epoch 803/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 4.3260 - val_acc: 0.6971\n",
      "Epoch 804/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0061 - acc: 0.9978 - val_loss: 4.1657 - val_acc: 0.7051\n",
      "Epoch 805/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 3.8467 - val_acc: 0.7356\n",
      "Epoch 806/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0211 - acc: 0.9952 - val_loss: 3.9192 - val_acc: 0.7260\n",
      "Epoch 807/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 4.0822 - val_acc: 0.7083\n",
      "Epoch 808/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0027 - acc: 0.9989 - val_loss: 4.1967 - val_acc: 0.7019\n",
      "Epoch 809/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 7.9429e-04 - acc: 0.9996 - val_loss: 3.8478 - val_acc: 0.7324\n",
      "Epoch 810/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0031 - acc: 0.9985 - val_loss: 4.0043 - val_acc: 0.7179\n",
      "Epoch 811/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0020 - acc: 0.9989 - val_loss: 3.7904 - val_acc: 0.7356\n",
      "Epoch 812/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 4.1874 - val_acc: 0.7019\n",
      "Epoch 813/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0102 - acc: 0.9978 - val_loss: 3.8968 - val_acc: 0.7308\n",
      "Epoch 814/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 4.0138 - val_acc: 0.7163\n",
      "Epoch 815/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 4.1217 - val_acc: 0.7099\n",
      "Epoch 816/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0044 - acc: 0.9981 - val_loss: 3.8743 - val_acc: 0.7372\n",
      "Epoch 817/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.8075 - val_acc: 0.7404\n",
      "Epoch 818/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0076 - acc: 0.9981 - val_loss: 4.5698 - val_acc: 0.6859\n",
      "Epoch 819/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 3.8038 - val_acc: 0.7324\n",
      "Epoch 820/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 3.9867 - val_acc: 0.7196\n",
      "Epoch 821/1000\n",
      "2682/2682 [==============================] - 1s 305us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 3.8160 - val_acc: 0.7340\n",
      "Epoch 822/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0128 - acc: 0.9978 - val_loss: 3.7560 - val_acc: 0.7388\n",
      "Epoch 823/1000\n",
      "2682/2682 [==============================] - 1s 295us/step - loss: 0.0097 - acc: 0.9981 - val_loss: 4.2841 - val_acc: 0.6987\n",
      "Epoch 824/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 4.4575 - val_acc: 0.6955\n",
      "Epoch 825/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0170 - acc: 0.9974 - val_loss: 3.8779 - val_acc: 0.7276\n",
      "Epoch 826/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0070 - acc: 0.9993 - val_loss: 4.0716 - val_acc: 0.7147\n",
      "Epoch 827/1000\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 4.4994 - val_acc: 0.6907\n",
      "Epoch 828/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0141 - acc: 0.9974 - val_loss: 4.0841 - val_acc: 0.7099\n",
      "Epoch 829/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 3.6378 - val_acc: 0.7404\n",
      "Epoch 830/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0024 - acc: 0.9989 - val_loss: 4.0383 - val_acc: 0.7147\n",
      "Epoch 831/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0050 - acc: 0.9981 - val_loss: 4.0926 - val_acc: 0.7115\n",
      "Epoch 832/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 3.9859 - val_acc: 0.7212\n",
      "Epoch 833/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0024 - acc: 0.9989 - val_loss: 4.1641 - val_acc: 0.7083\n",
      "Epoch 834/1000\n",
      "2682/2682 [==============================] - 1s 286us/step - loss: 0.0054 - acc: 0.9978 - val_loss: 3.8899 - val_acc: 0.7260\n",
      "Epoch 835/1000\n",
      "2682/2682 [==============================] - 1s 298us/step - loss: 0.0077 - acc: 0.9981 - val_loss: 3.9986 - val_acc: 0.7179\n",
      "Epoch 836/1000\n",
      "2682/2682 [==============================] - 1s 317us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 4.0850 - val_acc: 0.7147\n",
      "Epoch 837/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0035 - acc: 0.9996 - val_loss: 4.0401 - val_acc: 0.7196\n",
      "Epoch 838/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0069 - acc: 0.9981 - val_loss: 3.9092 - val_acc: 0.7228\n",
      "Epoch 839/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 3.9899 - val_acc: 0.7212\n",
      "Epoch 840/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0041 - acc: 0.9993 - val_loss: 3.8740 - val_acc: 0.7292\n",
      "Epoch 841/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0051 - acc: 0.9978 - val_loss: 3.6651 - val_acc: 0.7436\n",
      "Epoch 842/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 4.0783 - val_acc: 0.7019\n",
      "Epoch 843/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0116 - acc: 0.9978 - val_loss: 4.3751 - val_acc: 0.6987\n",
      "Epoch 844/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 3.8228 - val_acc: 0.7388\n",
      "Epoch 845/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 4.1445 - val_acc: 0.7099\n",
      "Epoch 846/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0096 - acc: 0.9985 - val_loss: 4.0225 - val_acc: 0.7131\n",
      "Epoch 847/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0081 - acc: 0.9978 - val_loss: 4.2331 - val_acc: 0.7003\n",
      "Epoch 848/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 3.8253 - val_acc: 0.7292\n",
      "Epoch 849/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 3.8490 - val_acc: 0.7356\n",
      "Epoch 850/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0069 - acc: 0.9981 - val_loss: 4.0748 - val_acc: 0.7147\n",
      "Epoch 851/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0122 - acc: 0.9978 - val_loss: 3.8801 - val_acc: 0.7260\n",
      "Epoch 852/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 4.0533 - val_acc: 0.7131\n",
      "Epoch 853/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 4.2282e-04 - acc: 1.0000 - val_loss: 4.0861 - val_acc: 0.7099\n",
      "Epoch 854/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 3.6813 - val_acc: 0.7388\n",
      "Epoch 855/1000\n",
      "2682/2682 [==============================] - 1s 281us/step - loss: 0.0074 - acc: 0.9989 - val_loss: 3.9024 - val_acc: 0.7284\n",
      "Epoch 856/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0091 - acc: 0.9985 - val_loss: 3.8679 - val_acc: 0.7356\n",
      "Epoch 857/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0054 - acc: 0.9985 - val_loss: 3.7977 - val_acc: 0.7356\n",
      "Epoch 858/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0094 - acc: 0.9978 - val_loss: 3.8453 - val_acc: 0.7388\n",
      "Epoch 859/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0072 - acc: 0.9985 - val_loss: 3.9532 - val_acc: 0.7228\n",
      "Epoch 860/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0032 - acc: 0.9996 - val_loss: 3.9837 - val_acc: 0.7212\n",
      "Epoch 861/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 3.9463 - val_acc: 0.7260\n",
      "Epoch 862/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0047 - acc: 0.9989 - val_loss: 4.0895 - val_acc: 0.7067\n",
      "Epoch 863/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0060 - acc: 0.9993 - val_loss: 4.1592 - val_acc: 0.7083\n",
      "Epoch 864/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 3.9474 - val_acc: 0.7228\n",
      "Epoch 865/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 3.7168 - val_acc: 0.7340\n",
      "Epoch 866/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0067 - acc: 0.9989 - val_loss: 3.8121 - val_acc: 0.7340\n",
      "Epoch 867/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0098 - acc: 0.9978 - val_loss: 4.0471 - val_acc: 0.7083\n",
      "Epoch 868/1000\n",
      "2682/2682 [==============================] - 1s 289us/step - loss: 0.0013 - acc: 0.9993 - val_loss: 3.7140 - val_acc: 0.7404\n",
      "Epoch 869/1000\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.0037 - acc: 0.9981 - val_loss: 4.1262 - val_acc: 0.7099\n",
      "Epoch 870/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0061 - acc: 0.9970 - val_loss: 4.0550 - val_acc: 0.7147\n",
      "Epoch 871/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0017 - acc: 0.9989 - val_loss: 4.1713 - val_acc: 0.7067\n",
      "Epoch 872/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0151 - acc: 0.9974 - val_loss: 3.9476 - val_acc: 0.7212\n",
      "Epoch 873/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0080 - acc: 0.9989 - val_loss: 3.9280 - val_acc: 0.7340\n",
      "Epoch 874/1000\n",
      "2682/2682 [==============================] - 1s 288us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 4.0020 - val_acc: 0.7147\n",
      "Epoch 875/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0010 - acc: 0.9996 - val_loss: 3.9722 - val_acc: 0.7179\n",
      "Epoch 876/1000\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.0091 - acc: 0.9978 - val_loss: 3.7723 - val_acc: 0.7420\n",
      "Epoch 877/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 4.4420 - val_acc: 0.6971\n",
      "Epoch 878/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.0056 - val_acc: 0.7179\n",
      "Epoch 879/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 7.7192e-04 - acc: 0.9996 - val_loss: 4.0570 - val_acc: 0.7163\n",
      "Epoch 880/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 4.2257 - val_acc: 0.7035\n",
      "Epoch 881/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 4.1182 - val_acc: 0.7067\n",
      "Epoch 882/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 3.8094 - val_acc: 0.7388\n",
      "Epoch 883/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0039 - acc: 0.9989 - val_loss: 4.0486 - val_acc: 0.7131\n",
      "Epoch 884/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0100 - acc: 0.9981 - val_loss: 4.3058 - val_acc: 0.7019\n",
      "Epoch 885/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0101 - acc: 0.9985 - val_loss: 4.0007 - val_acc: 0.7179\n",
      "Epoch 886/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0098 - acc: 0.9981 - val_loss: 3.8699 - val_acc: 0.7276\n",
      "Epoch 887/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0064 - acc: 0.9981 - val_loss: 3.9720 - val_acc: 0.7260\n",
      "Epoch 888/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.7444 - val_acc: 0.7372\n",
      "Epoch 889/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 4.2176 - val_acc: 0.7115\n",
      "Epoch 890/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 3.9159 - val_acc: 0.7292\n",
      "Epoch 891/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 4.0105 - val_acc: 0.7179\n",
      "Epoch 892/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.9942 - val_acc: 0.7196\n",
      "Epoch 893/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0014 - acc: 0.9993 - val_loss: 3.7825 - val_acc: 0.7372\n",
      "Epoch 894/1000\n",
      "2682/2682 [==============================] - 1s 268us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 4.1733 - val_acc: 0.7067\n",
      "Epoch 895/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0029 - acc: 0.9989 - val_loss: 3.8568 - val_acc: 0.7388\n",
      "Epoch 896/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 4.4050 - val_acc: 0.6939\n",
      "Epoch 897/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 4.2601 - val_acc: 0.7019\n",
      "Epoch 898/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0115 - acc: 0.9974 - val_loss: 3.9948 - val_acc: 0.7212\n",
      "Epoch 899/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0119 - acc: 0.9974 - val_loss: 3.8993 - val_acc: 0.7244\n",
      "Epoch 900/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0077 - acc: 0.9985 - val_loss: 4.0171 - val_acc: 0.7212\n",
      "Epoch 901/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0091 - acc: 0.9981 - val_loss: 3.9811 - val_acc: 0.7260\n",
      "Epoch 902/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 3.8964 - val_acc: 0.7324\n",
      "Epoch 903/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0093 - acc: 0.9978 - val_loss: 3.6847 - val_acc: 0.7420\n",
      "Epoch 904/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 3.7887 - val_acc: 0.7420\n",
      "Epoch 905/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0077 - acc: 0.9981 - val_loss: 3.8840 - val_acc: 0.7372\n",
      "Epoch 906/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 4.1796 - val_acc: 0.7051\n",
      "Epoch 907/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0062 - acc: 0.9974 - val_loss: 4.2008 - val_acc: 0.7083\n",
      "Epoch 908/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0018 - acc: 0.9989 - val_loss: 3.8293 - val_acc: 0.7356\n",
      "Epoch 909/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 4.0285 - val_acc: 0.7196\n",
      "Epoch 910/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0094 - acc: 0.9985 - val_loss: 3.7320 - val_acc: 0.7388\n",
      "Epoch 911/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 4.0723 - val_acc: 0.7179\n",
      "Epoch 912/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 4.4407 - val_acc: 0.6923\n",
      "Epoch 913/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0039 - acc: 0.9978 - val_loss: 4.2071 - val_acc: 0.7067\n",
      "Epoch 914/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 3.8362 - val_acc: 0.7340\n",
      "Epoch 915/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0102 - acc: 0.9974 - val_loss: 4.0903 - val_acc: 0.7163\n",
      "Epoch 916/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 3.9565 - val_acc: 0.7260\n",
      "Epoch 917/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 4.4684 - val_acc: 0.6939\n",
      "Epoch 918/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 3.8025 - val_acc: 0.7356\n",
      "Epoch 919/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 3.8812 - val_acc: 0.7276\n",
      "Epoch 920/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0011 - acc: 0.9993 - val_loss: 4.1124 - val_acc: 0.7067\n",
      "Epoch 921/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0188 - acc: 0.9963 - val_loss: 3.7537 - val_acc: 0.7436\n",
      "Epoch 922/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0027 - acc: 0.9989 - val_loss: 3.9477 - val_acc: 0.7244\n",
      "Epoch 923/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 4.2358 - val_acc: 0.7019\n",
      "Epoch 924/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0098 - acc: 0.9981 - val_loss: 4.0245 - val_acc: 0.7196\n",
      "Epoch 925/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0066 - acc: 0.9993 - val_loss: 3.8545 - val_acc: 0.7340\n",
      "Epoch 926/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0064 - acc: 0.9985 - val_loss: 4.0311 - val_acc: 0.7196\n",
      "Epoch 927/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 3.9964 - val_acc: 0.7244\n",
      "Epoch 928/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 4.1351 - val_acc: 0.7131\n",
      "Epoch 929/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 9.5985e-04 - acc: 0.9996 - val_loss: 4.0171 - val_acc: 0.7179\n",
      "Epoch 930/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 4.1184 - val_acc: 0.7067\n",
      "Epoch 931/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0023 - acc: 0.9989 - val_loss: 4.1161 - val_acc: 0.7147\n",
      "Epoch 932/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0058 - acc: 0.9978 - val_loss: 3.7433 - val_acc: 0.7420\n",
      "Epoch 933/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0106 - acc: 0.9978 - val_loss: 3.8545 - val_acc: 0.7356\n",
      "Epoch 934/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 4.5892 - val_acc: 0.6763\n",
      "Epoch 935/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0043 - acc: 0.9989 - val_loss: 4.0441 - val_acc: 0.7163\n",
      "Epoch 936/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0052 - acc: 0.9978 - val_loss: 3.8648 - val_acc: 0.7324\n",
      "Epoch 937/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0066 - acc: 0.9993 - val_loss: 4.0875 - val_acc: 0.7115\n",
      "Epoch 938/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0044 - acc: 0.9989 - val_loss: 4.1020 - val_acc: 0.7115\n",
      "Epoch 939/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 4.0105 - val_acc: 0.7163\n",
      "Epoch 940/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0064 - acc: 0.9985 - val_loss: 3.9210 - val_acc: 0.7276\n",
      "Epoch 941/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 3.7653 - val_acc: 0.7404\n",
      "Epoch 942/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0103 - acc: 0.9978 - val_loss: 3.8666 - val_acc: 0.7356\n",
      "Epoch 943/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 3.9237 - val_acc: 0.7260\n",
      "Epoch 944/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 4.2718 - val_acc: 0.6923\n",
      "Epoch 945/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0010 - acc: 0.9996 - val_loss: 4.4167 - val_acc: 0.6923\n",
      "Epoch 946/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0076 - acc: 0.9981 - val_loss: 3.9440 - val_acc: 0.7244\n",
      "Epoch 947/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 4.2347 - val_acc: 0.7035\n",
      "Epoch 948/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0038 - acc: 0.9985 - val_loss: 4.1335 - val_acc: 0.7147\n",
      "Epoch 949/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0024 - acc: 0.9996 - val_loss: 4.0826 - val_acc: 0.7131\n",
      "Epoch 950/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 3.9340 - val_acc: 0.7260\n",
      "Epoch 951/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0028 - acc: 0.9989 - val_loss: 4.0518 - val_acc: 0.7067\n",
      "Epoch 952/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0027 - acc: 0.9996 - val_loss: 4.3229 - val_acc: 0.7035\n",
      "Epoch 953/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0057 - acc: 0.9974 - val_loss: 3.8441 - val_acc: 0.7292\n",
      "Epoch 954/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0082 - acc: 0.9985 - val_loss: 4.0702 - val_acc: 0.7179\n",
      "Epoch 955/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0044 - acc: 0.9989 - val_loss: 3.9739 - val_acc: 0.7228\n",
      "Epoch 956/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 3.9675 - val_acc: 0.7244\n",
      "Epoch 957/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 3.9501 - val_acc: 0.7276\n",
      "Epoch 958/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0094 - acc: 0.9978 - val_loss: 4.1537 - val_acc: 0.7147\n",
      "Epoch 959/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0028 - acc: 0.9993 - val_loss: 3.9429 - val_acc: 0.7244\n",
      "Epoch 960/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 3.9131 - val_acc: 0.7340\n",
      "Epoch 961/1000\n",
      "2682/2682 [==============================] - 1s 270us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 4.0073 - val_acc: 0.7179\n",
      "Epoch 962/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 3.9583 - val_acc: 0.7260\n",
      "Epoch 963/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0048 - acc: 0.9978 - val_loss: 3.9118 - val_acc: 0.7276\n",
      "Epoch 964/1000\n",
      "2682/2682 [==============================] - 1s 269us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 3.8369 - val_acc: 0.7356\n",
      "Epoch 965/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0014 - acc: 0.9993 - val_loss: 4.0049 - val_acc: 0.7212\n",
      "Epoch 966/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 3.8119 - val_acc: 0.7404\n",
      "Epoch 967/1000\n",
      "2682/2682 [==============================] - 1s 290us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 4.2258 - val_acc: 0.7083\n",
      "Epoch 968/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0137 - acc: 0.9978 - val_loss: 4.1972 - val_acc: 0.7099\n",
      "Epoch 969/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0034 - acc: 0.9985 - val_loss: 4.3862 - val_acc: 0.7003\n",
      "Epoch 970/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0055 - acc: 0.9974 - val_loss: 4.1304 - val_acc: 0.7147\n",
      "Epoch 971/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0101 - acc: 0.9985 - val_loss: 4.0014 - val_acc: 0.7196\n",
      "Epoch 972/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 3.9301 - val_acc: 0.7244\n",
      "Epoch 973/1000\n",
      "2682/2682 [==============================] - 1s 299us/step - loss: 0.0061 - acc: 0.9985 - val_loss: 3.7835 - val_acc: 0.7372\n",
      "Epoch 974/1000\n",
      "2682/2682 [==============================] - 1s 282us/step - loss: 0.0016 - acc: 0.9993 - val_loss: 3.9767 - val_acc: 0.7155\n",
      "Epoch 975/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 4.1565 - val_acc: 0.7131\n",
      "Epoch 976/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0057 - acc: 0.9985 - val_loss: 4.0893 - val_acc: 0.7131\n",
      "Epoch 977/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 3.9055 - val_acc: 0.7308\n",
      "Epoch 978/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0085 - acc: 0.9985 - val_loss: 3.7569 - val_acc: 0.7388\n",
      "Epoch 979/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0027 - acc: 0.9989 - val_loss: 3.8333 - val_acc: 0.7324\n",
      "Epoch 980/1000\n",
      "2682/2682 [==============================] - 1s 274us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 3.9972 - val_acc: 0.7212\n",
      "Epoch 981/1000\n",
      "2682/2682 [==============================] - 1s 277us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 3.9430 - val_acc: 0.7196\n",
      "Epoch 982/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0039 - acc: 0.9985 - val_loss: 3.9820 - val_acc: 0.7260\n",
      "Epoch 983/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 8.0008e-05 - acc: 1.0000 - val_loss: 4.0085 - val_acc: 0.7212\n",
      "Epoch 984/1000\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.0100 - acc: 0.9985 - val_loss: 4.3124 - val_acc: 0.7083\n",
      "Epoch 985/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 3.9830 - val_acc: 0.7228\n",
      "Epoch 986/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0108 - acc: 0.9974 - val_loss: 3.9991 - val_acc: 0.7212\n",
      "Epoch 987/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 4.3140 - val_acc: 0.7083\n",
      "Epoch 988/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 4.0156 - val_acc: 0.7212\n",
      "Epoch 989/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0061 - acc: 0.9993 - val_loss: 4.5863 - val_acc: 0.6907\n",
      "Epoch 990/1000\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.0113 - acc: 0.9981 - val_loss: 4.0568 - val_acc: 0.7163\n",
      "Epoch 991/1000\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 4.2053 - val_acc: 0.7115\n",
      "Epoch 992/1000\n",
      "2682/2682 [==============================] - 1s 271us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 3.9434 - val_acc: 0.7228\n",
      "Epoch 993/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 3.7346 - val_acc: 0.7420\n",
      "Epoch 994/1000\n",
      "2682/2682 [==============================] - 1s 279us/step - loss: 0.0067 - acc: 0.9981 - val_loss: 4.0642 - val_acc: 0.7131\n",
      "Epoch 995/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0074 - acc: 0.9989 - val_loss: 3.7730 - val_acc: 0.7372\n",
      "Epoch 996/1000\n",
      "2682/2682 [==============================] - 1s 278us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 3.9150 - val_acc: 0.7260\n",
      "Epoch 997/1000\n",
      "2682/2682 [==============================] - 1s 272us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 4.4071 - val_acc: 0.6955\n",
      "Epoch 998/1000\n",
      "2682/2682 [==============================] - 1s 276us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 4.2796 - val_acc: 0.6955\n",
      "Epoch 999/1000\n",
      "2682/2682 [==============================] - 1s 273us/step - loss: 0.0012 - acc: 0.9993 - val_loss: 4.0460 - val_acc: 0.7179\n",
      "Epoch 1000/1000\n",
      "2682/2682 [==============================] - 1s 275us/step - loss: 0.0064 - acc: 0.9974 - val_loss: 4.2768 - val_acc: 0.7019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x19ff5672668>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,\n",
    "         y_train_array_k,\n",
    "         epochs=1000,\n",
    "         batch_size=batch_size,\n",
    "         validation_data=(test_features, y_test_k),\n",
    "         callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.22      0.36       234\n",
      "          1       0.68      0.99      0.81       390\n",
      "\n",
      "avg / total       0.77      0.70      0.64       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAELCAYAAADnUlzVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHqpJREFUeJzt3Xm8XPP9x/HX+97sImJJLInaU3vtTenPEmr7tVSjRX+t2poqqqK60IWq2lr8ilJRIloUtZT+VAUhxBJJLIk9FVF7gwSRhCSf3x/nXEbc5czcmXvmTN7PPM7jnnPmzDmfycz93O98z3dRRGBmZl2vKe8AzMyWVk7AZmY5cQI2M8uJE7CZWU6cgM3McuIEbGaWEydgM7OcOAGbmeXECdjMLCfdan2BGbPmu6udfcLFE1/IOwSrQ6fuOUSdPUfvzY/KnHPmPXx+p6/XGTVPwGZmXUrF+WLvBGxmjUW5FmrL4gRsZo3FJWAzs5y4BGxmlpOm5rwjyMwJ2Mwai6sgzMxy4ioIM7OcuARsZpYTl4DNzHLiErCZWU7cCsLMLCcuAZuZ5aTJdcBmZvlwCdjMLCduBWFmlhPfhDMzy4mrIMzMcuIqCDOznLgEbGaWE5eAzcxy4hKwmVlO3ArCzCwnLgGbmeXEdcBmZjlxCdjMLCcuAZuZ5cQlYDOzfKjJCdjMLBdyFYSZWU6Kk3+dgM2ssbgEbGaWEydgM7OcNBXoJlxxIjUzy0JlLO2dRuolaaKkRyU9LumX6f4rJD0taZqkSyV1T/dL0rmSpkt6TNIWHYXqBGxmDUVS5qUDC4BhEfEZYDNgd0lDgSuA9YFNgN7AYenxewDrpcsI4MKOLuAqCDNrKNWqA46IAN5NN7unS0TELSXXmggMTjf3Bi5Pn/eApP6SVo2IV9q6hkvAZtZQyikBSxohaVLJMmKJczVLegR4HRgbEQ+WPNYd+CZwa7prEPDvkqe/mO5rk0vAZtZQyikBR8QoYFQ7jy8CNpPUH7hB0sYRMS19+AJgfETc03Lp1k7R3vWdgM2soaip+s3QImK2pLuA3YFpkk4EBgDfKTnsRWD1ku3BwMvtnddVEGbWUKp1E07SgLTki6TewC7AU5IOA3YDDoiIxSVPuQk4MG0NMRSY0179L7gEbGYNpoodMVYFxkhqJimsXhMRf5e0EJgJ3J9e6/qIOBm4BdgTmA68Bxzc0QWcgM2ssVQp/0bEY8DmrexvNW+mrR+OLOcaTsBm1lDcFdnMLCdOwGZmOSnSWBBOwGbWWIpTAHYCNrPG4ioIM7OcOAGbmeXECdgAOHD4HvTp04empmaam5s579KruPj8s3lwwt10696d1QYN5tgTTqbvsv3yDtVqaNJVv+PVJx6iZ9/l+MKPfw/A7Jee4+FrL2DRB++jpmY23/e7rLDGEF6YfBdP33EdAN169mLzfY+g/6C18gy/cGrRFblWnIBr7Izz/shy/Zf/cHuLrYdyyOFH09ytG5dccA5X/+kSDj1iZI4RWq2tsc3OrPP5/2bSled8uG/qTaPZYLf9WWWDrXjliUlMvXk0Oxx1GsussDI7HHUaPfr05dUnJzHlmvMZNvKsHKMvniKVgIvTXqNBbPnZbWnulvzdW3+jTZn1+us5R2S1NmCdjemxzLIf3ynxwfx5ACycP5fey60AwIprbUCPPn0BWGGN9Zk3Z1aXxtoIqjgge821WQKW9A6tD6Umkl53/t7cAQlOGHk4kthz733Zc+99P/b4bf93I9vvvFtO0VmePrPPt7n3D79g6k2XErGYHY/+zSeOef7B21hl/S1ziK7Y6iGxZtVmAo6IZdt6rCPpoMYjAH591vkccOChlZ6q0M6+cAwrDhjI7Lfe4PhjDmf1NdZik82SX6irxlxMc3Mzw3b975yjtDw8N+EWPvPlwxj0me148eF7mPyXc9n+iFM+fPz1Zx/j+QfGssPRZ+QYZUEVJ/9mr4KQNFDSp1qW9o6NiFERsVVEbLW0Jl+AFQcMBKD/8iuy7fbDePqJZBznsbfcxIMTxvOjE08r1F9rq56ZD93JaptuC8CgzT7PWy888+Fjc16ewZSrz+Nzh/6Mnsv4i2a5ilQF0WEClrSXpGeBGcDdwPPAP2ocV+HNn/ce782d++H6lIn3s+ba6zLpgQlce8VoTjrjd/Tq1TvnKC0vvfutwKx/JX+Q//PsY/QdsBoA7731OvePPo2t/+dYlh3Y7mw21oamJmVe8palFcSvgKHA7RGxuaSdgANqG1bxvfXmm5x8QtK6YdHChey0655sNXQ7Dv7aF/ngg/c54ZjDAVh/o004+kc/zzNUq7EHL/8Ns6ZPZcHct7nlpIPYYPevs8V+R/HoDRcTixfR1K0HW3ztKACe/OdfeH/u2zz812RCXTU1s/MPzmnv9LaEeijZZqVkCMt2DpAmRcRWkh4FNo+IxZImRsQ2WS4wY9b89i9gS6WLJ76QdwhWh07dc0ins+eQH92aOec8c+buuWbrLCXg2ZL6AuOBKyS9DiysbVhmZpUpUgk4y024vYF5wEiS6Zf/BXyplkGZmVVKyr7krcMScETMBZDUD7i55hGZmXVCc3MdZNaMOkzAkr4DnExSCl5M2hEDWLu2oZmZla9IVRBZ6oCPAzaKCPeJNLO6V6D8mykB/4tkimUzs7rXaCXg44H7JD0ILGjZGRFH1ywqM7MKNVoCvgi4E5hKUgdsZla3CpR/MyXghRFxbM0jMTOrgnroYpxVlgQ8Lh3d7GY+XgXxZs2iMjOrUKNVQXw9/Xl8yT43QzOzulSg/Nt+ApbUBHwjIiZ0UTxmZp1SpBJwu12RI2Ix8NsuisXMrNOK1BU5y1gQt0kariL9WTGzpVaRBmTPUgd8LLAMsEjSPDwnnJnVsYZqBdGZueHMzLpaHRRsM8tSAkbSXsD26eZdEfH32oVkZla5eqhayCrLaGinA1sDV6S7vi/p8xHxk5pGZmZWgQLl30wl4D2BzdIWEUgaAzwMOAGbWd1pqBJwqj/Q0vNtuRrFYmbWaQ11Ew44DXhY0jiSFhDb8/FecWZmdaNIJeAO2wFHxFUk09Jfny6fi4i/1DowM7NKVKsjhqTVJY2T9KSkxyV9f4nHj5MUklZKtyXpXEnTJT0maYuOYs1aBdEEzEqPHyJpSESMz/hcM7MuU8US8ELgBxExRdKywGRJYyPiCUmrA18AXig5fg9gvXT5LHBh+rNNWVpBnAHsBzzOR+MBB8k09WZmdaVa+TciXgFeSdffkfQkMAh4AjgH+BHwt5Kn7A1cHhEBPCCpv6RV0/O0KksJ+MvApyNiQYdHmpnlrKmMDJwOtTuiZNeoiBjVynFrApsDD6b9Il6KiEeXKG0PAv5dsv1iuq9TCfg5oDslYwGbmdWrclpBpMn2Ewm3lKS+wHXAMSTVEj8Fdm3t0NYu0d65syTg94BHJN2B54QzszpXzVZokrqTJN8rIuJ6SZsAawEtpd/BwBRJ25CUeFcvefpg4OX2zp8lAd+ULmZmda9aN+HSESAvAZ6MiLMBImIqMLDkmOeBrSJilqSbgKMk/YXk5tuc9up/IdtgPGMqfwlmZl2ris2AtwO+CUyV9Ei674SIuKWN428h6Tk8naTm4OCOLpC1GZqZWSGo1arY8kXEvbRer1t6zJol6wEcWc41nIDNrKEUqCeyE7CZNZaGGAtC0s2004QiIvaqSURmZp1QTjvgvLVXAvZknGZWOAXKv20n4Ii4uysDMTOrhiKNhpZlLIj1SIak3BDo1bI/ItauYVxmZhUpUP7NdBNuNHAiyeATO5G0bSvQSzSzpUlzgTJwh+MBA70j4g5AETEzIk4ChtU2LDOzykjKvOQtSwl4vqQm4FlJRwEvUdIVz8ysnhSoFVqmEvAxQB/gaGBLkq5536plUGZmlWqoEnBEPJSuvkuGvs1mZnmqg7yaWZZWEONopUNGRLge2MzqTj2UbLPKUgd8XMl6L2A4yaDEZmZ1p7lAlcBZqiAmL7FrgiR30jCzulSc9JutCmKFks0mkhtxq9QsIjOzTmiUsSBaTCapAxZJ1cMM4NBaBmVmVqkC5d9MCXiDiJhfukNSzxrFY2bWKUW6CZelHfB9rey7v9qBmJlVg5R9yVt74wGvQjKnfW9Jm/NR3XY/ko4ZZmZ1p1FaQewGHEQytfJZfJSA3wZOyHqBVfv36vggW+qc89Nz8w7B6tCpe57f6XMUqQqivfGAxwBjJA2PiOu6MCYzs4plqVetF1li3VJS/5YNSctLOqWGMZmZVaxIY0FkScB7RMTslo2IeAvYs3YhmZlVrknZl7xlaYbWLKlnRCwAkNQbcDM0M6tLjXITrsWfgTskjSbpkHEIcHlNozIzq1CB8m+msSDOlPQYsAtJS4hfRcQ/ax6ZmVkF6qBqN7MsJWAi4lbgVgBJ20n6fUQcWdPIzMwq0GhjQSBpM+AAYD+SsSCur2VQZmaVKlIztPZ6wg0B9idJvG8AV5NMzLlTF8VmZla2AhWA2y0BPwXcA3wpIqYDSBrZJVGZmVWoSK0g2iutDwdeBcZJuljSzhRrrGMzWwoVqR1wmwk4Im6IiP2A9YG7gJHAypIulLRrF8VnZlaWJinzkrcO66sjYm5EXBERXyQZmOcR4Cc1j8zMrAJFGo6yrBuGEfFmRFzkGZHNrF4VqQoiUzM0M7OiUIFuVTkBm1lD6VaghsAFCtXMrGPVHI5S0qWSXpc0bYn935P0tKTHJZ1Zsv94SdPTx3br6PwuAZtZQ6ly3e5lwPmUDEAmaSdgb2DTiFggaWC6f0OSzmsbAasBt0saEhGL2oy1qqGameWsmq0gImI88OYSu78LnN4yRG9EvJ7u3xv4S0QsiIgZwHRgm/bO7wRsZg2lnHbAkkZImlSyjMhwiSHAf0l6UNLdkrZO9w8C/l1y3Ivpvja5CsLMGkpzGcXKiBgFjCrzEt2A5YGhwNbANZLWpvWewtHRiczMGkZT7ZuhvQhcHxEBTJS0GFgp3b96yXGDgZfbO5GrIMysoXRBT7gbgWHJtTQE6AHMAm4C9pfUU9JawHrAxPZO5BKwmTWUaraCkHQVsCOwkqQXgROBS4FL06Zp7wPfSkvDj0u6BngCWAgc2V4LCHACNrMGU81BdiLigDYe+kYbx/8a+HXW8zsBm1lDqYdBdrJyAjazhlKkAdmdgM2soRSpZYETsJk1lCxjPNQLJ2AzayjFSb9OwGbWYOphqqGsnIDNrKEU6B6cE7CZNRbXAZuZ5cStIMzMcuISsJlZToqTfp2AzazBuARsZpaTZidgM7N8FCf9OgGbWYMpUAHYCdjMGksXTElUNU7AZtZQXAI2M8uJXAI2M8uHW0GYmeWkQPnXCdjMGosTsJlZTlwHbGaWE48HbGaWE8+IYWaWE1dB2CcsWrSIA742nIErr8z5F1yUdzjWRXr26MbtlxxDjx7d6NbczA23P8wpf7iFHbcZwqnH7ENTk5j73gK+feKfeO7fswAY/oXN+enhexIBU595iYNOuCzfF1EwroKwT7jiT5ez9trr8O7cd/MOxbrQgvcXsvuIc5k77326dWvizkuP5bYJT3DuCfvz1ZEX8fSM1xjx1f/iJ4ftzogT/8w6nxrAcYfsyrCDzmb2O/MYsHzfvF9C4RSpBFyk2TsK67VXX+We8Xexz/B98w7FcjB33vsAdO/WTLduzUQEEUG/ZXoB0G/Z3rzynzkAHLLPtlx0zXhmvzMPgP+85T/Y5ZKyL3nrsAQsaShwHrAB0ANoBuZGRL8ax9Ywzjz9VEb+4IfMnTs371AsB01N4r4rf8w6qw/goqvH89C0mRxx8pXccN4RzF/wPm/Pnc8OB54FwHprDATgztEjaW5q4pSLbmHsfU/mGX7h1EFezSxLCfh84ADgWaA3cBhJQm6TpBGSJkmadMnFozofZYHdfdc4VlhhBTbcaOO8Q7GcLF4cDN3/dNbd7WdstfEabLjOqnzvf3Zin+9dwLq7/5w//e0BzvjBVwBobm5m3U8NZNdv/44Dj7+MC3/xdZbr2zvnV1AszVLmJW+Z6oAjYrqk5ohYBIyWdF8Hx48CRgHMX0h0PszieuThKdx1153ce894FixYwNy573L8j4/jtDN+m3do1sXmvDuP8ZOeZbftNmSTIYN4aNpMAP562xT+9vsjAHjp9dlMfGwGCxcuZubLb/DM86+z7qcGMPmJF/IMvVjyz6uZZSkBvyepB/CIpDMljQSWqXFcDeP7I3/A2DvH84+xd3LGb89m688OdfJdiqy0fN8PS7C9enZn2Gc/zVMzXqNf396s+6mkumHY0PV5esZrANw87lF22HoIACv2X4b11hjIjJfeyCf4glIZ//KWpQT8TZJ636OAkcDqwPBaBmXWKFZZqR8Xn/xNmpuaaGoS142dwj/umcaRv7qSq357GItjMbPfnsd3TvozAGPve5JdPrcBU677KYsWBSf87428Ocf3DspRBzULmSmitjUES3sVhLVu+a2PyjsEq0PzHj6/0+nzoefmZM45W6+9XK7pus0SsKRrIuJrkqbCJ5NoRGxa08jMzCpRoBJwe1UQ309/frErAjEzq4aGGAsiIl5Jf87sunDMzDqnOOk3QysISV+R9KykOZLelvSOpLe7Ijgzs7KpjKWjU0kjJT0uaZqkqyT1krSWpAfTvHh12kqsIlmaoZ0J7BURy0VEv4hY1r3gzKxeVasZmqRBwNHAVhGxMUlrsP2BM4BzImI94C3g0EpjzZKAX4sI94U0s0Ko8lgQ3YDekroBfYBXgGHAX9PHxwBfrjTWLO2AJ0m6GrgRWNCyMyKur/SiZma1Us49OEkjgBElu0alPXmJiJck/RZ4AZgH3AZMBmZHxML0+BeBQZXGmiUB9wPeA3Yt2ReAE7CZ1Z1yeriVDpvwifNIywN7A2sBs4FrgT1aO035USY6TMARcXClJzcz62pVbIW2CzAjIv6TnFfXA9sC/SV1S0vBg4GXK71AllYQgyXdIOl1Sa9Juk7S4EovaGZWS1VsBPECMFRSH0kCdgaeAMYBLYN7fwv4W6WxZrkJNxq4CViNpK7j5nSfmVn9qVIGjogHSW62TQGmkuTLUcCPgWMlTQdWBC6pNNQsdcADIqI04V4m6ZhKL2hmVkvVHOUsIk4ETlxi93PANtU4f5YS8CxJ35DUnC7fADw+npnVpSZlX/KWJQEfAnwNeJWkDdy+6T4zs/pTxUrgWsvSCuIFYK8uiMXMrNPqYaD1rLJMyrkW8D1gzdLjI8JJ2czqToEGQ8t0E+5Gkrt8NwOLaxuOmVnnFCj/ZkrA8yPi3JpHYmZWDQXKwFkS8O8knUjSD7p0LIgpNYvKzKxCDTEge4lNSCbmHMZHVRCRbpuZ1ZXipN9sCXgfYO2IeL/WwZiZdVqBMnCWdsCPAv1rHYiZWTVUa0D2rpClBLwy8JSkh/h4HbCboZlZ3SlQFXCmBLxkP2gzs7rVUAk4Iu7uikDMzKqhHqoWssrSE+4dPhrxvQfQHZjriTnNrB41Wgl42dJtSV+mSkOxmZlVW4Hyb6ZWEB8TETfiNsBmVqeqPCtyTWWpgvhKyWYTsBWdmITOzKy26iCzZpSlFcSXStYXAs+TzBRqZlZ36mGg9aw8K7KZNZR6qFrIKsusyEMk3SFpWrq9qaSf1T40M7PyFaknXJabcBcDxwMfAETEY8D+tQzKzKxijTQlEdAnIibq4+X6hTWKx8ysU+ogr2aWJQHPkrQOacsHSfuSTM5pZlZ3Gm084COBUcD6kl4CZgDfqGlUZmaVKk7+zdQK4jlgF0nLAE0R8U7twzIzq0yB8m+mjhg9geGksyK31AVHxMk1jczMrAIFqoHIVAXxN2AOMJmS8YDNzOpRPTQvyypLAh4cEbvXPBIzsyooUgk4Szvg+yRtUvNIzMyqoKEG4wE+DxwkaQZJFYSAiIhNaxqZmVkFGq0KYo+aR2FmViX1ULLNKksztJmStiApCQcwISKm1DwyM7MKFCj/ZhqM5xfAGGBFYCVgtAfjMbO61WBjQRwAbB4R8wEknQ5MAU6pZWBmZpVotDrg54FewPx0uyfwr1oFZGbWGQ01IDtJy4fHJY0lqQP+AnCvpHMBIuLoGsZnZlaeBkvAN6RLi7tqE4qZWecVqQpCEZ5fs6tIGhERo/KOw+qLPxdLr7KnpbdOGZF3AFaX/LlYSjkBm5nlxAnYzCwnbd6Ek3Qz6TRErYmIvWoSUWNzPZ+1xp+LpVSbN+Ek7dDeEyPi7ppEZGa2lHArCDOznGSZkmg94DRgQ5IecQBExNo1jMvMrOFluQk3GrgQWAjsBFwO/KmWQZVL0iJJj0iaJulaSX06ca4dJf09Xd9L0k/aOba/pCMquMZJko6rNMZaKX3t9czvd/46+r+ybLIk4N4RcQdJdcXMiDgJGFbbsMo2LyI2i4iNgfeBw0sfVKLsFh8RcVNEnN7OIf2Bsn8ha0FSc94xdKGl/v3OW4b/K8sgy4d0fvphflbSUZL2AQbWOK7OuAdYV9Kakp6UdAHJ6G2rS9pV0v2SpqQlp74AknaX9JSke4GvtJxI0kGSzk/XV5Z0g6RH02Vb4HRgnbQ09pv0uB9KekjSY5J+WXKun0p6WtLtwKdbC1zSZZLOlXSfpOck7Zvul6TfpCW+qZL2S/fvKGmcpCuBqelrfkrSH9Njr5C0i6QJkp6VtE36vG3Sazyc/mw1noIo+vv9B0n3SHpG0hdL4rhe0q3p+3ZmyXPaek3PS1opXd9K0l3p+kmSxki6LT3mK5LOTD9Ht0rqnh63c/p5mCrpUiWzobec95fp9aZKWr+V/6svSXowff7tklbu7Ju61IiIdhdga6AvMJikOuJ6YGhHz+vKBXg3/dmNZBbn7wJrAotbYiUZy3g8sEy6/WPgFyT12v8G1iMZxuMa4O/pMQcB56frVwPHpOvNwHLpNaaVxLErSZMikfxx+zuwPbAlMBXoA/QDpgPHtfI6LgOuTZ+7ITA93T8cGJted2XgBWBVYEdgLrBWetyaJFVFm6TnmAxcmsazN3Bjelw/oFu6vgtwXbq+Y8trr+elwd7vW9Pnrge8mMZ3EPBces1ewExg9bZeU7r+PLBSur4VcFe6fhJwL9Ad+AzwHrBH+tgNwJdL/k+GpPsvL3ntzwPfS9ePAP7Yyv/V8nx0Q/8w4Ky8PyNFWbLMiPFQuvoucHBHx+ekt6RH0vV7gEuA1YCZEfFAun8oSVKboGTOkh7A/cD6wIyIeBZA0p9pvWvoMOBAgIhYBMyRtPwSx+yaLg+n231JfrGWBW6IiPfSa9zUzmu5MSIWA0+UlCQ+D1yVXvc1SXeT/GF8G5gYETNKnj8jIqam13kcuCMiQtJUkgQCyS/2GCU3WIPkl7NIGun9viZ9v5+V9FwaHyTv25z0+U8Aa5BUgbT2mjryj4j4IP0MNJMkfUj+SKxJUkKfERHPpPvHAEcC/5tuX5/+nEzJN4YSg4GrJa2axjSjlWOsFVlaQYyjlQ4ZEVFP9cDzImKz0h3pB3Ru6S5gbEQcsMRxm9FOh5MyCTgtIi5a4hrHlHGNBUucr/Rna+YusV36/MUl24v56P3+FTAuIvaRtCbFG+Gukd7vJY9r2S59HxeRvHetvqbUQj6qUuy1xGMLACJisaQPIi2q8tFnoqPhw1piaYljSecBZ0fETZJ2JCl1WwZZ6oCPA36YLj8HHgEm1TKoGnkA2E7SugCS+kgaAjwFrCVpnfS41j7cAHeQfNVFUrOkfsA7JKWdFv8EDimplxskaSDJ18Z9JPWWtCzwpTJjHw/sl153AMnX3IllnqPUcsBL6fpBnThPPSvK+/1VSU1pPGsDT1fwmiCpKtgyXR/ezjla8xSwZst5gW8C5XS0Kv08favMay/VOkzAETG5ZJkQEccCn+2C2KoqIv5DkmyukvQYyYd5/UimWhoB/J+SmzIz2zjF94Gd0q9xk4GNIuINkq+D0yT9JiJuA64E7k+P+yuwbCSTmF5N8sfrOpKvzeW4AXgMeBS4E/hRRLxa5jlKnQmcJmkCyVfShlOg9/tpkmT3D+DwNL6yXlP68C+B30m6h6Skmll6zYOBa9PXsRj4QxmnOCl97j3ArHKuvbTrsCecpBVKNptI/sqeGxFFvnNuljtJl5HcAPxr3rFYPrLMiDGZpF5KJPVMM4BDaxmUmdnSIEsJuNeSX4sk9YyIBW09x8zMOpblJtx9rezL0vTFzMza0d54wKsAg0jaXG7OR01V+pE0MDczs05orw54N5I7roOBs/goAb8NnFDbsMzMGl+WOuDhEXFdF8VjZrbUyFIHvKWk/i0bkpaXdEoNYzIzWypkScB7RMTslo2IeAvYs3YhmZktHbIk4OaWoekAJPUGerZzvJmZZZClI8afgTskjSbpkHEIyXB1ZmbWCZkm5ZS0O8m4sQJui4h/1jowM7NGV/asyJK2A74eEUfWJiQzs6VDliqIljFUDwD2IxkL4vr2n2FmZh1pryfcEGB/ksT7BsnweoqInbooNjOzhtZmFYSkxSTjmB4aEdPTfc9FxNpdGJ+ZWcNqrxnacOBVYJykiyXtTMdTl5iZWUZZuiIvQzJz6gEkExWOIZlw8Lbah2dm1rjKagWRzo7xVWC/OpuU08yscMpuhmZmZtWRpSuymZnVgBOwmVlOnIDNzHLiBGxmlpP/B1T1zfdrX2OjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19f528a6eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_base = model.predict(test_features)\n",
    "model_pred = [label_decoder(i) for i in model_base]\n",
    "cm = confusion_matrix(y_test, model_pred)\n",
    "sns.heatmap(cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "base_inc = keras.applications.InceptionV3(include_top=False, weights=weight_path, input_shape = (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_X_train = base_inc.predict(X_train_array)\n",
    "inc_X_test = base_inc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation='relu', input_dim=5*5*2048))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/1000\n",
      "2682/2682 [==============================] - 5s 2ms/step - loss: 4.9249 - acc: 0.6775 - val_loss: 4.8874 - val_acc: 0.6819\n",
      "Epoch 2/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 2.8788 - acc: 0.8134 - val_loss: 4.3084 - val_acc: 0.7163\n",
      "Epoch 3/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 2.4888 - acc: 0.8389 - val_loss: 5.5148 - val_acc: 0.6522\n",
      "Epoch 4/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 1.9072 - acc: 0.8753 - val_loss: 4.1607 - val_acc: 0.7284\n",
      "Epoch 5/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 1.6851 - acc: 0.8887 - val_loss: 5.6775 - val_acc: 0.6434\n",
      "Epoch 6/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 1.7431 - acc: 0.8859 - val_loss: 4.5757 - val_acc: 0.7043\n",
      "Epoch 7/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 1.4620 - acc: 0.9049 - val_loss: 4.0135 - val_acc: 0.7388\n",
      "Epoch 8/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 1.6229 - acc: 0.8947 - val_loss: 3.8750 - val_acc: 0.7452\n",
      "Epoch 9/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 1.5080 - acc: 0.9023 - val_loss: 5.5565 - val_acc: 0.6514\n",
      "Epoch 10/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 1.2723 - acc: 0.9176 - val_loss: 5.2164 - val_acc: 0.6707\n",
      "Epoch 11/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 1.1940 - acc: 0.9221 - val_loss: 4.8378 - val_acc: 0.6923\n",
      "Epoch 12/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 1.1695 - acc: 0.9234 - val_loss: 4.8060 - val_acc: 0.6939\n",
      "Epoch 13/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 1.2470 - acc: 0.9169 - val_loss: 5.5343 - val_acc: 0.6514\n",
      "Epoch 14/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 1.1444 - acc: 0.9245 - val_loss: 3.7853 - val_acc: 0.7548\n",
      "Epoch 15/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 1.0742 - acc: 0.9292 - val_loss: 4.3745 - val_acc: 0.7204\n",
      "Epoch 16/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 1.0294 - acc: 0.9318 - val_loss: 4.3834 - val_acc: 0.7163\n",
      "Epoch 17/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.9649 - acc: 0.9361 - val_loss: 3.7778 - val_acc: 0.7588\n",
      "Epoch 18/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 1.0124 - acc: 0.9333 - val_loss: 4.5383 - val_acc: 0.7091\n",
      "Epoch 19/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.9044 - acc: 0.9409 - val_loss: 4.7499 - val_acc: 0.6987\n",
      "Epoch 20/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.9234 - acc: 0.9387 - val_loss: 4.7498 - val_acc: 0.6987\n",
      "Epoch 21/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.9938 - acc: 0.9349 - val_loss: 5.2970 - val_acc: 0.6603\n",
      "Epoch 22/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.9745 - acc: 0.9359 - val_loss: 3.8216 - val_acc: 0.7468\n",
      "Epoch 23/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.7838 - acc: 0.9485 - val_loss: 4.4182 - val_acc: 0.7139\n",
      "Epoch 24/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.8382 - acc: 0.9452 - val_loss: 3.7736 - val_acc: 0.7564\n",
      "Epoch 25/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.8962 - acc: 0.9420 - val_loss: 3.7173 - val_acc: 0.7556\n",
      "Epoch 26/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.9746 - acc: 0.9355 - val_loss: 4.1852 - val_acc: 0.7340\n",
      "Epoch 27/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.7888 - acc: 0.9480 - val_loss: 5.5850 - val_acc: 0.6498\n",
      "Epoch 28/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.7538 - acc: 0.9506 - val_loss: 4.6038 - val_acc: 0.7083\n",
      "Epoch 29/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.7929 - acc: 0.9484 - val_loss: 5.5881 - val_acc: 0.6498\n",
      "Epoch 30/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.7627 - acc: 0.9497 - val_loss: 3.9958 - val_acc: 0.7444\n",
      "Epoch 31/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.6946 - acc: 0.9538 - val_loss: 4.1760 - val_acc: 0.7292\n",
      "Epoch 32/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.6818 - acc: 0.9553 - val_loss: 5.4213 - val_acc: 0.6571\n",
      "Epoch 33/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.6049 - acc: 0.9601 - val_loss: 3.6887 - val_acc: 0.7612\n",
      "Epoch 34/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.6235 - acc: 0.9590 - val_loss: 3.8032 - val_acc: 0.7532\n",
      "Epoch 35/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.7205 - acc: 0.9525 - val_loss: 5.2100 - val_acc: 0.6715\n",
      "Epoch 36/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.6171 - acc: 0.9584 - val_loss: 4.6345 - val_acc: 0.7067\n",
      "Epoch 37/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.7346 - acc: 0.9502 - val_loss: 5.5662 - val_acc: 0.6522\n",
      "Epoch 38/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.6733 - acc: 0.9556 - val_loss: 4.2819 - val_acc: 0.7260\n",
      "Epoch 39/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.5884 - acc: 0.9612 - val_loss: 4.2566 - val_acc: 0.7268\n",
      "Epoch 40/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.5962 - acc: 0.9614 - val_loss: 5.0294 - val_acc: 0.6827\n",
      "Epoch 41/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.5946 - acc: 0.9610 - val_loss: 4.7128 - val_acc: 0.7011\n",
      "Epoch 42/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.5735 - acc: 0.9622 - val_loss: 5.5393 - val_acc: 0.6538\n",
      "Epoch 43/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.7057 - acc: 0.9528 - val_loss: 3.9265 - val_acc: 0.7484\n",
      "Epoch 44/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.6024 - acc: 0.9599 - val_loss: 4.8170 - val_acc: 0.6947\n",
      "Epoch 45/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.5508 - acc: 0.9635 - val_loss: 4.0635 - val_acc: 0.7420\n",
      "Epoch 46/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.5791 - acc: 0.9618 - val_loss: 5.5369 - val_acc: 0.6538\n",
      "Epoch 47/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.6238 - acc: 0.9590 - val_loss: 4.1320 - val_acc: 0.7372\n",
      "Epoch 48/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.6956 - acc: 0.9547 - val_loss: 5.4152 - val_acc: 0.6579\n",
      "Epoch 49/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.6902 - acc: 0.9553 - val_loss: 4.8134 - val_acc: 0.6939\n",
      "Epoch 50/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.6961 - acc: 0.9541 - val_loss: 4.4596 - val_acc: 0.7155\n",
      "Epoch 51/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.5122 - acc: 0.9663 - val_loss: 4.8019 - val_acc: 0.6939\n",
      "Epoch 52/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.5651 - acc: 0.9633 - val_loss: 4.4266 - val_acc: 0.7188\n",
      "Epoch 53/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.5892 - acc: 0.9612 - val_loss: 5.5944 - val_acc: 0.6498\n",
      "Epoch 54/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.5515 - acc: 0.9635 - val_loss: 4.0703 - val_acc: 0.7388\n",
      "Epoch 55/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.5602 - acc: 0.9635 - val_loss: 5.5580 - val_acc: 0.6522\n",
      "Epoch 56/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.4861 - acc: 0.9681 - val_loss: 4.9086 - val_acc: 0.6899\n",
      "Epoch 57/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.5351 - acc: 0.9657 - val_loss: 4.5122 - val_acc: 0.7115\n",
      "Epoch 58/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.5428 - acc: 0.9638 - val_loss: 5.1221 - val_acc: 0.6771\n",
      "Epoch 59/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.4644 - acc: 0.9691 - val_loss: 5.2151 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.4437 - acc: 0.9705 - val_loss: 3.7198 - val_acc: 0.7588\n",
      "Epoch 61/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.4636 - acc: 0.9704 - val_loss: 4.8741 - val_acc: 0.6907\n",
      "Epoch 62/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.4309 - acc: 0.9713 - val_loss: 3.8020 - val_acc: 0.7580\n",
      "Epoch 63/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.4632 - acc: 0.9692 - val_loss: 4.9667 - val_acc: 0.6859\n",
      "Epoch 64/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.4187 - acc: 0.9724 - val_loss: 3.9943 - val_acc: 0.7404\n",
      "Epoch 65/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.4275 - acc: 0.9713 - val_loss: 4.9409 - val_acc: 0.6867\n",
      "Epoch 66/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.3803 - acc: 0.9739 - val_loss: 4.2103 - val_acc: 0.7284\n",
      "Epoch 67/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.3730 - acc: 0.9750 - val_loss: 4.8822 - val_acc: 0.6923\n",
      "Epoch 68/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.4139 - acc: 0.9732 - val_loss: 5.1858 - val_acc: 0.6739\n",
      "Epoch 69/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.4002 - acc: 0.9737 - val_loss: 3.8543 - val_acc: 0.7524\n",
      "Epoch 70/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.4268 - acc: 0.9724 - val_loss: 4.9324 - val_acc: 0.6883\n",
      "Epoch 71/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.4384 - acc: 0.9715 - val_loss: 4.5493 - val_acc: 0.7099\n",
      "Epoch 72/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.3337 - acc: 0.9773 - val_loss: 4.0839 - val_acc: 0.7380\n",
      "Epoch 73/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.3547 - acc: 0.9769 - val_loss: 4.4949 - val_acc: 0.7147\n",
      "Epoch 74/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.3949 - acc: 0.9743 - val_loss: 4.6523 - val_acc: 0.7035\n",
      "Epoch 75/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.3429 - acc: 0.9767 - val_loss: 4.7125 - val_acc: 0.6987\n",
      "Epoch 76/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.3814 - acc: 0.9750 - val_loss: 4.9321 - val_acc: 0.6883\n",
      "Epoch 77/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.4190 - acc: 0.9726 - val_loss: 5.2038 - val_acc: 0.6731\n",
      "Epoch 78/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.2892 - acc: 0.9810 - val_loss: 4.8314 - val_acc: 0.6947\n",
      "Epoch 79/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.3033 - acc: 0.9802 - val_loss: 4.7582 - val_acc: 0.6939\n",
      "Epoch 80/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.4018 - acc: 0.9728 - val_loss: 4.3775 - val_acc: 0.7220\n",
      "Epoch 81/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.3385 - acc: 0.9774 - val_loss: 4.8512 - val_acc: 0.6931\n",
      "Epoch 82/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.3618 - acc: 0.9760 - val_loss: 4.6427 - val_acc: 0.7035\n",
      "Epoch 83/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.4074 - acc: 0.9726 - val_loss: 4.7626 - val_acc: 0.6939\n",
      "Epoch 84/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.3343 - acc: 0.9771 - val_loss: 3.9583 - val_acc: 0.7468\n",
      "Epoch 85/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.3829 - acc: 0.9750 - val_loss: 4.0356 - val_acc: 0.7412\n",
      "Epoch 86/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.5882 - acc: 0.9620 - val_loss: 3.7430 - val_acc: 0.7564\n",
      "Epoch 87/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.3478 - acc: 0.9767 - val_loss: 3.9931 - val_acc: 0.7436\n",
      "Epoch 88/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.3269 - acc: 0.9784 - val_loss: 5.1976 - val_acc: 0.6723\n",
      "Epoch 89/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.3700 - acc: 0.9758 - val_loss: 4.1465 - val_acc: 0.7340\n",
      "Epoch 90/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.2892 - acc: 0.9802 - val_loss: 4.5556 - val_acc: 0.7059\n",
      "Epoch 91/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.2939 - acc: 0.9806 - val_loss: 4.1169 - val_acc: 0.7364\n",
      "Epoch 92/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.3611 - acc: 0.9756 - val_loss: 3.8625 - val_acc: 0.7508\n",
      "Epoch 93/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.3249 - acc: 0.9787 - val_loss: 4.6592 - val_acc: 0.7027\n",
      "Epoch 94/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.2929 - acc: 0.9808 - val_loss: 5.0296 - val_acc: 0.6827\n",
      "Epoch 95/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.3175 - acc: 0.9782 - val_loss: 4.4483 - val_acc: 0.7179\n",
      "Epoch 96/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.2960 - acc: 0.9804 - val_loss: 4.5528 - val_acc: 0.7107\n",
      "Epoch 97/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.2765 - acc: 0.9815 - val_loss: 4.6325 - val_acc: 0.7067\n",
      "Epoch 98/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.4404 - acc: 0.9705 - val_loss: 4.9205 - val_acc: 0.6875\n",
      "Epoch 99/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.3322 - acc: 0.9773 - val_loss: 4.3773 - val_acc: 0.7212\n",
      "Epoch 100/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.2855 - acc: 0.9810 - val_loss: 4.5522 - val_acc: 0.7123\n",
      "Epoch 101/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.3393 - acc: 0.9778 - val_loss: 5.3829 - val_acc: 0.6611\n",
      "Epoch 102/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.3601 - acc: 0.9765 - val_loss: 4.8707 - val_acc: 0.6907\n",
      "Epoch 103/1000\n",
      "2682/2682 [==============================] - 3s 999us/step - loss: 0.2552 - acc: 0.9827 - val_loss: 4.4698 - val_acc: 0.7147\n",
      "Epoch 104/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.2517 - acc: 0.9834 - val_loss: 4.6824 - val_acc: 0.7043\n",
      "Epoch 105/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.3390 - acc: 0.9778 - val_loss: 4.3104 - val_acc: 0.7212\n",
      "Epoch 106/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2487 - acc: 0.9828 - val_loss: 4.8924 - val_acc: 0.6915\n",
      "Epoch 107/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.2356 - acc: 0.9838 - val_loss: 4.1459 - val_acc: 0.7356\n",
      "Epoch 108/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2732 - acc: 0.9821 - val_loss: 5.1845 - val_acc: 0.6731\n",
      "Epoch 109/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.2828 - acc: 0.9814 - val_loss: 4.2420 - val_acc: 0.7292\n",
      "Epoch 110/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.2314 - acc: 0.9845 - val_loss: 5.2384 - val_acc: 0.6707\n",
      "Epoch 111/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2092 - acc: 0.9866 - val_loss: 4.8967 - val_acc: 0.6899\n",
      "Epoch 112/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.3032 - acc: 0.9793 - val_loss: 4.5870 - val_acc: 0.7075\n",
      "Epoch 113/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1797 - acc: 0.9883 - val_loss: 4.9976 - val_acc: 0.6819\n",
      "Epoch 114/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2543 - acc: 0.9832 - val_loss: 5.3977 - val_acc: 0.6603\n",
      "Epoch 115/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.2464 - acc: 0.9834 - val_loss: 4.0459 - val_acc: 0.7404\n",
      "Epoch 116/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.2347 - acc: 0.9840 - val_loss: 4.2587 - val_acc: 0.7244\n",
      "Epoch 117/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1901 - acc: 0.9871 - val_loss: 4.2227 - val_acc: 0.7292\n",
      "Epoch 118/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.2583 - acc: 0.9828 - val_loss: 4.8845 - val_acc: 0.6891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.2119 - acc: 0.9856 - val_loss: 4.9202 - val_acc: 0.6907\n",
      "Epoch 120/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.2349 - acc: 0.9845 - val_loss: 4.5189 - val_acc: 0.7147\n",
      "Epoch 121/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.1875 - acc: 0.9871 - val_loss: 4.4569 - val_acc: 0.7155\n",
      "Epoch 122/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.2422 - acc: 0.9840 - val_loss: 4.5407 - val_acc: 0.7099\n",
      "Epoch 123/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.2159 - acc: 0.9860 - val_loss: 4.9699 - val_acc: 0.6851\n",
      "Epoch 124/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.2291 - acc: 0.9849 - val_loss: 4.4982 - val_acc: 0.7131\n",
      "Epoch 125/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.2359 - acc: 0.9843 - val_loss: 4.9313 - val_acc: 0.6891\n",
      "Epoch 126/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.2057 - acc: 0.9862 - val_loss: 4.1728 - val_acc: 0.7300\n",
      "Epoch 127/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.2619 - acc: 0.9828 - val_loss: 5.6092 - val_acc: 0.6458\n",
      "Epoch 128/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.2359 - acc: 0.9847 - val_loss: 4.7246 - val_acc: 0.6979\n",
      "Epoch 129/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.2011 - acc: 0.9862 - val_loss: 5.0881 - val_acc: 0.6779\n",
      "Epoch 130/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2316 - acc: 0.9849 - val_loss: 5.3915 - val_acc: 0.6619\n",
      "Epoch 131/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.2440 - acc: 0.9838 - val_loss: 4.3932 - val_acc: 0.7196\n",
      "Epoch 132/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2442 - acc: 0.9840 - val_loss: 4.6782 - val_acc: 0.7027\n",
      "Epoch 133/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1963 - acc: 0.9870 - val_loss: 4.8615 - val_acc: 0.6923\n",
      "Epoch 134/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.2016 - acc: 0.9873 - val_loss: 4.7690 - val_acc: 0.6979\n",
      "Epoch 135/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1873 - acc: 0.9877 - val_loss: 5.1656 - val_acc: 0.6763\n",
      "Epoch 136/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1923 - acc: 0.9873 - val_loss: 4.5689 - val_acc: 0.7083\n",
      "Epoch 137/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1917 - acc: 0.9871 - val_loss: 4.5510 - val_acc: 0.7107\n",
      "Epoch 138/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1771 - acc: 0.9884 - val_loss: 5.1864 - val_acc: 0.6739\n",
      "Epoch 139/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1396 - acc: 0.9912 - val_loss: 4.9126 - val_acc: 0.6891\n",
      "Epoch 140/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1950 - acc: 0.9871 - val_loss: 5.2903 - val_acc: 0.6675\n",
      "Epoch 141/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1544 - acc: 0.9894 - val_loss: 5.3887 - val_acc: 0.6619\n",
      "Epoch 142/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1761 - acc: 0.9886 - val_loss: 5.2575 - val_acc: 0.6675\n",
      "Epoch 143/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1728 - acc: 0.9883 - val_loss: 5.0134 - val_acc: 0.6843\n",
      "Epoch 144/1000\n",
      "2682/2682 [==============================] - 3s 1000us/step - loss: 0.1957 - acc: 0.9864 - val_loss: 4.9039 - val_acc: 0.6883\n",
      "Epoch 145/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.1515 - acc: 0.9899 - val_loss: 4.4995 - val_acc: 0.7115\n",
      "Epoch 146/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.1944 - acc: 0.9870 - val_loss: 4.4120 - val_acc: 0.7196\n",
      "Epoch 147/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.2033 - acc: 0.9864 - val_loss: 4.4032 - val_acc: 0.7212\n",
      "Epoch 148/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.2321 - acc: 0.9845 - val_loss: 4.2494 - val_acc: 0.7308\n",
      "Epoch 149/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1973 - acc: 0.9864 - val_loss: 5.0222 - val_acc: 0.6843\n",
      "Epoch 150/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1786 - acc: 0.9883 - val_loss: 4.5399 - val_acc: 0.7083\n",
      "Epoch 151/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2075 - acc: 0.9868 - val_loss: 4.7780 - val_acc: 0.6963\n",
      "Epoch 152/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2094 - acc: 0.9860 - val_loss: 4.6894 - val_acc: 0.7035\n",
      "Epoch 153/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.2126 - acc: 0.9851 - val_loss: 5.0300 - val_acc: 0.6843\n",
      "Epoch 154/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1968 - acc: 0.9871 - val_loss: 5.1013 - val_acc: 0.6771\n",
      "Epoch 155/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.2287 - acc: 0.9843 - val_loss: 4.9429 - val_acc: 0.6883\n",
      "Epoch 156/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1743 - acc: 0.9884 - val_loss: 5.0188 - val_acc: 0.6827\n",
      "Epoch 157/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1605 - acc: 0.9892 - val_loss: 5.0624 - val_acc: 0.6811\n",
      "Epoch 158/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1772 - acc: 0.9879 - val_loss: 4.9449 - val_acc: 0.6859\n",
      "Epoch 159/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.1661 - acc: 0.9890 - val_loss: 4.4957 - val_acc: 0.7147\n",
      "Epoch 160/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1743 - acc: 0.9886 - val_loss: 4.8337 - val_acc: 0.6907\n",
      "Epoch 161/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1826 - acc: 0.9881 - val_loss: 5.1957 - val_acc: 0.6747\n",
      "Epoch 162/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1375 - acc: 0.9914 - val_loss: 4.9227 - val_acc: 0.6867\n",
      "Epoch 163/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.1527 - acc: 0.9897 - val_loss: 5.0084 - val_acc: 0.6827\n",
      "Epoch 164/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.1338 - acc: 0.9914 - val_loss: 5.0804 - val_acc: 0.6811\n",
      "Epoch 165/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1166 - acc: 0.9927 - val_loss: 5.0802 - val_acc: 0.6803\n",
      "Epoch 166/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1414 - acc: 0.9905 - val_loss: 5.1542 - val_acc: 0.6755\n",
      "Epoch 167/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1755 - acc: 0.9883 - val_loss: 4.4496 - val_acc: 0.7139\n",
      "Epoch 168/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.1397 - acc: 0.9905 - val_loss: 4.8509 - val_acc: 0.6915\n",
      "Epoch 169/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1451 - acc: 0.9907 - val_loss: 5.0218 - val_acc: 0.6803\n",
      "Epoch 170/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1483 - acc: 0.9905 - val_loss: 4.9763 - val_acc: 0.6851\n",
      "Epoch 171/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1101 - acc: 0.9929 - val_loss: 4.9732 - val_acc: 0.6851\n",
      "Epoch 172/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1170 - acc: 0.9924 - val_loss: 4.4978 - val_acc: 0.7115\n",
      "Epoch 173/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1291 - acc: 0.9912 - val_loss: 4.7370 - val_acc: 0.6971\n",
      "Epoch 174/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.2017 - acc: 0.9868 - val_loss: 5.5158 - val_acc: 0.6538\n",
      "Epoch 175/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1164 - acc: 0.9920 - val_loss: 5.5939 - val_acc: 0.6490\n",
      "Epoch 176/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1213 - acc: 0.9916 - val_loss: 4.4746 - val_acc: 0.7131\n",
      "Epoch 177/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1355 - acc: 0.9914 - val_loss: 5.0987 - val_acc: 0.6795\n",
      "Epoch 178/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1136 - acc: 0.9916 - val_loss: 4.8768 - val_acc: 0.6899\n",
      "Epoch 179/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1625 - acc: 0.9892 - val_loss: 5.0393 - val_acc: 0.6819\n",
      "Epoch 180/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1244 - acc: 0.9916 - val_loss: 5.2335 - val_acc: 0.6667\n",
      "Epoch 181/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1488 - acc: 0.9897 - val_loss: 4.8360 - val_acc: 0.6907\n",
      "Epoch 182/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1135 - acc: 0.9914 - val_loss: 5.0874 - val_acc: 0.6811\n",
      "Epoch 183/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1256 - acc: 0.9918 - val_loss: 5.2338 - val_acc: 0.6715\n",
      "Epoch 184/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1433 - acc: 0.9901 - val_loss: 4.9676 - val_acc: 0.6875\n",
      "Epoch 185/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.1271 - acc: 0.9916 - val_loss: 4.8779 - val_acc: 0.6915\n",
      "Epoch 186/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.1234 - acc: 0.9916 - val_loss: 4.7620 - val_acc: 0.6963\n",
      "Epoch 187/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0946 - acc: 0.9938 - val_loss: 4.8919 - val_acc: 0.6883\n",
      "Epoch 188/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.1172 - acc: 0.9924 - val_loss: 4.9549 - val_acc: 0.6867\n",
      "Epoch 189/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1149 - acc: 0.9927 - val_loss: 4.5868 - val_acc: 0.7091\n",
      "Epoch 190/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1451 - acc: 0.9903 - val_loss: 5.3870 - val_acc: 0.6611\n",
      "Epoch 191/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1361 - acc: 0.9907 - val_loss: 4.0490 - val_acc: 0.7380\n",
      "Epoch 192/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.1682 - acc: 0.9884 - val_loss: 5.1526 - val_acc: 0.6771\n",
      "Epoch 193/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.1158 - acc: 0.9927 - val_loss: 5.1825 - val_acc: 0.6739\n",
      "Epoch 194/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1441 - acc: 0.9907 - val_loss: 5.3063 - val_acc: 0.6667\n",
      "Epoch 195/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.1166 - acc: 0.9924 - val_loss: 4.7247 - val_acc: 0.6979\n",
      "Epoch 196/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.1009 - acc: 0.9933 - val_loss: 4.8302 - val_acc: 0.6947\n",
      "Epoch 197/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1035 - acc: 0.9933 - val_loss: 5.2379 - val_acc: 0.6699\n",
      "Epoch 198/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1108 - acc: 0.9929 - val_loss: 5.0247 - val_acc: 0.6843\n",
      "Epoch 199/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.1269 - acc: 0.9918 - val_loss: 4.6779 - val_acc: 0.7011\n",
      "Epoch 200/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1386 - acc: 0.9907 - val_loss: 5.3925 - val_acc: 0.6595\n",
      "Epoch 201/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1017 - acc: 0.9933 - val_loss: 5.0699 - val_acc: 0.6819\n",
      "Epoch 202/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1206 - acc: 0.9924 - val_loss: 5.1147 - val_acc: 0.6795\n",
      "Epoch 203/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.1256 - acc: 0.9916 - val_loss: 4.7119 - val_acc: 0.6987\n",
      "Epoch 204/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.1024 - acc: 0.9929 - val_loss: 4.7925 - val_acc: 0.6947\n",
      "Epoch 205/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1105 - acc: 0.9929 - val_loss: 4.9517 - val_acc: 0.6883\n",
      "Epoch 206/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1298 - acc: 0.9911 - val_loss: 5.0833 - val_acc: 0.6811\n",
      "Epoch 207/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1041 - acc: 0.9933 - val_loss: 5.0901 - val_acc: 0.6811\n",
      "Epoch 208/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1283 - acc: 0.9914 - val_loss: 5.0340 - val_acc: 0.6827\n",
      "Epoch 209/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0930 - acc: 0.9937 - val_loss: 4.8523 - val_acc: 0.6931\n",
      "Epoch 210/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1092 - acc: 0.9925 - val_loss: 5.0307 - val_acc: 0.6835\n",
      "Epoch 211/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1246 - acc: 0.9916 - val_loss: 5.0264 - val_acc: 0.6835\n",
      "Epoch 212/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1071 - acc: 0.9929 - val_loss: 4.5650 - val_acc: 0.7075\n",
      "Epoch 213/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0937 - acc: 0.9940 - val_loss: 5.1207 - val_acc: 0.6763\n",
      "Epoch 214/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.1154 - acc: 0.9927 - val_loss: 5.4987 - val_acc: 0.6538\n",
      "Epoch 215/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1016 - acc: 0.9937 - val_loss: 5.0292 - val_acc: 0.6827\n",
      "Epoch 216/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1130 - acc: 0.9922 - val_loss: 5.5314 - val_acc: 0.6538\n",
      "Epoch 217/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.1159 - acc: 0.9920 - val_loss: 5.3701 - val_acc: 0.6627\n",
      "Epoch 218/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0918 - acc: 0.9935 - val_loss: 5.1123 - val_acc: 0.6803\n",
      "Epoch 219/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.1128 - acc: 0.9922 - val_loss: 5.2932 - val_acc: 0.6691\n",
      "Epoch 220/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1108 - acc: 0.9925 - val_loss: 4.9089 - val_acc: 0.6915\n",
      "Epoch 221/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0939 - acc: 0.9935 - val_loss: 4.9233 - val_acc: 0.6883\n",
      "Epoch 222/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0907 - acc: 0.9935 - val_loss: 4.8350 - val_acc: 0.6963\n",
      "Epoch 223/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1391 - acc: 0.9909 - val_loss: 4.8289 - val_acc: 0.6963\n",
      "Epoch 224/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0839 - acc: 0.9942 - val_loss: 5.0845 - val_acc: 0.6795\n",
      "Epoch 225/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0680 - acc: 0.9955 - val_loss: 5.2321 - val_acc: 0.6715\n",
      "Epoch 226/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.1207 - acc: 0.9922 - val_loss: 5.1129 - val_acc: 0.6795\n",
      "Epoch 227/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0837 - acc: 0.9948 - val_loss: 5.0990 - val_acc: 0.6795\n",
      "Epoch 228/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1533 - acc: 0.9896 - val_loss: 5.3200 - val_acc: 0.6627\n",
      "Epoch 229/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.1096 - acc: 0.9920 - val_loss: 4.8687 - val_acc: 0.6907\n",
      "Epoch 230/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0971 - acc: 0.9933 - val_loss: 5.1555 - val_acc: 0.6747\n",
      "Epoch 231/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0936 - acc: 0.9938 - val_loss: 5.0928 - val_acc: 0.6803\n",
      "Epoch 232/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0872 - acc: 0.9942 - val_loss: 5.0663 - val_acc: 0.6811\n",
      "Epoch 233/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0892 - acc: 0.9940 - val_loss: 5.0931 - val_acc: 0.6803\n",
      "Epoch 234/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.1140 - acc: 0.9927 - val_loss: 4.3514 - val_acc: 0.7204\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0976 - acc: 0.9937 - val_loss: 5.5623 - val_acc: 0.6506\n",
      "Epoch 236/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1136 - acc: 0.9929 - val_loss: 4.9121 - val_acc: 0.6875\n",
      "Epoch 237/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0877 - acc: 0.9940 - val_loss: 5.4131 - val_acc: 0.6619\n",
      "Epoch 238/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.1330 - acc: 0.9909 - val_loss: 5.0649 - val_acc: 0.6811\n",
      "Epoch 239/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1164 - acc: 0.9924 - val_loss: 5.0095 - val_acc: 0.6875\n",
      "Epoch 240/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0868 - acc: 0.9940 - val_loss: 4.9193 - val_acc: 0.6891\n",
      "Epoch 241/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0795 - acc: 0.9948 - val_loss: 5.6872 - val_acc: 0.6442\n",
      "Epoch 242/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0869 - acc: 0.9942 - val_loss: 5.1889 - val_acc: 0.6739\n",
      "Epoch 243/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.1068 - acc: 0.9927 - val_loss: 5.3392 - val_acc: 0.6619\n",
      "Epoch 244/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0815 - acc: 0.9944 - val_loss: 4.9906 - val_acc: 0.6859\n",
      "Epoch 245/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0728 - acc: 0.9952 - val_loss: 4.7554 - val_acc: 0.6987\n",
      "Epoch 246/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0781 - acc: 0.9948 - val_loss: 5.0194 - val_acc: 0.6827\n",
      "Epoch 247/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0864 - acc: 0.9946 - val_loss: 4.8090 - val_acc: 0.6963\n",
      "Epoch 248/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0763 - acc: 0.9948 - val_loss: 4.6559 - val_acc: 0.6995\n",
      "Epoch 249/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0857 - acc: 0.9938 - val_loss: 4.4837 - val_acc: 0.7123\n",
      "Epoch 250/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0776 - acc: 0.9948 - val_loss: 5.3508 - val_acc: 0.6627\n",
      "Epoch 251/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0837 - acc: 0.9948 - val_loss: 4.8401 - val_acc: 0.6931\n",
      "Epoch 252/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0680 - acc: 0.9953 - val_loss: 5.5198 - val_acc: 0.6522\n",
      "Epoch 253/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0659 - acc: 0.9959 - val_loss: 5.2811 - val_acc: 0.6683\n",
      "Epoch 254/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0698 - acc: 0.9952 - val_loss: 4.4479 - val_acc: 0.7131\n",
      "Epoch 255/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0853 - acc: 0.9940 - val_loss: 4.8023 - val_acc: 0.6955\n",
      "Epoch 256/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0840 - acc: 0.9946 - val_loss: 4.6662 - val_acc: 0.7027\n",
      "Epoch 257/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0681 - acc: 0.9953 - val_loss: 4.8755 - val_acc: 0.6915\n",
      "Epoch 258/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0771 - acc: 0.9952 - val_loss: 5.1356 - val_acc: 0.6779\n",
      "Epoch 259/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0839 - acc: 0.9942 - val_loss: 5.4186 - val_acc: 0.6579\n",
      "Epoch 260/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.1059 - acc: 0.9929 - val_loss: 5.0425 - val_acc: 0.6819\n",
      "Epoch 261/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 4.7488 - val_acc: 0.7011\n",
      "Epoch 262/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0955 - acc: 0.9931 - val_loss: 5.3483 - val_acc: 0.6643\n",
      "Epoch 263/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0786 - acc: 0.9940 - val_loss: 5.6073 - val_acc: 0.6482\n",
      "Epoch 264/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0748 - acc: 0.9942 - val_loss: 5.0302 - val_acc: 0.6835\n",
      "Epoch 265/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0709 - acc: 0.9952 - val_loss: 4.6010 - val_acc: 0.7059\n",
      "Epoch 266/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0714 - acc: 0.9950 - val_loss: 4.8606 - val_acc: 0.6899\n",
      "Epoch 267/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0600 - acc: 0.9959 - val_loss: 4.8517 - val_acc: 0.6947\n",
      "Epoch 268/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0479 - acc: 0.9970 - val_loss: 5.1424 - val_acc: 0.6779\n",
      "Epoch 269/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0796 - acc: 0.9944 - val_loss: 4.5000 - val_acc: 0.7083\n",
      "Epoch 270/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0555 - acc: 0.9965 - val_loss: 5.2029 - val_acc: 0.6707\n",
      "Epoch 271/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0494 - acc: 0.9968 - val_loss: 4.9813 - val_acc: 0.6843\n",
      "Epoch 272/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.1048 - acc: 0.9931 - val_loss: 4.2595 - val_acc: 0.7268\n",
      "Epoch 273/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0957 - acc: 0.9929 - val_loss: 4.0686 - val_acc: 0.7348\n",
      "Epoch 274/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0944 - acc: 0.9931 - val_loss: 4.9580 - val_acc: 0.6867\n",
      "Epoch 275/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0991 - acc: 0.9931 - val_loss: 4.8495 - val_acc: 0.6915\n",
      "Epoch 276/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0887 - acc: 0.9937 - val_loss: 5.1539 - val_acc: 0.6755\n",
      "Epoch 277/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0679 - acc: 0.9955 - val_loss: 5.1129 - val_acc: 0.6755\n",
      "Epoch 278/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0772 - acc: 0.9948 - val_loss: 5.1684 - val_acc: 0.6747\n",
      "Epoch 279/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0700 - acc: 0.9953 - val_loss: 4.8170 - val_acc: 0.6923\n",
      "Epoch 280/1000\n",
      "2682/2682 [==============================] - 3s 999us/step - loss: 0.0645 - acc: 0.9959 - val_loss: 4.8676 - val_acc: 0.6899\n",
      "Epoch 281/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0597 - acc: 0.9959 - val_loss: 4.6542 - val_acc: 0.7043\n",
      "Epoch 282/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.1118 - acc: 0.9922 - val_loss: 5.2919 - val_acc: 0.6683\n",
      "Epoch 283/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0890 - acc: 0.9942 - val_loss: 5.1988 - val_acc: 0.6723\n",
      "Epoch 284/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0652 - acc: 0.9955 - val_loss: 5.0174 - val_acc: 0.6843\n",
      "Epoch 285/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0772 - acc: 0.9950 - val_loss: 4.9542 - val_acc: 0.6875\n",
      "Epoch 286/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0922 - acc: 0.9938 - val_loss: 5.1377 - val_acc: 0.6779\n",
      "Epoch 287/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0979 - acc: 0.9935 - val_loss: 5.3896 - val_acc: 0.6611\n",
      "Epoch 288/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0956 - acc: 0.9933 - val_loss: 5.1025 - val_acc: 0.6795\n",
      "Epoch 289/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0828 - acc: 0.9942 - val_loss: 5.3430 - val_acc: 0.6667\n",
      "Epoch 290/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0852 - acc: 0.9942 - val_loss: 5.3405 - val_acc: 0.6667\n",
      "Epoch 291/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0560 - acc: 0.9953 - val_loss: 5.4287 - val_acc: 0.6579\n",
      "Epoch 292/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0606 - acc: 0.9957 - val_loss: 5.3029 - val_acc: 0.6675\n",
      "Epoch 293/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0948 - acc: 0.9931 - val_loss: 5.3516 - val_acc: 0.6651\n",
      "Epoch 294/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0459 - acc: 0.9963 - val_loss: 5.2573 - val_acc: 0.6707\n",
      "Epoch 295/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0427 - acc: 0.9972 - val_loss: 4.6583 - val_acc: 0.7067\n",
      "Epoch 296/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0478 - acc: 0.9970 - val_loss: 4.9830 - val_acc: 0.6827\n",
      "Epoch 297/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0411 - acc: 0.9972 - val_loss: 5.0327 - val_acc: 0.6827\n",
      "Epoch 298/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0506 - acc: 0.9966 - val_loss: 5.1096 - val_acc: 0.6787\n",
      "Epoch 299/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0378 - acc: 0.9974 - val_loss: 5.0218 - val_acc: 0.6851\n",
      "Epoch 300/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0562 - acc: 0.9959 - val_loss: 4.8507 - val_acc: 0.6923\n",
      "Epoch 301/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0419 - acc: 0.9974 - val_loss: 5.0846 - val_acc: 0.6779\n",
      "Epoch 302/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0468 - acc: 0.9968 - val_loss: 4.4845 - val_acc: 0.7147\n",
      "Epoch 303/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0387 - acc: 0.9974 - val_loss: 5.0743 - val_acc: 0.6811\n",
      "Epoch 304/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0445 - acc: 0.9968 - val_loss: 4.9681 - val_acc: 0.6851\n",
      "Epoch 305/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0477 - acc: 0.9968 - val_loss: 4.9455 - val_acc: 0.6851\n",
      "Epoch 306/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9907 - val_acc: 0.6835\n",
      "Epoch 307/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0624 - acc: 0.9955 - val_loss: 4.7389 - val_acc: 0.7019\n",
      "Epoch 308/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0458 - acc: 0.9968 - val_loss: 5.5001 - val_acc: 0.6514\n",
      "Epoch 309/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0797 - acc: 0.9948 - val_loss: 5.1882 - val_acc: 0.6731\n",
      "Epoch 310/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0406 - acc: 0.9974 - val_loss: 5.1528 - val_acc: 0.6739\n",
      "Epoch 311/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0399 - acc: 0.9972 - val_loss: 4.8039 - val_acc: 0.6971\n",
      "Epoch 312/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0453 - acc: 0.9966 - val_loss: 5.2046 - val_acc: 0.6723\n",
      "Epoch 313/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.5843 - val_acc: 0.7083\n",
      "Epoch 314/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0585 - acc: 0.9963 - val_loss: 4.7102 - val_acc: 0.6987\n",
      "Epoch 315/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0375 - acc: 0.9974 - val_loss: 5.1572 - val_acc: 0.6747\n",
      "Epoch 316/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0418 - acc: 0.9974 - val_loss: 5.1408 - val_acc: 0.6755\n",
      "Epoch 317/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0304 - acc: 0.9979 - val_loss: 4.7763 - val_acc: 0.6947\n",
      "Epoch 318/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0366 - acc: 0.9974 - val_loss: 5.5052 - val_acc: 0.6554\n",
      "Epoch 319/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0376 - acc: 0.9976 - val_loss: 5.3017 - val_acc: 0.6651\n",
      "Epoch 320/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0499 - acc: 0.9965 - val_loss: 5.1674 - val_acc: 0.6747\n",
      "Epoch 321/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0328 - acc: 0.9976 - val_loss: 4.7047 - val_acc: 0.7043\n",
      "Epoch 322/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0354 - acc: 0.9974 - val_loss: 5.3884 - val_acc: 0.6611\n",
      "Epoch 323/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0323 - acc: 0.9978 - val_loss: 5.2092 - val_acc: 0.6723\n",
      "Epoch 324/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0382 - acc: 0.9974 - val_loss: 5.1077 - val_acc: 0.6763\n",
      "Epoch 325/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0378 - acc: 0.9972 - val_loss: 5.1354 - val_acc: 0.6763\n",
      "Epoch 326/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0312 - acc: 0.9979 - val_loss: 4.8645 - val_acc: 0.6891\n",
      "Epoch 327/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0363 - acc: 0.9976 - val_loss: 5.3073 - val_acc: 0.6659\n",
      "Epoch 328/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0379 - acc: 0.9974 - val_loss: 5.1193 - val_acc: 0.6755\n",
      "Epoch 329/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3029 - val_acc: 0.6643\n",
      "Epoch 330/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0355 - acc: 0.9974 - val_loss: 5.2464 - val_acc: 0.6715\n",
      "Epoch 331/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0304 - acc: 0.9979 - val_loss: 5.0516 - val_acc: 0.6803\n",
      "Epoch 332/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0330 - acc: 0.9978 - val_loss: 5.2698 - val_acc: 0.6691\n",
      "Epoch 333/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1866 - val_acc: 0.6723\n",
      "Epoch 334/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1866 - val_acc: 0.6723\n",
      "Epoch 335/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3349 - val_acc: 0.6667\n",
      "Epoch 336/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0334 - acc: 0.9976 - val_loss: 5.2109 - val_acc: 0.6731\n",
      "Epoch 337/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3277 - val_acc: 0.6651\n",
      "Epoch 338/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0362 - acc: 0.9976 - val_loss: 5.1123 - val_acc: 0.6763\n",
      "Epoch 339/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1123 - val_acc: 0.6763\n",
      "Epoch 340/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1964 - val_acc: 0.6731\n",
      "Epoch 341/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1894 - val_acc: 0.6731\n",
      "Epoch 342/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0301 - acc: 0.9979 - val_loss: 4.7266 - val_acc: 0.6963\n",
      "Epoch 343/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0312 - acc: 0.9979 - val_loss: 5.2305 - val_acc: 0.6715\n",
      "Epoch 344/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2510 - val_acc: 0.6707\n",
      "Epoch 345/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1248 - val_acc: 0.6747\n",
      "Epoch 346/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1188 - val_acc: 0.6747\n",
      "Epoch 347/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.6590 - val_acc: 0.7059\n",
      "Epoch 348/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0235 - val_acc: 0.6819\n",
      "Epoch 349/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0235 - val_acc: 0.6819\n",
      "Epoch 350/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0235 - val_acc: 0.6819\n",
      "Epoch 351/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0235 - val_acc: 0.6819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0416 - val_acc: 0.6811\n",
      "Epoch 353/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.7425 - val_acc: 0.6955\n",
      "Epoch 354/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8738 - val_acc: 0.6867\n",
      "Epoch 355/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0307 - acc: 0.9979 - val_loss: 5.1743 - val_acc: 0.6747\n",
      "Epoch 356/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0333 - acc: 0.9978 - val_loss: 4.7519 - val_acc: 0.6955\n",
      "Epoch 357/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0386 - acc: 0.9972 - val_loss: 5.0736 - val_acc: 0.6779\n",
      "Epoch 358/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1111 - val_acc: 0.6779\n",
      "Epoch 359/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0421 - acc: 0.9968 - val_loss: 5.2864 - val_acc: 0.6683\n",
      "Epoch 360/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0300 - acc: 0.9979 - val_loss: 5.0716 - val_acc: 0.6779\n",
      "Epoch 361/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1076 - val_acc: 0.6771\n",
      "Epoch 362/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0393 - acc: 0.9972 - val_loss: 5.0460 - val_acc: 0.6795\n",
      "Epoch 363/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0331 - acc: 0.9978 - val_loss: 5.0300 - val_acc: 0.6811\n",
      "Epoch 364/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0300 - val_acc: 0.6811\n",
      "Epoch 365/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0300 - val_acc: 0.6811\n",
      "Epoch 366/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0300 - val_acc: 0.6811\n",
      "Epoch 367/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2493 - val_acc: 0.6683\n",
      "Epoch 368/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0977 - val_acc: 0.6803\n",
      "Epoch 369/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0977 - val_acc: 0.6803\n",
      "Epoch 370/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0977 - val_acc: 0.6803\n",
      "Epoch 371/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0977 - val_acc: 0.6803\n",
      "Epoch 372/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0307 - acc: 0.9979 - val_loss: 5.3375 - val_acc: 0.6651\n",
      "Epoch 373/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1703 - val_acc: 0.6747\n",
      "Epoch 374/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1703 - val_acc: 0.6747\n",
      "Epoch 375/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.8441 - val_acc: 0.6875\n",
      "Epoch 376/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0373 - acc: 0.9974 - val_loss: 4.9549 - val_acc: 0.6843\n",
      "Epoch 377/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0335 - acc: 0.9978 - val_loss: 5.2696 - val_acc: 0.6683\n",
      "Epoch 378/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2086 - val_acc: 0.6731\n",
      "Epoch 379/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0358 - acc: 0.9976 - val_loss: 5.2228 - val_acc: 0.6723\n",
      "Epoch 380/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2256 - val_acc: 0.6723\n",
      "Epoch 381/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0397 - acc: 0.9970 - val_loss: 5.1193 - val_acc: 0.6763\n",
      "Epoch 382/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.1743 - val_acc: 0.6739\n",
      "Epoch 383/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1743 - val_acc: 0.6739\n",
      "Epoch 384/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1743 - val_acc: 0.6739\n",
      "Epoch 385/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1743 - val_acc: 0.6739\n",
      "Epoch 386/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3245 - val_acc: 0.6667\n",
      "Epoch 387/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8378 - val_acc: 0.6883\n",
      "Epoch 388/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 4.8870 - val_acc: 0.6843\n",
      "Epoch 389/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0352 - acc: 0.9978 - val_loss: 5.2468 - val_acc: 0.6707\n",
      "Epoch 390/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.3131 - val_acc: 0.6675\n",
      "Epoch 391/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0340 - acc: 0.9978 - val_loss: 5.0551 - val_acc: 0.6803\n",
      "Epoch 392/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0418 - acc: 0.9974 - val_loss: 5.0844 - val_acc: 0.6787\n",
      "Epoch 393/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0346 - acc: 0.9978 - val_loss: 5.2296 - val_acc: 0.6715\n",
      "Epoch 394/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2296 - val_acc: 0.6715\n",
      "Epoch 395/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0317 - acc: 0.9978 - val_loss: 5.2299 - val_acc: 0.6715\n",
      "Epoch 396/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2299 - val_acc: 0.6715\n",
      "Epoch 397/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0329 - acc: 0.9976 - val_loss: 4.8049 - val_acc: 0.6899\n",
      "Epoch 398/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0399 - acc: 0.9974 - val_loss: 4.8068 - val_acc: 0.6875\n",
      "Epoch 399/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0369 - acc: 0.9976 - val_loss: 4.7415 - val_acc: 0.6963\n",
      "Epoch 400/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0432 - acc: 0.9972 - val_loss: 5.1554 - val_acc: 0.6763\n",
      "Epoch 401/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0612 - acc: 0.9959 - val_loss: 4.6613 - val_acc: 0.7011\n",
      "Epoch 402/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0533 - acc: 0.9966 - val_loss: 5.1722 - val_acc: 0.6747\n",
      "Epoch 403/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0342 - acc: 0.9978 - val_loss: 5.1508 - val_acc: 0.6771\n",
      "Epoch 404/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0365 - acc: 0.9976 - val_loss: 5.2475 - val_acc: 0.6691\n",
      "Epoch 405/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2087 - val_acc: 0.6731\n",
      "Epoch 406/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0344 - acc: 0.9978 - val_loss: 5.1320 - val_acc: 0.6779\n",
      "Epoch 407/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1612 - val_acc: 0.6763\n",
      "Epoch 408/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1612 - val_acc: 0.6763\n",
      "Epoch 409/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0312 - acc: 0.9978 - val_loss: 5.4380 - val_acc: 0.6579\n",
      "Epoch 410/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0418 - acc: 0.9974 - val_loss: 5.1541 - val_acc: 0.6779\n",
      "Epoch 411/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 4.5956 - val_acc: 0.7075\n",
      "Epoch 412/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0303 - acc: 0.9979 - val_loss: 5.2404 - val_acc: 0.6707\n",
      "Epoch 413/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0320 - acc: 0.9976 - val_loss: 5.2721 - val_acc: 0.6675\n",
      "Epoch 414/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2721 - val_acc: 0.6675\n",
      "Epoch 415/1000\n",
      "2682/2682 [==============================] - 3s 998us/step - loss: 0.0369 - acc: 0.9974 - val_loss: 5.1323 - val_acc: 0.6771\n",
      "Epoch 416/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1323 - val_acc: 0.6771\n",
      "Epoch 417/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1323 - val_acc: 0.6771\n",
      "Epoch 418/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1191 - val_acc: 0.6779\n",
      "Epoch 419/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1191 - val_acc: 0.6779\n",
      "Epoch 420/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1191 - val_acc: 0.6779\n",
      "Epoch 421/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0411 - acc: 0.9974 - val_loss: 5.1168 - val_acc: 0.6779\n",
      "Epoch 422/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0327 - acc: 0.9978 - val_loss: 5.2143 - val_acc: 0.6723\n",
      "Epoch 423/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2143 - val_acc: 0.6723\n",
      "Epoch 424/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.5318 - val_acc: 0.6530\n",
      "Epoch 425/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0423 - acc: 0.9970 - val_loss: 5.1471 - val_acc: 0.6747\n",
      "Epoch 426/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0301 - acc: 0.9979 - val_loss: 4.9287 - val_acc: 0.6835\n",
      "Epoch 427/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0363 - acc: 0.9976 - val_loss: 4.8175 - val_acc: 0.6931\n",
      "Epoch 428/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8027 - val_acc: 0.6939\n",
      "Epoch 429/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0364 - acc: 0.9974 - val_loss: 4.7955 - val_acc: 0.6979\n",
      "Epoch 430/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0363 - acc: 0.9974 - val_loss: 4.9674 - val_acc: 0.6827\n",
      "Epoch 431/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9674 - val_acc: 0.6827\n",
      "Epoch 432/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2514 - val_acc: 0.6715\n",
      "Epoch 433/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0396 - acc: 0.9974 - val_loss: 5.4611 - val_acc: 0.6546\n",
      "Epoch 434/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0394 - acc: 0.9972 - val_loss: 4.8820 - val_acc: 0.6891\n",
      "Epoch 435/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1545 - val_acc: 0.6731\n",
      "Epoch 436/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0429 - acc: 0.9970 - val_loss: 5.0101 - val_acc: 0.6795\n",
      "Epoch 437/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0451 - val_acc: 0.6795\n",
      "Epoch 438/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.5810 - val_acc: 0.7035\n",
      "Epoch 439/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0418 - acc: 0.9974 - val_loss: 4.7716 - val_acc: 0.6987\n",
      "Epoch 440/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0326 - acc: 0.9978 - val_loss: 5.0962 - val_acc: 0.6763\n",
      "Epoch 441/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0962 - val_acc: 0.6763\n",
      "Epoch 442/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0962 - val_acc: 0.6763\n",
      "Epoch 443/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8793 - val_acc: 0.6907\n",
      "Epoch 444/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8793 - val_acc: 0.6907\n",
      "Epoch 445/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1619 - val_acc: 0.6747\n",
      "Epoch 446/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0447 - val_acc: 0.6827\n",
      "Epoch 447/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0795 - val_acc: 0.6803\n",
      "Epoch 448/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0795 - val_acc: 0.6803\n",
      "Epoch 449/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0795 - val_acc: 0.6803\n",
      "Epoch 450/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6755\n",
      "Epoch 451/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1581 - val_acc: 0.6731\n",
      "Epoch 452/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0397 - acc: 0.9974 - val_loss: 5.2061 - val_acc: 0.6683\n",
      "Epoch 453/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0334 - acc: 0.9978 - val_loss: 4.7407 - val_acc: 0.6979\n",
      "Epoch 454/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7830 - val_acc: 0.6931\n",
      "Epoch 455/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7830 - val_acc: 0.6931\n",
      "Epoch 456/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0044 - val_acc: 0.6827\n",
      "Epoch 457/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3019 - val_acc: 0.6667\n",
      "Epoch 458/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2016 - val_acc: 0.6731\n",
      "Epoch 459/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0466 - acc: 0.9968 - val_loss: 5.0170 - val_acc: 0.6827\n",
      "Epoch 460/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0373 - acc: 0.9976 - val_loss: 5.0340 - val_acc: 0.6795\n",
      "Epoch 461/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0355 - acc: 0.9976 - val_loss: 5.1568 - val_acc: 0.6731\n",
      "Epoch 462/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.1317 - val_acc: 0.6763\n",
      "Epoch 463/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1317 - val_acc: 0.6763\n",
      "Epoch 464/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1317 - val_acc: 0.6763\n",
      "Epoch 465/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1232 - val_acc: 0.6763\n",
      "Epoch 466/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1232 - val_acc: 0.6763\n",
      "Epoch 467/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0057 - val_acc: 0.6803\n",
      "Epoch 468/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0057 - val_acc: 0.6803\n",
      "Epoch 469/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0301 - acc: 0.9979 - val_loss: 5.1309 - val_acc: 0.6763\n",
      "Epoch 470/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0322 - acc: 0.9978 - val_loss: 5.0998 - val_acc: 0.6795\n",
      "Epoch 471/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0998 - val_acc: 0.6795\n",
      "Epoch 472/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0998 - val_acc: 0.6795\n",
      "Epoch 473/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0303 - acc: 0.9979 - val_loss: 4.8039 - val_acc: 0.6947\n",
      "Epoch 474/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.8695 - val_acc: 0.6891\n",
      "Epoch 475/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0368 - acc: 0.9974 - val_loss: 4.8612 - val_acc: 0.6891\n",
      "Epoch 476/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8612 - val_acc: 0.6891\n",
      "Epoch 477/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1716 - val_acc: 0.6739\n",
      "Epoch 478/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2279 - val_acc: 0.6715\n",
      "Epoch 479/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2221 - val_acc: 0.6723\n",
      "Epoch 480/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0307 - acc: 0.9978 - val_loss: 4.6352 - val_acc: 0.7035\n",
      "Epoch 481/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0324 - acc: 0.9976 - val_loss: 5.0418 - val_acc: 0.6835\n",
      "Epoch 482/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0418 - val_acc: 0.6835\n",
      "Epoch 483/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0532 - val_acc: 0.6827\n",
      "Epoch 484/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0302 - acc: 0.9979 - val_loss: 5.3169 - val_acc: 0.6667\n",
      "Epoch 485/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0346 - acc: 0.9978 - val_loss: 5.2169 - val_acc: 0.6731\n",
      "Epoch 486/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2967 - val_acc: 0.6683\n",
      "Epoch 487/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0399 - acc: 0.9974 - val_loss: 5.1329 - val_acc: 0.6755\n",
      "Epoch 488/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1089 - val_acc: 0.6779\n",
      "Epoch 489/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1069 - val_acc: 0.6779\n",
      "Epoch 490/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1069 - val_acc: 0.6779\n",
      "Epoch 491/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9958 - val_acc: 0.6859\n",
      "Epoch 492/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0315 - acc: 0.9978 - val_loss: 5.1799 - val_acc: 0.6747\n",
      "Epoch 493/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0318 - acc: 0.9978 - val_loss: 5.3800 - val_acc: 0.6603\n",
      "Epoch 494/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0344 - acc: 0.9978 - val_loss: 5.1942 - val_acc: 0.6715\n",
      "Epoch 495/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0306 - acc: 0.9978 - val_loss: 5.0123 - val_acc: 0.6843\n",
      "Epoch 496/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0123 - val_acc: 0.6843\n",
      "Epoch 497/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0123 - val_acc: 0.6843\n",
      "Epoch 498/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0314 - acc: 0.9979 - val_loss: 5.2086 - val_acc: 0.6715\n",
      "Epoch 499/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2086 - val_acc: 0.6715\n",
      "Epoch 500/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2086 - val_acc: 0.6715\n",
      "Epoch 501/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7386 - val_acc: 0.6955\n",
      "Epoch 502/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7386 - val_acc: 0.6955\n",
      "Epoch 503/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1595 - val_acc: 0.6739\n",
      "Epoch 504/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1011 - val_acc: 0.6771\n",
      "Epoch 505/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0343 - acc: 0.9978 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 506/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 507/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 508/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 509/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 510/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 511/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 512/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2156 - val_acc: 0.6739\n",
      "Epoch 513/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3619 - val_acc: 0.6635\n",
      "Epoch 514/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.9963 - val_acc: 0.6835\n",
      "Epoch 515/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0357 - acc: 0.9978 - val_loss: 5.2762 - val_acc: 0.6691\n",
      "Epoch 516/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2762 - val_acc: 0.6691\n",
      "Epoch 517/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.0857 - val_acc: 0.6779\n",
      "Epoch 518/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0892 - val_acc: 0.6779\n",
      "Epoch 519/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0892 - val_acc: 0.6779\n",
      "Epoch 520/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0356 - acc: 0.9978 - val_loss: 5.1082 - val_acc: 0.6779\n",
      "Epoch 521/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1082 - val_acc: 0.6779\n",
      "Epoch 522/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0342 - acc: 0.9978 - val_loss: 5.2514 - val_acc: 0.6699\n",
      "Epoch 523/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2896 - val_acc: 0.6667\n",
      "Epoch 524/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2831 - val_acc: 0.6667\n",
      "Epoch 525/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2999 - val_acc: 0.6667\n",
      "Epoch 526/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2999 - val_acc: 0.6667\n",
      "Epoch 527/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0306 - acc: 0.9978 - val_loss: 5.0257 - val_acc: 0.6843\n",
      "Epoch 528/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0257 - val_acc: 0.6843\n",
      "Epoch 529/1000\n",
      "2682/2682 [==============================] - 3s 971us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0257 - val_acc: 0.6843\n",
      "Epoch 530/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0257 - val_acc: 0.6843\n",
      "Epoch 531/1000\n",
      "2682/2682 [==============================] - 3s 972us/step - loss: 0.0342 - acc: 0.9978 - val_loss: 5.2304 - val_acc: 0.6731\n",
      "Epoch 532/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.0306 - val_acc: 0.6827\n",
      "Epoch 533/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0306 - val_acc: 0.6827\n",
      "Epoch 534/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1653 - val_acc: 0.6747\n",
      "Epoch 535/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.3452 - val_acc: 0.6651\n",
      "Epoch 536/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0314 - acc: 0.9978 - val_loss: 5.2115 - val_acc: 0.6715\n",
      "Epoch 537/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2115 - val_acc: 0.6715\n",
      "Epoch 538/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0318 - acc: 0.9978 - val_loss: 4.7282 - val_acc: 0.6971\n",
      "Epoch 539/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0361 - acc: 0.9976 - val_loss: 5.1942 - val_acc: 0.6731\n",
      "Epoch 540/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2211 - val_acc: 0.6731\n",
      "Epoch 541/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2211 - val_acc: 0.6731\n",
      "Epoch 542/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0361 - acc: 0.9976 - val_loss: 5.0822 - val_acc: 0.6795\n",
      "Epoch 543/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0822 - val_acc: 0.6795\n",
      "Epoch 544/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0409 - acc: 0.9974 - val_loss: 5.0363 - val_acc: 0.6827\n",
      "Epoch 545/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.0477 - val_acc: 0.6819\n",
      "Epoch 546/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0444 - val_acc: 0.6827\n",
      "Epoch 547/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0444 - val_acc: 0.6827\n",
      "Epoch 548/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1036 - val_acc: 0.6771\n",
      "Epoch 549/1000\n",
      "2682/2682 [==============================] - 3s 972us/step - loss: 0.0355 - acc: 0.9978 - val_loss: 4.8693 - val_acc: 0.6883\n",
      "Epoch 550/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 551/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 552/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 553/1000\n",
      "2682/2682 [==============================] - 3s 972us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 554/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 555/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 556/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8442 - val_acc: 0.6899\n",
      "Epoch 557/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0303 - acc: 0.9979 - val_loss: 5.2263 - val_acc: 0.6715\n",
      "Epoch 558/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0316 - acc: 0.9979 - val_loss: 4.9164 - val_acc: 0.6875\n",
      "Epoch 559/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.3173 - val_acc: 0.6651\n",
      "Epoch 560/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0304 - acc: 0.9979 - val_loss: 4.9740 - val_acc: 0.6859\n",
      "Epoch 561/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1435 - val_acc: 0.6763\n",
      "Epoch 562/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1435 - val_acc: 0.6763\n",
      "Epoch 563/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 564/1000\n",
      "2682/2682 [==============================] - 3s 971us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 565/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 566/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 567/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 568/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 569/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 570/1000\n",
      "2682/2682 [==============================] - 3s 970us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 571/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 572/1000\n",
      "2682/2682 [==============================] - 3s 969us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2158 - val_acc: 0.6731\n",
      "Epoch 573/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2267 - val_acc: 0.6731\n",
      "Epoch 574/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1268 - val_acc: 0.6779\n",
      "Epoch 575/1000\n",
      "2682/2682 [==============================] - 3s 971us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8122 - val_acc: 0.6939\n",
      "Epoch 576/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8122 - val_acc: 0.6939\n",
      "Epoch 577/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.0706 - val_acc: 0.6795\n",
      "Epoch 578/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0781 - val_acc: 0.6795\n",
      "Epoch 579/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0781 - val_acc: 0.6795\n",
      "Epoch 580/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0365 - acc: 0.9976 - val_loss: 5.0774 - val_acc: 0.6803\n",
      "Epoch 581/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2344 - val_acc: 0.6715\n",
      "Epoch 582/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2344 - val_acc: 0.6715\n",
      "Epoch 583/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2558 - val_acc: 0.6715\n",
      "Epoch 584/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0350 - acc: 0.9978 - val_loss: 4.8640 - val_acc: 0.6923\n",
      "Epoch 585/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0344 - acc: 0.9978 - val_loss: 5.1920 - val_acc: 0.6731\n",
      "Epoch 586/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1920 - val_acc: 0.6731\n",
      "Epoch 587/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1806 - val_acc: 0.6739\n",
      "Epoch 588/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2540 - val_acc: 0.6699\n",
      "Epoch 589/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.4016 - val_acc: 0.6619\n",
      "Epoch 590/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0343 - acc: 0.9974 - val_loss: 4.6874 - val_acc: 0.6987\n",
      "Epoch 591/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0984 - val_acc: 0.6779\n",
      "Epoch 592/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0984 - val_acc: 0.6779\n",
      "Epoch 593/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0984 - val_acc: 0.6779\n",
      "Epoch 594/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0984 - val_acc: 0.6779\n",
      "Epoch 595/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0984 - val_acc: 0.6779\n",
      "Epoch 596/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0360 - acc: 0.9976 - val_loss: 4.6538 - val_acc: 0.7035\n",
      "Epoch 597/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0349 - acc: 0.9978 - val_loss: 5.0412 - val_acc: 0.6843\n",
      "Epoch 598/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0310 - acc: 0.9976 - val_loss: 5.1769 - val_acc: 0.6747\n",
      "Epoch 599/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3445 - val_acc: 0.6659\n",
      "Epoch 600/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0357 - acc: 0.9978 - val_loss: 5.1563 - val_acc: 0.6763\n",
      "Epoch 601/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1797 - val_acc: 0.6731\n",
      "Epoch 602/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1797 - val_acc: 0.6731\n",
      "Epoch 603/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1797 - val_acc: 0.6731\n",
      "Epoch 604/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0334 - acc: 0.9974 - val_loss: 5.1984 - val_acc: 0.6731\n",
      "Epoch 605/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1984 - val_acc: 0.6731\n",
      "Epoch 606/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0309 - acc: 0.9979 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 607/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 608/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 609/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 610/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 611/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 612/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 613/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2774 - val_acc: 0.6683\n",
      "Epoch 614/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 615/1000\n",
      "2682/2682 [==============================] - 3s 998us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 616/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 617/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 618/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 619/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2740 - val_acc: 0.6683\n",
      "Epoch 620/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0372 - acc: 0.9974 - val_loss: 5.1689 - val_acc: 0.6739\n",
      "Epoch 621/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0316 - acc: 0.9978 - val_loss: 5.0248 - val_acc: 0.6835\n",
      "Epoch 622/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0248 - val_acc: 0.6835\n",
      "Epoch 623/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0248 - val_acc: 0.6835\n",
      "Epoch 624/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0361 - acc: 0.9976 - val_loss: 5.2556 - val_acc: 0.6699\n",
      "Epoch 625/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2556 - val_acc: 0.6699\n",
      "Epoch 626/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0357 - acc: 0.9978 - val_loss: 5.3274 - val_acc: 0.6659\n",
      "Epoch 627/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3371 - val_acc: 0.6667\n",
      "Epoch 628/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3371 - val_acc: 0.6667\n",
      "Epoch 629/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1747 - val_acc: 0.6739\n",
      "Epoch 630/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1747 - val_acc: 0.6739\n",
      "Epoch 631/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 632/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 633/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 634/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 635/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 636/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 637/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 638/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 639/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1700 - val_acc: 0.6747\n",
      "Epoch 640/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0353 - acc: 0.9976 - val_loss: 5.2329 - val_acc: 0.6691\n",
      "Epoch 641/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8966 - val_acc: 0.6907\n",
      "Epoch 642/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8966 - val_acc: 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 643/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0356 - acc: 0.9978 - val_loss: 5.0926 - val_acc: 0.6787\n",
      "Epoch 644/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0926 - val_acc: 0.6787\n",
      "Epoch 645/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0276 - val_acc: 0.6827\n",
      "Epoch 646/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0276 - val_acc: 0.6827\n",
      "Epoch 647/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0276 - val_acc: 0.6827\n",
      "Epoch 648/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0276 - val_acc: 0.6827\n",
      "Epoch 649/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0304 - acc: 0.9979 - val_loss: 4.7995 - val_acc: 0.6939\n",
      "Epoch 650/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 651/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 652/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 653/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 654/1000\n",
      "2682/2682 [==============================] - 3s 1000us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 655/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1943 - val_acc: 0.6747\n",
      "Epoch 656/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1921 - val_acc: 0.6747\n",
      "Epoch 657/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1921 - val_acc: 0.6747\n",
      "Epoch 658/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1921 - val_acc: 0.6747\n",
      "Epoch 659/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0309 - acc: 0.9978 - val_loss: 5.0523 - val_acc: 0.6835\n",
      "Epoch 660/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0523 - val_acc: 0.6835\n",
      "Epoch 661/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0523 - val_acc: 0.6835\n",
      "Epoch 662/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0523 - val_acc: 0.6835\n",
      "Epoch 663/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0523 - val_acc: 0.6835\n",
      "Epoch 664/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7961 - val_acc: 0.6891\n",
      "Epoch 665/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0338 - acc: 0.9978 - val_loss: 5.1372 - val_acc: 0.6755\n",
      "Epoch 666/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1372 - val_acc: 0.6755\n",
      "Epoch 667/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1372 - val_acc: 0.6755\n",
      "Epoch 668/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1372 - val_acc: 0.6755\n",
      "Epoch 669/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1372 - val_acc: 0.6755\n",
      "Epoch 670/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0343 - acc: 0.9978 - val_loss: 5.2829 - val_acc: 0.6675\n",
      "Epoch 671/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2829 - val_acc: 0.6675\n",
      "Epoch 672/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0324 - acc: 0.9976 - val_loss: 4.9496 - val_acc: 0.6875\n",
      "Epoch 673/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9496 - val_acc: 0.6875\n",
      "Epoch 674/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9496 - val_acc: 0.6875\n",
      "Epoch 675/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1638 - val_acc: 0.6731\n",
      "Epoch 676/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8558 - val_acc: 0.6907\n",
      "Epoch 677/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8558 - val_acc: 0.6907\n",
      "Epoch 678/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9490 - val_acc: 0.6875\n",
      "Epoch 679/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9490 - val_acc: 0.6875\n",
      "Epoch 680/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9490 - val_acc: 0.6875\n",
      "Epoch 681/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 4.7110 - val_acc: 0.6987\n",
      "Epoch 682/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8316 - val_acc: 0.6915\n",
      "Epoch 683/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0331 - acc: 0.9978 - val_loss: 4.7440 - val_acc: 0.6979\n",
      "Epoch 684/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7504 - val_acc: 0.6955\n",
      "Epoch 685/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.1456 - val_acc: 0.6755\n",
      "Epoch 686/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1283 - val_acc: 0.6755\n",
      "Epoch 687/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1283 - val_acc: 0.6755\n",
      "Epoch 688/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1283 - val_acc: 0.6755\n",
      "Epoch 689/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9927 - val_acc: 0.6843\n",
      "Epoch 690/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9927 - val_acc: 0.6843\n",
      "Epoch 691/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0177 - val_acc: 0.6843\n",
      "Epoch 692/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1564 - val_acc: 0.6755\n",
      "Epoch 693/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1564 - val_acc: 0.6755\n",
      "Epoch 694/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1564 - val_acc: 0.6755\n",
      "Epoch 695/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1564 - val_acc: 0.6755\n",
      "Epoch 696/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0506 - val_acc: 0.6819\n",
      "Epoch 697/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0506 - val_acc: 0.6819\n",
      "Epoch 698/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0455 - val_acc: 0.6819\n",
      "Epoch 699/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0314 - acc: 0.9978 - val_loss: 5.2758 - val_acc: 0.6683\n",
      "Epoch 700/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0355 - acc: 0.9978 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 701/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 702/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 703/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 704/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 705/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 706/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 707/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 708/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 709/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 710/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 711/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8295 - val_acc: 0.6947\n",
      "Epoch 712/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0310 - acc: 0.9979 - val_loss: 5.0777 - val_acc: 0.6811\n",
      "Epoch 713/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1744 - val_acc: 0.6747\n",
      "Epoch 714/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1744 - val_acc: 0.6747\n",
      "Epoch 715/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2184 - val_acc: 0.6723\n",
      "Epoch 716/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2184 - val_acc: 0.6723\n",
      "Epoch 717/1000\n",
      "2682/2682 [==============================] - 3s 999us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2184 - val_acc: 0.6723\n",
      "Epoch 718/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2184 - val_acc: 0.6723\n",
      "Epoch 719/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2107 - val_acc: 0.6731\n",
      "Epoch 720/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0528 - val_acc: 0.6819\n",
      "Epoch 721/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0528 - val_acc: 0.6819\n",
      "Epoch 722/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8024 - val_acc: 0.6923\n",
      "Epoch 723/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9800 - val_acc: 0.6827\n",
      "Epoch 724/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9800 - val_acc: 0.6827\n",
      "Epoch 725/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9800 - val_acc: 0.6827\n",
      "Epoch 726/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0337 - acc: 0.9978 - val_loss: 4.9877 - val_acc: 0.6835\n",
      "Epoch 727/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1682 - val_acc: 0.6731\n",
      "Epoch 728/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1682 - val_acc: 0.6731\n",
      "Epoch 729/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1682 - val_acc: 0.6731\n",
      "Epoch 730/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1588 - val_acc: 0.6755\n",
      "Epoch 731/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0294 - val_acc: 0.6827\n",
      "Epoch 732/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0294 - val_acc: 0.6827\n",
      "Epoch 733/1000\n",
      "2682/2682 [==============================] - 3s 999us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2093 - val_acc: 0.6699\n",
      "Epoch 734/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2093 - val_acc: 0.6699\n",
      "Epoch 735/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1984 - val_acc: 0.6699\n",
      "Epoch 736/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0302 - acc: 0.9979 - val_loss: 4.9555 - val_acc: 0.6859\n",
      "Epoch 737/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9555 - val_acc: 0.6859\n",
      "Epoch 738/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 4.9748 - val_acc: 0.6875\n",
      "Epoch 739/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1790 - val_acc: 0.6731\n",
      "Epoch 740/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1790 - val_acc: 0.6731\n",
      "Epoch 741/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9700 - val_acc: 0.6875\n",
      "Epoch 742/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9700 - val_acc: 0.6875\n",
      "Epoch 743/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9700 - val_acc: 0.6875\n",
      "Epoch 744/1000\n",
      "2682/2682 [==============================] - 3s 972us/step - loss: 0.0334 - acc: 0.9978 - val_loss: 5.2172 - val_acc: 0.6715\n",
      "Epoch 745/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2172 - val_acc: 0.6715\n",
      "Epoch 746/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2172 - val_acc: 0.6715\n",
      "Epoch 747/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2172 - val_acc: 0.6715\n",
      "Epoch 748/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2242 - val_acc: 0.6723\n",
      "Epoch 749/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2242 - val_acc: 0.6723\n",
      "Epoch 750/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2242 - val_acc: 0.6723\n",
      "Epoch 751/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2224 - val_acc: 0.6723\n",
      "Epoch 752/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6731\n",
      "Epoch 753/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1674 - val_acc: 0.6739\n",
      "Epoch 754/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2458 - val_acc: 0.6699\n",
      "Epoch 755/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3345 - val_acc: 0.6651\n",
      "Epoch 756/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3345 - val_acc: 0.6651\n",
      "Epoch 757/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3345 - val_acc: 0.6651\n",
      "Epoch 758/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2542 - val_acc: 0.6683\n",
      "Epoch 759/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2542 - val_acc: 0.6683\n",
      "Epoch 760/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1023 - val_acc: 0.6763\n",
      "Epoch 761/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0917 - val_acc: 0.6779\n",
      "Epoch 762/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2744 - val_acc: 0.6683\n",
      "Epoch 763/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2744 - val_acc: 0.6683\n",
      "Epoch 764/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0300 - acc: 0.9979 - val_loss: 5.1879 - val_acc: 0.6731\n",
      "Epoch 765/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1879 - val_acc: 0.6731\n",
      "Epoch 766/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1879 - val_acc: 0.6731\n",
      "Epoch 767/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0557 - val_acc: 0.6779\n",
      "Epoch 768/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0557 - val_acc: 0.6779\n",
      "Epoch 769/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0557 - val_acc: 0.6779\n",
      "Epoch 770/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0658 - val_acc: 0.6779\n",
      "Epoch 771/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0658 - val_acc: 0.6779\n",
      "Epoch 772/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0658 - val_acc: 0.6779\n",
      "Epoch 773/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.2570 - val_acc: 0.6699\n",
      "Epoch 774/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3049 - val_acc: 0.6667\n",
      "Epoch 775/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3049 - val_acc: 0.6667\n",
      "Epoch 776/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0303 - acc: 0.9978 - val_loss: 5.2549 - val_acc: 0.6699\n",
      "Epoch 777/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2549 - val_acc: 0.6699\n",
      "Epoch 778/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2549 - val_acc: 0.6699\n",
      "Epoch 779/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2549 - val_acc: 0.6699\n",
      "Epoch 780/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2429 - val_acc: 0.6699\n",
      "Epoch 781/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2429 - val_acc: 0.6699\n",
      "Epoch 782/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2429 - val_acc: 0.6699\n",
      "Epoch 783/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2429 - val_acc: 0.6699\n",
      "Epoch 784/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 785/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 786/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 787/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 788/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 789/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 790/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 791/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 792/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 793/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 794/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 795/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2104 - val_acc: 0.6715\n",
      "Epoch 796/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0355 - acc: 0.9978 - val_loss: 5.2365 - val_acc: 0.6699\n",
      "Epoch 797/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2365 - val_acc: 0.6699\n",
      "Epoch 798/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2313 - val_acc: 0.6699\n",
      "Epoch 799/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 800/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 801/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 802/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 803/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 804/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1896 - val_acc: 0.6731\n",
      "Epoch 805/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 806/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 807/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 808/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 809/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 810/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 811/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 812/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 813/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 814/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1522 - val_acc: 0.6747\n",
      "Epoch 815/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6747\n",
      "Epoch 816/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6747\n",
      "Epoch 817/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6747\n",
      "Epoch 818/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6747\n",
      "Epoch 819/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6747\n",
      "Epoch 820/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0302 - acc: 0.9979 - val_loss: 4.7415 - val_acc: 0.6987\n",
      "Epoch 821/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7415 - val_acc: 0.6987\n",
      "Epoch 822/1000\n",
      "2682/2682 [==============================] - 3s 998us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.7415 - val_acc: 0.6987\n",
      "Epoch 823/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0306 - acc: 0.9979 - val_loss: 5.2765 - val_acc: 0.6699\n",
      "Epoch 824/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0373 - acc: 0.9974 - val_loss: 5.1970 - val_acc: 0.6739\n",
      "Epoch 825/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9908 - val_acc: 0.6827\n",
      "Epoch 826/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9969 - val_acc: 0.6819\n",
      "Epoch 827/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.0009 - val_acc: 0.6819\n",
      "Epoch 828/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0361 - acc: 0.9976 - val_loss: 5.4078 - val_acc: 0.6571\n",
      "Epoch 829/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0314 - acc: 0.9978 - val_loss: 5.2573 - val_acc: 0.6707\n",
      "Epoch 830/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2573 - val_acc: 0.6707\n",
      "Epoch 831/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2573 - val_acc: 0.6707\n",
      "Epoch 832/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0355 - acc: 0.9978 - val_loss: 5.4001 - val_acc: 0.6595\n",
      "Epoch 833/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1869 - val_acc: 0.6731\n",
      "Epoch 834/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1869 - val_acc: 0.6731\n",
      "Epoch 835/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1869 - val_acc: 0.6731\n",
      "Epoch 836/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0398 - acc: 0.9974 - val_loss: 5.4168 - val_acc: 0.6603\n",
      "Epoch 837/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.3247 - val_acc: 0.6659\n",
      "Epoch 838/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0342 - acc: 0.9974 - val_loss: 5.2555 - val_acc: 0.6691\n",
      "Epoch 839/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2555 - val_acc: 0.6691\n",
      "Epoch 840/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0487 - val_acc: 0.6811\n",
      "Epoch 841/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0487 - val_acc: 0.6811\n",
      "Epoch 842/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0322 - acc: 0.9978 - val_loss: 5.2821 - val_acc: 0.6675\n",
      "Epoch 843/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2821 - val_acc: 0.6675\n",
      "Epoch 844/1000\n",
      "2682/2682 [==============================] - 3s 997us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2821 - val_acc: 0.6675\n",
      "Epoch 845/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2797 - val_acc: 0.6691\n",
      "Epoch 846/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0379 - acc: 0.9974 - val_loss: 4.8130 - val_acc: 0.6915\n",
      "Epoch 847/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8217 - val_acc: 0.6907\n",
      "Epoch 848/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.8217 - val_acc: 0.6907\n",
      "Epoch 849/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1770 - val_acc: 0.6747\n",
      "Epoch 850/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1770 - val_acc: 0.6747\n",
      "Epoch 851/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1770 - val_acc: 0.6747\n",
      "Epoch 852/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1770 - val_acc: 0.6747\n",
      "Epoch 853/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1964 - val_acc: 0.6747\n",
      "Epoch 854/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1964 - val_acc: 0.6747\n",
      "Epoch 855/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1722 - val_acc: 0.6747\n",
      "Epoch 856/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1722 - val_acc: 0.6747\n",
      "Epoch 857/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0329 - acc: 0.9978 - val_loss: 5.2463 - val_acc: 0.6683\n",
      "Epoch 858/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2463 - val_acc: 0.6683\n",
      "Epoch 859/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0313 - acc: 0.9978 - val_loss: 5.0926 - val_acc: 0.6795\n",
      "Epoch 860/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0926 - val_acc: 0.6795\n",
      "Epoch 861/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1089 - val_acc: 0.6763\n",
      "Epoch 862/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1089 - val_acc: 0.6763\n",
      "Epoch 863/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1089 - val_acc: 0.6763\n",
      "Epoch 864/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 865/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 866/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 867/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 868/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 869/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 870/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 871/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1066 - val_acc: 0.6763\n",
      "Epoch 872/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1469 - val_acc: 0.6747\n",
      "Epoch 873/1000\n",
      "2682/2682 [==============================] - 3s 995us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1484 - val_acc: 0.6747\n",
      "Epoch 874/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1484 - val_acc: 0.6747\n",
      "Epoch 875/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0327 - acc: 0.9978 - val_loss: 4.9997 - val_acc: 0.6843\n",
      "Epoch 876/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9997 - val_acc: 0.6843\n",
      "Epoch 877/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 878/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 879/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 880/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 881/1000\n",
      "2682/2682 [==============================] - 3s 998us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 882/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0503 - val_acc: 0.6827\n",
      "Epoch 883/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0806 - val_acc: 0.6803\n",
      "Epoch 884/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0806 - val_acc: 0.6803\n",
      "Epoch 885/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1587 - val_acc: 0.6747\n",
      "Epoch 886/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1587 - val_acc: 0.6747\n",
      "Epoch 887/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1587 - val_acc: 0.6747\n",
      "Epoch 888/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2120 - val_acc: 0.6731\n",
      "Epoch 889/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2120 - val_acc: 0.6731\n",
      "Epoch 890/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2120 - val_acc: 0.6731\n",
      "Epoch 891/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2120 - val_acc: 0.6731\n",
      "Epoch 892/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1928 - val_acc: 0.6739\n",
      "Epoch 893/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1928 - val_acc: 0.6739\n",
      "Epoch 894/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0320 - acc: 0.9978 - val_loss: 5.0758 - val_acc: 0.6819\n",
      "Epoch 895/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0732 - val_acc: 0.6827\n",
      "Epoch 896/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9750 - val_acc: 0.6827\n",
      "Epoch 897/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9750 - val_acc: 0.6827\n",
      "Epoch 898/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9834 - val_acc: 0.6827\n",
      "Epoch 899/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1399 - val_acc: 0.6779\n",
      "Epoch 900/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1399 - val_acc: 0.6779\n",
      "Epoch 901/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1399 - val_acc: 0.6779\n",
      "Epoch 902/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 903/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 904/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 905/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 906/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 907/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1397 - val_acc: 0.6779\n",
      "Epoch 908/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1557 - val_acc: 0.6763\n",
      "Epoch 909/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1557 - val_acc: 0.6763\n",
      "Epoch 910/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 911/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 912/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 913/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 914/1000\n",
      "2682/2682 [==============================] - 3s 994us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 915/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 916/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1550 - val_acc: 0.6779\n",
      "Epoch 917/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1219 - val_acc: 0.6779\n",
      "Epoch 918/1000\n",
      "2682/2682 [==============================] - 3s 996us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1219 - val_acc: 0.6779\n",
      "Epoch 919/1000\n",
      "2682/2682 [==============================] - 3s 998us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1219 - val_acc: 0.6779\n",
      "Epoch 920/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 921/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 922/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 923/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 924/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 925/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 926/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 927/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2933 - val_acc: 0.6675\n",
      "Epoch 928/1000\n",
      "2682/2682 [==============================] - 3s 987us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 929/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 930/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 931/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 932/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 933/1000\n",
      "2682/2682 [==============================] - 3s 989us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 934/1000\n",
      "2682/2682 [==============================] - 3s 991us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 935/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2890 - val_acc: 0.6675\n",
      "Epoch 936/1000\n",
      "2682/2682 [==============================] - 3s 990us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1157 - val_acc: 0.6763\n",
      "Epoch 937/1000\n",
      "2682/2682 [==============================] - 3s 993us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1157 - val_acc: 0.6763\n",
      "Epoch 938/1000\n",
      "2682/2682 [==============================] - 3s 992us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1157 - val_acc: 0.6763\n",
      "Epoch 939/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0396 - acc: 0.9970 - val_loss: 5.0066 - val_acc: 0.6827\n",
      "Epoch 940/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0326 - acc: 0.9976 - val_loss: 5.3899 - val_acc: 0.6611\n",
      "Epoch 941/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.3508 - val_acc: 0.6651\n",
      "Epoch 942/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.6865 - val_acc: 0.6442\n",
      "Epoch 943/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2484 - val_acc: 0.6707\n",
      "Epoch 944/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0312 - acc: 0.9979 - val_loss: 5.4463 - val_acc: 0.6571\n",
      "Epoch 945/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0347 - acc: 0.9978 - val_loss: 5.2920 - val_acc: 0.6659\n",
      "Epoch 946/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1986 - val_acc: 0.6747\n",
      "Epoch 947/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0312 - acc: 0.9978 - val_loss: 5.2080 - val_acc: 0.6731\n",
      "Epoch 948/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2722 - val_acc: 0.6683\n",
      "Epoch 949/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2722 - val_acc: 0.6683\n",
      "Epoch 950/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2722 - val_acc: 0.6683\n",
      "Epoch 951/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0387 - acc: 0.9974 - val_loss: 4.8572 - val_acc: 0.6939\n",
      "Epoch 952/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2809 - val_acc: 0.6691\n",
      "Epoch 953/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2809 - val_acc: 0.6691\n",
      "Epoch 954/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2809 - val_acc: 0.6691\n",
      "Epoch 955/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2786 - val_acc: 0.6691\n",
      "Epoch 956/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1948 - val_acc: 0.6739\n",
      "Epoch 957/1000\n",
      "2682/2682 [==============================] - 3s 985us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1948 - val_acc: 0.6739\n",
      "Epoch 958/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1948 - val_acc: 0.6739\n",
      "Epoch 959/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0307 - acc: 0.9979 - val_loss: 4.7956 - val_acc: 0.6955\n",
      "Epoch 960/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0358 - acc: 0.9978 - val_loss: 5.1569 - val_acc: 0.6763\n",
      "Epoch 961/1000\n",
      "2682/2682 [==============================] - 3s 972us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1569 - val_acc: 0.6763\n",
      "Epoch 962/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1545 - val_acc: 0.6763\n",
      "Epoch 963/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1545 - val_acc: 0.6763\n",
      "Epoch 964/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1063 - val_acc: 0.6803\n",
      "Epoch 965/1000\n",
      "2682/2682 [==============================] - 3s 986us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0027 - val_acc: 0.6843\n",
      "Epoch 966/1000\n",
      "2682/2682 [==============================] - 3s 983us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0027 - val_acc: 0.6843\n",
      "Epoch 967/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0027 - val_acc: 0.6843\n",
      "Epoch 968/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9602 - val_acc: 0.6859\n",
      "Epoch 969/1000\n",
      "2682/2682 [==============================] - 3s 971us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9602 - val_acc: 0.6859\n",
      "Epoch 970/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9858 - val_acc: 0.6859\n",
      "Epoch 971/1000\n",
      "2682/2682 [==============================] - 3s 974us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9858 - val_acc: 0.6859\n",
      "Epoch 972/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0323 - acc: 0.9978 - val_loss: 5.1694 - val_acc: 0.6763\n",
      "Epoch 973/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1694 - val_acc: 0.6763\n",
      "Epoch 974/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1694 - val_acc: 0.6763\n",
      "Epoch 975/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0316 - acc: 0.9978 - val_loss: 5.3446 - val_acc: 0.6651\n",
      "Epoch 976/1000\n",
      "2682/2682 [==============================] - 3s 979us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2841 - val_acc: 0.6699\n",
      "Epoch 977/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0377 - acc: 0.9974 - val_loss: 5.0840 - val_acc: 0.6795\n",
      "Epoch 978/1000\n",
      "2682/2682 [==============================] - 3s 982us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2811 - val_acc: 0.6683\n",
      "Epoch 979/1000\n",
      "2682/2682 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2811 - val_acc: 0.6683\n",
      "Epoch 980/1000\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2811 - val_acc: 0.6683\n",
      "Epoch 981/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2938 - val_acc: 0.6683\n",
      "Epoch 982/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2992 - val_acc: 0.6675\n",
      "Epoch 983/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0359 - acc: 0.9978 - val_loss: 5.2984 - val_acc: 0.6675\n",
      "Epoch 984/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2984 - val_acc: 0.6675\n",
      "Epoch 985/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2984 - val_acc: 0.6675\n",
      "Epoch 986/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.2984 - val_acc: 0.6675\n",
      "Epoch 987/1000\n",
      "2682/2682 [==============================] - 3s 981us/step - loss: 0.0300 - acc: 0.9981 - val_loss: 5.1986 - val_acc: 0.6731\n",
      "Epoch 988/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1986 - val_acc: 0.6731\n",
      "Epoch 989/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9082 - val_acc: 0.6859\n",
      "Epoch 990/1000\n",
      "2682/2682 [==============================] - 3s 975us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9082 - val_acc: 0.6859\n",
      "Epoch 991/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9082 - val_acc: 0.6859\n",
      "Epoch 992/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 4.9082 - val_acc: 0.6859\n",
      "Epoch 993/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1658 - val_acc: 0.6763\n",
      "Epoch 994/1000\n",
      "2682/2682 [==============================] - 3s 980us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1658 - val_acc: 0.6763\n",
      "Epoch 995/1000\n",
      "2682/2682 [==============================] - 3s 984us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1658 - val_acc: 0.6763\n",
      "Epoch 996/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1658 - val_acc: 0.6763\n",
      "Epoch 997/1000\n",
      "2682/2682 [==============================] - 3s 977us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.1658 - val_acc: 0.6763\n",
      "Epoch 998/1000\n",
      "2682/2682 [==============================] - 3s 976us/step - loss: 0.0301 - acc: 0.9979 - val_loss: 5.0617 - val_acc: 0.6827\n",
      "Epoch 999/1000\n",
      "2682/2682 [==============================] - 3s 973us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0888 - val_acc: 0.6795\n",
      "Epoch 1000/1000\n",
      "2682/2682 [==============================] - 3s 978us/step - loss: 0.0299 - acc: 0.9981 - val_loss: 5.0888 - val_acc: 0.6795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1a087a29dd8>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inc_X_train,\n",
    "         y_train_array_k,\n",
    "         epochs=1000,\n",
    "         batch_size=batch_size,\n",
    "         validation_data=(inc_X_test, y_test_k),\n",
    "         callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.15      0.26       234\n",
      "          1       0.66      1.00      0.80       390\n",
      "\n",
      "avg / total       0.78      0.68      0.59       624\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a088262390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_base = model.predict(inc_X_test)\n",
    "model_pred = [label_decoder(i) for i in model_base]\n",
    "cm = confusion_matrix(y_test, model_pred)\n",
    "sns.heatmap(cm, annot=True,cmap='Blues',xticklabels = ['Predicted normal','Predicted pneumonia'],\n",
    "           yticklabels=['Actual normal', 'Actual pneumonia'], fmt='d')\n",
    "print(classification_report(y_test, model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate features from augmented images using ResNet\n",
    "X_train_aug = base_resnet.predict_generator(train_aug,steps = len(np.array(train_images_usample))/24)\n",
    "X_test_aug = base_resnet.predict_generator(test_aug, steps = len(np.array(test_images))/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2682, 1, 1, 2048)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = keras.models.Sequential()\n",
    "aug_model.add(keras.layers.Flatten())\n",
    "aug_model.add(keras.layers.Dense(256, activation='relu', input_dim=1*1*2048))\n",
    "aug_model.add(keras.layers.Dropout(0.5))\n",
    "aug_model.add(keras.layers.Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/20\n",
      "2682/2682 [==============================] - 102s 38ms/step - loss: 0.7198 - acc: 0.4914 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "2682/2682 [==============================] - 1s 304us/step - loss: 0.6932 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "2682/2682 [==============================] - 1s 285us/step - loss: 0.6932 - acc: 0.4925 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4840 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 5/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4952 - val_loss: 0.6927 - val_acc: 0.6250\n",
      "Epoch 6/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4918 - val_loss: 0.6927 - val_acc: 0.6250\n",
      "Epoch 7/20\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.6932 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.3750\n",
      "Epoch 8/20\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6930 - val_acc: 0.6250\n",
      "Epoch 10/20\n",
      "2682/2682 [==============================] - 1s 287us/step - loss: 0.6932 - acc: 0.4959 - val_loss: 0.6928 - val_acc: 0.6250\n",
      "Epoch 11/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6933 - val_acc: 0.3750\n",
      "Epoch 12/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6930 - val_acc: 0.6250\n",
      "Epoch 13/20\n",
      "2682/2682 [==============================] - 1s 280us/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6934 - val_acc: 0.3750\n",
      "Epoch 14/20\n",
      "2682/2682 [==============================] - 1s 291us/step - loss: 0.6932 - acc: 0.4911 - val_loss: 0.6931 - val_acc: 0.6250\n",
      "Epoch 15/20\n",
      "2682/2682 [==============================] - 1s 294us/step - loss: 0.6932 - acc: 0.4851 - val_loss: 0.6931 - val_acc: 0.6250\n",
      "Epoch 16/20\n",
      "2682/2682 [==============================] - 1s 302us/step - loss: 0.6932 - acc: 0.4784 - val_loss: 0.6931 - val_acc: 0.6250\n",
      "Epoch 17/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4955 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 18/20\n",
      "2682/2682 [==============================] - 1s 283us/step - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6927 - val_acc: 0.6250\n",
      "Epoch 19/20\n",
      "2682/2682 [==============================] - 1s 284us/step - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6929 - val_acc: 0.6250\n",
      "Epoch 20/20\n",
      "2682/2682 [==============================] - 1s 309us/step - loss: 0.6932 - acc: 0.4903 - val_loss: 0.6933 - val_acc: 0.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1ef20a4f7b8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_model.fit(X_train_aug,\n",
    "         y_train,\n",
    "         epochs=20,\n",
    "         batch_size=batch_size,\n",
    "         validation_data=(X_test_aug, y_test),\n",
    "         callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet does not seem to be performing well with augmentation. What about InceptionV3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = base_inc.predict_generator(train_aug,steps = len(np.array(train_images_usample))/24)\n",
    "X_test_aug = base_inc.predict_generator(test_aug, steps = len(np.array(test_images))/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = keras.models.Sequential()\n",
    "aug_model.add(keras.layers.Flatten())\n",
    "aug_model.add(keras.layers.Dense(256, activation='relu', input_dim=1*1*2048))\n",
    "aug_model.add(keras.layers.Dropout(0.5))\n",
    "aug_model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    "aug_model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2682 samples, validate on 624 samples\n",
      "Epoch 1/20\n",
      "2682/2682 [==============================] - 99s 37ms/step - loss: 1.0952 - acc: 0.4896 - val_loss: 0.6925 - val_acc: 0.6250\n",
      "Epoch 2/20\n",
      "2682/2682 [==============================] - 3s 966us/step - loss: 0.6952 - acc: 0.4864 - val_loss: 0.6923 - val_acc: 0.6250\n",
      "Epoch 3/20\n",
      "2682/2682 [==============================] - 3s 955us/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6925 - val_acc: 0.6250\n",
      "Epoch 4/20\n",
      "2682/2682 [==============================] - 3s 961us/step - loss: 0.6966 - acc: 0.4905 - val_loss: 0.6924 - val_acc: 0.6250\n",
      "Epoch 5/20\n",
      "2682/2682 [==============================] - 3s 958us/step - loss: 0.6936 - acc: 0.4957 - val_loss: 0.6920 - val_acc: 0.6250\n",
      "Epoch 6/20\n",
      "2682/2682 [==============================] - 3s 958us/step - loss: 0.6932 - acc: 0.4911 - val_loss: 0.6921 - val_acc: 0.6250\n",
      "Epoch 7/20\n",
      "2682/2682 [==============================] - 3s 962us/step - loss: 0.6946 - acc: 0.5000 - val_loss: 0.6924 - val_acc: 0.6250\n",
      "Epoch 8/20\n",
      "2682/2682 [==============================] - 3s 988us/step - loss: 0.6933 - acc: 0.4961 - val_loss: 0.6926 - val_acc: 0.6250\n",
      "Epoch 9/20\n",
      "2682/2682 [==============================] - 3s 966us/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.6234\n",
      "Epoch 10/20\n",
      "2682/2682 [==============================] - 3s 958us/step - loss: 0.6930 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5024\n",
      "Epoch 11/20\n",
      "2682/2682 [==============================] - 3s 954us/step - loss: 0.6932 - acc: 0.4897 - val_loss: 0.6927 - val_acc: 0.6250\n",
      "Epoch 12/20\n",
      "2682/2682 [==============================] - 3s 955us/step - loss: 0.6932 - acc: 0.4918 - val_loss: 0.6929 - val_acc: 0.6242\n",
      "Epoch 13/20\n",
      "2682/2682 [==============================] - 3s 954us/step - loss: 0.6932 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "2682/2682 [==============================] - 3s 947us/step - loss: 0.6932 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "2682/2682 [==============================] - 3s 951us/step - loss: 0.6932 - acc: 0.4870 - val_loss: 0.6933 - val_acc: 0.3750\n",
      "Epoch 16/20\n",
      "2682/2682 [==============================] - 3s 958us/step - loss: 0.6932 - acc: 0.4914 - val_loss: 0.6932 - val_acc: 0.3750\n",
      "Epoch 17/20\n",
      "2682/2682 [==============================] - 3s 951us/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6932 - val_acc: 0.4992\n",
      "Epoch 18/20\n",
      "2682/2682 [==============================] - 3s 962us/step - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.3750\n",
      "Epoch 19/20\n",
      "2682/2682 [==============================] - 3s 951us/step - loss: 0.6932 - acc: 0.4959 - val_loss: 0.6937 - val_acc: 0.3750\n",
      "Epoch 20/20\n",
      "2682/2682 [==============================] - 3s 966us/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6936 - val_acc: 0.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1ef13903390>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_model.fit(X_train_aug,\n",
    "         y_train,\n",
    "         epochs=20,\n",
    "         batch_size=batch_size,\n",
    "         validation_data=(X_test_aug, y_test),\n",
    "         callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3 is also not performing better than baseline here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
